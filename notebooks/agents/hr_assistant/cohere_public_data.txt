Representation Models are now available in the sizes of medium-20220217 and large-20220217 as well as an updated version of small-20220217 . Our previous small model will be available as small-20211115 . In addition, the maximum tokens length per text has increased from 512 to 1024. We recommend keeping text lengths below 128 tokens for optimal performance; for any text longer than 128 tokens, the text is spliced and the resulting embeddings of each component are then averaged and returned.*****Effective December 2, 2022, we will be consolidating our generative models and only serving our Medium (focused on speed) and X-Large (focused on quality). We will also be discontinuing support for our Medium embedding model. This means that as of this date, our Small and Large generative models and Medium embedding model will be deprecated. If you are currently using a Small or Large generative model, then we recommend that you proactively change to a Medium or X-Large model before December 2, 2022. Additionally, if you are currently using a Medium embed model, we recommend that you change to a Small embed model.*****Calls to Generate large-20220926 and xlarge-20220609 will route to the new and improved X-Large model (xlarge-20221108). Calls to Generate small-20220926 will route to the new and improved Medium model (medium-20221108). If you have any questions or concerns about this change, please don’t hesitate to contact us at: team@cohere.com.*****We test the PCW approach on in-context learning with models that range in size between 750 million and 178 billion parameters, and show substantial improvements for tasks with diverse input and output spaces. Our results motivate further investigation of Parallel Context Windows as a method for applying off-the-shelf LLMs in other settings that require long text sequences. A Length-Extrapolatable Transformer Authors: Yutao Sun, Li Dong, Barun Patra, Shuming Ma, Shaohan Huang, Alon Benhaim, Vishrav Chaudhary, Xia Song, Furu Wei Position modeling plays a critical role in Transformers. In this paper, we focus on length extrapolation, i.e., training on short texts while evaluating longer sequences.*****## Selecting the Right Model Size Larger models are more capable of complex tasks but smaller models have faster response times and are less expensive. Here is a rough guideline for which model size to use for various tasks:*****We've retrained our small , medium , and large generation and representation models. Updated representation models now support contexts up to 4096 tokens (previously 1024 tokens). We recommend keeping text lengths below 512 tokens for optimal performance; for any text longer than 512 tokens, the text is spliced and the resulting embeddings of each component are then averaged and returned.*****Nightly versions of our Common models are now available. This means that every week, you can expect the performance of command-nightly to improve as we continually retrain them. Command-nightly will be available in two sizes - medium and xlarge . The xlarge model demonstrates better performance, and medium is a great option for developers who require fast response, like those building chatbots. You can find more information here . If you were previously using the command-xlarge-20221108 model, you will now be redirected to the command-xlarge-nightly model. Please note that access to the command-xlarge-20221108 model will be discontinued after January 30, 2023.*****By training over 400 language models ranging from 70 million to over 16 billion parameters on 5 to 500 billion tokens, we find that for compute-optimal training, the model size and the number of training tokens should be scaled equally: for every doubling of model size the number of training tokens should also be doubled. We test this hypothesis by training a predicted compute-optimal model, Chinchilla, that uses the same compute budget as Gopher but with 70B parameters and 4× more data. Chinchilla uniformly and significantly outperforms Gopher (280B), GPT-3 (175B), Jurassic-1 (178B), and Megatron-Turing NLG (530B) on a large range of downstream evaluation tasks.*****# Classification Pricing  Classifications will cost $5 per 1000 text classified across the platform, regardless of model size (small, medium, large, xlarge) or usage of a finetuned model. Get an API Key Share what you're building on our co:mmunity forum or shoot us an email .*****#### Embed Capture the semantic meaning of text by representing text as numbers. Model Price per Unit Default $1.0 per 1000 Embeddings Custom $2.0 per 1000 Embeddings More details*****## Totally transparent pricing We're making it easy to explore, learn, and experiment with the Cohere Platform. All models Free Limit of 100 calls/minute Default models $ 1.00 per 1000 Embeddings Custom models $ 2.00 per 1000 Embeddings Examples Stay updated*****Here’s a summary of the new pricing model per endpoint: We also want to make it easier for you to use our largest, best performing models by removing the pricing differences by model size — pricing of each endpoint will be standardized across sizes. This way, it’s a no brainer to get the best product Cohere has to offer! Custom Generate and Embed models will continue to be twice the price of their standard counterparts, whereas pricing for both Classify categories remain the same. Here’s a summary of the new costs: 3. For starters, we’re removing jargon in our product terminology.*****### Why Cohere Free Developer Tier Learn and iterate with the Cohere API free of charge until you go to production. Easy To Use No prior ML/AI experience required. Get started with just a few examples. Customizable Models Customize our models with your own data sets, and deploy them easily to production.*****The Embed endpoint takes a piece of text and turns it into a vector embedding. Embeddings represent text in the form of numbers that capture its meaning and context. This gives us the ability to turn unstructured text data into a structured form that can be processed and analyzed.*****To upgrade your key now, simply complete the “Go to Production” workflow under the Usage tab in your dashboard (it takes less than 3 minutes). We hope that the new free tier will allow you to prototype using Cohere without worrying about having to upgrade in order to complete your experiment or proof of concept. 2. Thanks to your feedback and our relentless focus on improving efficiencies, we’ve simplified our pricing structure to make it easier for you to estimate the costs of using our platform. We’re also reducing the price of our embeddings (decreased by more than 10x depending on the length of your inputs) to enable and encourage a broader range of applications and tasks to be built with Cohere.*****# Uncover trends and patterns in text Turn text into numerical representations of language for deeper insights at scale. Embed makes it possible to algorithmically categorize and score text quickly to extract meaning.*****#### Embed Capture the semantic meaning of text by representing text as numbers. Model Price per Unit Default $1.0 per 1000 Embeddings Custom $2.0 per 1000 Embeddings More details*****## Totally transparent pricing We're making it easy to explore, learn, and experiment with the Cohere Platform. All models Free Limit of 100 calls/minute Default models $ 1.00 per 1000 Embeddings Custom models $ 2.00 per 1000 Embeddings Examples Stay updated*****Here’s a summary of the new pricing model per endpoint: We also want to make it easier for you to use our largest, best performing models by removing the pricing differences by model size — pricing of each endpoint will be standardized across sizes. This way, it’s a no brainer to get the best product Cohere has to offer! Custom Generate and Embed models will continue to be twice the price of their standard counterparts, whereas pricing for both Classify categories remain the same. Here’s a summary of the new costs: 3. For starters, we’re removing jargon in our product terminology.*****### Why Cohere Free Developer Tier Learn and iterate with the Cohere API free of charge until you go to production. Easy To Use No prior ML/AI experience required. Get started with just a few examples. Customizable Models Customize our models with your own data sets, and deploy them easily to production.*****The Embed endpoint takes a piece of text and turns it into a vector embedding. Embeddings represent text in the form of numbers that capture its meaning and context. This gives us the ability to turn unstructured text data into a structured form that can be processed and analyzed.*****We also launched our Multilingual Text Understanding Model that delivers 3x better performance than existing models, expanding functionality and access of NLP to more than 100 of the world’s most prevalent languages. Meanwhile, Sandbox , our open-sourced library of GitHub Repos, went live, empowering our developer community to experiment and work with language AI. We also introduced the Command-beta for zero-shot, instruction-guided prompting. And in an effort to make our stack increasingly accessible, we reduced our pricing . We built on our thriving community of developers through hackathons and events. We deepened our engagement with you, launching our co:mmunity Discord channel*****#### Summarize Summarize a given input text such as an article into a short paragraph or key bullet points Model Price per Unit Default $5.0 per 1000 Summarization Units* Custom $10.0 per 1000 Summarization Units* More details API Key Offering API Key Offering*****Other Questions What is the difference between a Trial API key and Production API key? Are there any account limitations upon signup? What is the difference between an organization and a personal account? Which model should I pick? When do I get billed?*****# Classification Pricing  Classifications will cost $5 per 1000 text classified across the platform, regardless of model size (small, medium, large, xlarge) or usage of a finetuned model. Get an API Key Share what you're building on our co:mmunity forum or shoot us an email .*****So, we’re replacing our current $75 free credit program with a new freemium tier that gives you more hands-on experience with our API. To enable the free tier, we’re introducing two types of API keys that give users access to all of Cohere’s endpoints: Trial API keys and Production API keys. All current API keys will automatically be converted to a Trial API key, which is rate-limited to 100 API calls per minute and cannot be used in production scenarios. by signing up for an account . For developers looking for higher rate limits, or to serve Cohere to users in an application, you can upgrade to a Production API key where the throughput is 100 times higher (10,000 calls per minute).*****Free, rate limited Trial Keys for experimentation, testing, and playground usage Production keys with no rate limit for serving Cohere in production applications Flat rate pricing for Generate and Embed endpoints Reduced pricing for Classify endpoint New UI for dashboard including sign up and onboarding - everything except playground New use-case specific Quickstart Guides to learn about using Cohere API Replacing "Finetune" nomenclature with "Custom Model" Inviting team members is now more intuitive. Teams enable users to share custom models with each other Generative custom models now show accuracy and loss metrics alongside logs Embed and Classify custom models now show logs alongside accuracy, loss, precision, f1, recall*****## Totally transparent pricing We're making it easy to explore, learn, and experiment with the Cohere Platform. All models Free Limit of 100 calls/minute Default models $ 1.00 per 1000 Embeddings Custom models $ 2.00 per 1000 Embeddings Examples Stay updated*****## Production Key Specifications Production keys for all endpoints are rate-limited at 10,000 calls per minute and are intended for serving Cohere in a public-facing application and testing purposes. Usage of Production keys is metered at price points which can be found on our pricing page. To get a Production key, you will need to complete a few steps in our Go to Production workflow. You can start the process by navigating to the Billing and Usage page in your Cohere dashboard as the Admin of your organization (or asking your organization Admin to complete these steps). From there, click on the Get your Production key button to start the process.*****## Benchmarks We extensively benchmarked our new model to ensure the best performance across a wide range of applications, domains and languages. Specifically, we used: MTEB BEIR BEIR Mr. Tydi MIRACL Amazon MASSIVE dataset We compared our results against other state-of-the-art multilingual embedding models, specifically paraphrase-multilingual-mpnet-base-v2 (the best model from Sentence-Transformers ), LaBSE (from Google), and Universal Sentence Encoder cMLM (from Google). The following chart shows how they compare: The Cohere multilingual-22-12 model performs much better in all use cases. In particular, we see a robust improvement in multilingual search. The other models we tested against perform rather poorly, in many cases less effectively than keyword search.*****Token likelihood is a useful tool for model evaluation. For instance, let's say you've trained a custom model and would like to know how much it's improved over the default model - you could use token likelihoods to compare the performance of the models on some held-out text. Here is a quick demonstration of how to use the return_likelihoods parameter from the Generate endpoint for model evaluation.*****join the conversation on Discord. Stay tuned for more episodes in our Talking Language AI series!*****Sentence Transformers is one of the most popular Language AI/NLP tools. Tens of thousands of users rely on it to build systems for text classification, neural/semantic search, text clustering, and other language AI tasks. In Episode 3 of our series on applied NLP topics, tools, and people, we take a deep dive into this important tool with Nils Reimers, our Director and Principal Scientist of Machine Learning at Cohere. View the full episode (also embedded below). Feel free to post questions or comments in the thread on this episode in the Cohere Discord channel. Nils is the creator of Sentence-BERT and has authored several well-known research papers, including Sentence-BERT and the popular Sentence Transformers library.*****He’s also worked as a Research Scientist at HuggingFace, (co-)founded several web companies, and worked as an AI consultant in the area of investment banking, media, and IoT. In our conversation, Nils gives us an introduction to the Sentence-BERT package and the large language models provided in it. He also shares some lessons from his experience in open-source development of such a popular package. Finally, Nils touches on his research collaborations on how to evaluate embeddings through works like MTEB: Massive Text Embedding Benchmark and BEIR. To go deeper into these tools, and other concepts around embeddings, watch the video and*****So we can scale the number of exemplars with linear complexity instead of quadratic complexity with respect to length. Experimental results on a diverse set of tasks show that our approach improves end-task performance and reduces evaluation variance over conventional in-context learning as the number of demonstration examples increases. Code has been released at this URL . Parallel Context Windows Improve In-Context Learning of Large Language Models Authors: Nir Ratner, Yoav Levine, Yonatan Belinkov, Ori Ram, Omri Abend, Ehud Karpas, Amnon Shashua, Kevin Leyton-Brown, Yoav Shoham For applications that require processing large amounts of text at inference time, Large Language Models (LLMs) are handicapped by their limited context windows, which are typically 2048 tokens.*****Cohere's multilingual text understanding model is now available! The multilingual-22-12 model can be used to semantically search within a single language, as well as across languages. Compared to keyword search, where you often need separate tokenizers and indices to handle different languages, the deployment of the multilingual model for search is trivial: no language-specific handling is needed — everything can be done by a single model within a single index. In addition to our new model, you can now detect the language of a data source using co.detect_language() endpoint. For more information, see our multilingual docs .*****## How Does the Multilingual Text Understanding Model Work? Cohere’s multilingual text understanding model maps text to a semantic vector space (also known as “ embeddings ”), positioning texts with a similar meaning in close proximity. This process unlocks a range of valuable use cases for multilingual settings. For example, one can map a query to this vector space during a search to locate relevant documents nearby. This often yields search results that are several times better than keyword search. To train multilingual models, you need large quantities (hundreds of millions) of suitable training pairs, like question/answer pairs. So far, such training data has been primarily available in English, and prior work tried to use machine translation to map it to other languages.*****## What is a Multilingual Text Understanding Model? Multilingual text understanding models are powerful models that can derive insights from text data across languages. At Cohere, we’ve trained our model specifically to be used for and While many of these models are available for English, similar existing multilingual models only work well for short sentences and can’t capture the meaning behind longer text. This prevents them from being used for semantic search, which typically aims to match a short query with a longer, relevant document. In this blog post, we will cover three relevant use cases that showcase the power of Cohere’s new multilingual model:*****### Develop, test, and experiment with the industry’s first multilingual text understanding model that supports 100+ languages Humans speak over 7100 languages, yet the majority of language models only support the English language. This makes it incredibly challenging to build products and projects using multilingual language understanding. Cohere’s mission is to solve that by empowering our developers with technology that possesses the power of language. That’s why today we’re introducing our first multilingual text understanding model that supports over 100 languages and delivers 3X better performance than existing open-source models. This will enable new markets, countries, and global companies to better serve their customers across the globe.*****The multilingual-22-12 model can be used to semantically search within a single language, as well as across languages. Compared to keyword search, where you often need separate tokenizers and indices to handle different languages, the deployment of the multilingual model for search is trivial: no language-specific handling is needed — everything can be done by a single model within a single index. We’re extremely proud of the performance of our multilingual understanding model. It outperformed the industry standard (the next best model) in search tasks by more than 230%.*****## Getting Started with Cohere’s Multilingual Model To get started using Cohere’s multilingual model, just create a free account and get your API key . You can then either query our REST API endpoints or install our SDK to use the model within Python. The following video navigates through the Cohere Platform to select the multilingual model, and shows how the multilingual model can embed text from multiple languages into the embedding space. Additionally, we have added to the Cohere Sandbox , a collection of experimental, open-source GitHub repositories  that make building applications using large language models fast and easy with Cohere.*****This is what makes the Cohere multilingual model so powerful: it has seen thousands of topics in each language.*****## Training Data Training embedding models require data in a specific format. For example, question/answer pairs or title/document pairs. We can then learn which text pairs should be closer together in the vector space in order to enable applications like semantic search. To train Cohere’s new multilingual model, we processed and carefully cleaned terabytes of data from various sources: Wikipedia, news publications, scientific articles, and online communities across hundreds of languages. This resulted in a large training corpus of more than 900 million training pairs for English and 450 million training pairs for other languages. Other multilingual embedding models often rely on machine translation for training dataset creation, which creates an awkward bias for these models.*****## Use Cases Here are a few examples of language understanding systems that can be built on top of large language models.*****### Classify Classification is one of the most common use cases in language processing. Building systems on top of language models can automate language-based tasks and save time and energy. There's more than one way to build a classifier on top of Cohere's language models. It's worth experimenting to see which method works best for your use case. The simpler methods can get you quick results, while the more advanced methods need more data and will lead to better results. On the simpler side are methods like using the Classify endpoint for classification . More industrial-grade classifiers can be built by fitting a classifier on top of the embed endpoint (see:*****## Use Cases for Embeddings What can you do with embeddings? Quite a lot. Our Embed endpoint takes a piece of text and creates a vector embedding, which represents the text as numbers that capture its meaning and context. Embedding transforms unstructured text data into a structured form that allows you to cluster, categorize, and semantically (contextually) search the text. Cohere’s powerful language models can find relationships between pieces of text (words, phrases, sentences, paragraphs, or documents) that would not surface with linear keyword search. Some examples of industries that could benefit from vector search include: Overall, any industry that relies on large amounts of information, and the ability to quickly and accurately search for that information, could benefit from vector search.*****The Cohere platform builds natural language understanding and generation into your product with a few lines of code. Our large language models can solve a broad spectrum of natural language use cases, including classification, semantic search, paraphrasing, summarization, and content generation. By training a custom model , users can customize large language models to their use case and trained on their data. The models can be accessed through the playground , SDK and the CLI tool.*****### Building a Chatbot with a Persona This is an example of a conversational use case. Try the preset here . If you are looking for more examples, visit this page to get more use case ideas.*****### Extracting Keywords from Emails This is an example of an extraction use case. Try the preset here .*****### Summarize Large language models present a breakthrough in text generation. For the first time in history, we have software programs that can write text that sounds like it’s written by humans. These capabilities open doors to use cases like summarization or paraphrasing. A summarization prompt in the Cohere playground shows this output (in bold): Large language models can be adapted to new tasks with impressive speed. For tasks which appear in the training data (i.e. documents on the web), language models can successfully summarize text without being shown any examples at all. Summarization and paraphrasing both use the generate endpoint.*****## Example Use Cases A use case would typically emerge at the intersection of the three areas we’ve looked at. Let’s look at a few examples. These examples are screenshots from the Cohere Playground . Each example comes with a preset link, which you can experiment with for yourself. This list is by no means exhaustive, but it is meant to illustrate the breadth of areas we discussed in the previous sections.*****### Summarizing Chat Transcripts This is an example of a summarization use case. Try the preset here .*****To understand how model prompting works, let’s start by entering the following prompt into the playground. Once upon a time in a magical land called Clicking Generate gives us a continuation of the text (model-generated text is in ). This is the most basic form of prompting, which is simply asking the model to complete the text that we have entered. But this type of prompt is rather open-ended, and in more practical applications, you will need to make the prompt tighter, so that the output generated will be more predictable. With that, let’s now dive into how you can design more effective prompts.*****## Selecting the Model We have only used one model so far: command-xlarge-nightly . But as we discussed in Part 1, you can prompt Cohere’s text generation model in two ways: by instruction and by example. So, the first thing that you want to define when calling the endpoint is the model type, depending on how you are constructing your prompt. Here are the available models at the time of writing: command-xlarge-nightly command-medium-nightly xlarge medium The sizes implied in the model names represent the parameter size of the models. So, which one do you choose? It depends on your use case, but as a rule of thumb, smaller models are faster, while larger models are generally more fluent and coherent.*****At Cohere, our focus is on language. We want to enable developers to add language AI to their technology stack and build impactful applications with it. In this multi-part guide, we will go through everything that you need to know about generative AI with Cohere’s large language models (LLMs). Here is what we’ll cover throughout this series: Part 1: Model Prompting Part 2: Use Case Ideation Part 3: The Generate Endpoint Part 4: Creating Custom Models In this Part 1 article, we will cover the following topics: Getting Started with the Cohere Playground Prompting the Models Controlling the Model Output Saving and Sharing Your Prompts*****### 2. Try multiple formulations of your prompt to get the best generations When using generate, it is useful to try a range of different prompts for the problem you are trying to solve. Different formulations of the same prompt which might sound similar to humans can lead to generations that are quite different from each other. This might happen, for instance, because our models have learned that the different formulations are actually used in very different contexts and for different purposes. Below we give a number of examples that we've found to work particularly well for different tasks. In the summarization example, if “In summary,” doesn’t lead to a good generation, we may want to try “To summarize in plain language,“ or “The main point to take from this article is that”.*****### Being Creative vs. Predictable Probably the most useful set of parameters are the ones that we can tune to control the randomness of the output. The beauty of working with LLMs is, for the same prompt, the next generated token will not be the same every time. Rather, it is sampled from a long list of possible tokens. This is where the creative aspect of LLM comes from, allowing us to generate a variety of outputs, given the exact same prompt. But depending on your application, you may want to reduce, or increase, this level of randomness. You can do this by adjusting a number of parameters.*****### Prompt Design Prompting is at the heart of working with LLMs. The prompt provides a context for the text that we want the model to generate. The prompts we create can be anything from simple instructions to more complex pieces of text, and they are used to encourage the model to produce a specific type of output. Coming up with a good prompt is a bit of both science and art. On the one hand, we know the broad patterns that enable us to construct a prompt that will generate the output that we want. But on the other hand, there is so much room for creativity and imagination in coming up with prompts that can get the best out of a model.*****## 3. Extract tags from these articles We now proceed to the tags extraction step. Compared to the previous two steps, this step is not about sorting or filtering articles, but rather enriching them with more information. We do this by prompting Cohere’s Generate endpoint with a few examples of text and its tags. We then feed the articles from the classifier step and the endpoint will generate the corresponding tags. There is more than one way to construct the prompt, depending on what you'd like to extract. In my case, the tags I'd like to extract are primarily the names of a person, company, or organization, and perhaps also some generic keywords.*****### Prompting by Instruction The Command-Xlarge model works best when we provide an instruction-based prompt. One way to do this is by using imperative verbs to tell the model what to do, for example: generate, write, list, provide, and other variations. Let’s say that we are creating social media ad copy for a wireless earbuds product. We can write the prompt as follows. Generate a social ad copy for the product: Wireless Earbuds. At this point, ensure that you select command-xlarge in the MODEL dropdown in the right pane. Then, click on Generate. This generates the following output. That’s not bad.*****#### Generate Produce text completions based on a given input. Model Price per Unit Default $2.5 per 1000 Generation Units* Custom $5.0 per 1000 Generation Units* More details*****#### Embed Capture the semantic meaning of text by representing text as numbers. Model Price per Unit Default $1.0 per 1000 Embeddings Custom $2.0 per 1000 Embeddings More details*****#### Summarize Summarize a given input text such as an article into a short paragraph or key bullet points Model Price per Unit Default $5.0 per 1000 Summarization Units* Custom $10.0 per 1000 Summarization Units* More details API Key Offering API Key Offering*****#### Classify Analyze and organize text into specific categories. Model Price per Unit Default $2.0 per 1000 Classifications Custom $2.0 per 1000 Classifications More details*****## Totally transparent pricing We're making it easy to explore, learn, and experiment with the Cohere Platform. All models Free Limit of 100 calls/minute Default models $ 1.00 per 1000 Embeddings Custom models $ 2.00 per 1000 Embeddings Examples Stay updated*****#### Frequently Asked Questions 1. How do I get a Trial API Key? When an account is created, we automatically create an Trial API key for you. This API key will be available on the dashboard for you to copy, as well as in the dashboard section called “API Keys.” 2. How do I get a Production API key? To get a Production key, you'll need to have Owner privileges (or ask your organization Owner to complete the following steps). Navigate to the Billing and Usage page in your Cohere dashboard. Click on the button and fill out the Go to Production workflow.*****Today, we’re thrilled to announce an entirely reimagined platform pricing structure designed to provide more flexibility, control, and value to every developer and business building on the Cohere Platform. Our new pricing opens up the platform to allow you greater access and a simpler experience when choosing the right resources for your project. You’ll be able to experiment more for free, estimate costs more easily, and see less jargon from us. Read on for the full details! 1. Natural language processing (NLP) has become part of the public consciousness due to its rapid evolution and increasing number of applications. We want to make it easier for every developer to explore the vast potential of NLP and experiment with it on the Cohere Platform.*****Other Questions What is the difference between a Trial API key and Production API key? Are there any account limitations upon signup? What is the difference between an organization and a personal account? Which model should I pick? When do I get billed?*****Prompt engineering is a fascinating topic. It is about figuring out the optimal way to prompt a model for a particular task, so we can shape the output to be how we want it to be. In this example, we have a startup idea generator. We want the endpoint to generate a startup idea and its name, given an industry/vertical as the input.*****Artificial intelligence (AI) is the simulation of human intelligence processes by machines, especially computer systems. Artificial intelligence (AI) is used in many applications today, including voice recognition, self-driving cars, and even some household appliances. AI is also used in video games, where it can control the behavior of non-player characters in order to create more realistic interactions between the player and the game world. We call the Generate endpoint via the co.generate() method to generate the potential links for each of these two notes. We won’t cover the details here, but if you’d like to understand what the parameters used in this method mean, you can read about them in the*****Given a simple prompt, such as “cave monster,” the app uses Cohere Generate to create an image, description, and even a backstory for a new creature. The team built their app during a Cal Hacks 9.0 hackathon. See their GitHub repo .*****This endpoint generates a succinct version of the original text that relays the most important information. Ideal use cases include, but are not limited to: news articles, blogs, chat transcripts, scientific articles, meeting notes, and any text that you should like to see a summary of! The endpoint can: Summarize a single document Control output length 🚧 Experimental Features These features are extremely experimental. Using these feature could lead to a substantial decrease in performance over the overall model. It is included as a feature based on user feedback — and our team is actively working on delivering a better solution.*****Given an article, the user and the AI bot can discuss the article in the form of a debate. The app uses Cohere’s Generate and Classify endpoints. See his GitHub repo . Co:here Chat by Jonathan Fernandes — Jonathan wanted to improve the efficiency of child-based support systems by automating repetitive parts of the workflow, including summarization, sentiment analysis, log creation, and duration. His app uses Cohere’s Generate , Classify , and Embed endpoints, as well as the sandbox-conversant-lib library currently in development in the Cohere Sandbox. See his GitHub repo .*****In this example, we want to summarize a passage from a news article into its main point. Install the SDK, if you haven't already.*****The app allows users to enter their favorite courses in order to generate topics better suited to their interests. He built the app on Streamlit and used the Cohere Generate endpoint and X-Large language model. See his GitHub repo . SuperTransformer by Amir Nagri and Team Megatron–Amir and team built an AI-assisted email inbox manager that helps busy people organize incoming emails based on urgency and importance. Using Cohere Classify, the app categorizes emails and adds an appropriate action label. The team also built their app during a lablab hackathon. See their GitHub repo . SCRIPTure by Arnav Kartikeya and Satyajith Bavisetty, Atharva Gupta–Inspired by Dungeons & Dragons, this app is a fantasy creature generator that helps creative writers develop new characters that fit their storylines.*****The student models can even outperform the teacher in some tasks while reducing model size requirements by several orders of magnitude. They conduct extensive ablation studies and sample studies to understand the reasoning capabilities of student models, and identify several important nuances that have been overlooked in concurrent fine-tuning works on CoT. SparseGPT: Massive Language Models Can Be Accurately Pruned in One-Shot Authors: Elias Frantar, Dan Alistarh This paper presents a new pruning method called SparseGPT that can reduce the number of weights in large-scale generative pre-trained transformer (GPT) models by at least 50% without any retraining and minimal loss of accuracy.*****Last week, our CEO and co-founder Aidan Gomez was featured on stage at the Elevate Festival in Toronto. He was joined by Stephen Marche, a novelist and essayist who has written about NLP for The New Yorker, The Atlantic and The New York Times. Aidan and Stephen sat down for a discussion on language technology and what the future holds. In case you missed it, here’s part of their conversation: : My “holy shit” moment using language models was when I was writing a piece for The New Yorker. I asked GPT-3 to finish famous unfinished poems, and they worked. They truly sounded like Coleridge or Shakespeare.*****The experiments in the paper demonstrate that this approach can significantly improve task performance, for example, increasing the accuracy of a smaller model on a dataset called GSM8K from 8.11% to 21.99% when finetuned on PaLM-540B generated chains of thought. Large Language Models Are Reasoning Teachers Authors: Namgyu Ho, Laura Schmid, Se-Young Yun This paper explores a method for transferring reasoning capabilities from large language models to smaller models through fine-tuning. The authors propose "Fine-tune-CoT," a method that leverages the capabilities of very large language models (such as GPT-3) to generate reasoning samples and teach smaller models. They evaluate their method on publicly available language models across a wide range of complex tasks and model sizes and find that Fine-tune-CoT enables substantial reasoning capability in small models, whereas previous prompt-based baselines exhibit near-random performance.*****In the #grounded-qa-bot channel in the Cohere co:mmunity on Discord , there’s a bot that will answer your questions. Ask any question in the chat, add the question mark emoji, and the co_search bot will attempt to answer the question. This answer, however, is not simply a GPT model’s output to an input prompt. It is informed by a web search. There have been recent large language models (LLMs) from research labs that are trained on using a database or have the ability to search the web for information. Unlike those custom models, this is a bot that you, a developer who does not necessarily work at a giant tech company, can build today.*****By training over 400 language models ranging from 70 million to over 16 billion parameters on 5 to 500 billion tokens, we find that for compute-optimal training, the model size and the number of training tokens should be scaled equally: for every doubling of model size the number of training tokens should also be doubled. We test this hypothesis by training a predicted compute-optimal model, Chinchilla, that uses the same compute budget as Gopher but with 70B parameters and 4× more data. Chinchilla uniformly and significantly outperforms Gopher (280B), GPT-3 (175B), Jurassic-1 (178B), and Megatron-Turing NLG (530B) on a large range of downstream evaluation tasks.*****### Tempering excitement with care As social media gets swept up in posts that claim “ I made model X do impossible task Y 🤯 ”, it’s important to arm oneself with a discerning eye to filter these claims. One of the key questions to ask is whether a demonstrated capability is a 🍒 cherry-picked example that a model produces 40% of the time, or if it points to robust and reliable model behavior. Reliability is key for an AI capability to become part of a customer-facing product. Take for instance, the many capabilities attributed to large GPT models in the last few years.*****MTEB: Massive Text Embedding Benchmark Authors: Niklas Muennighoff, Nouamane Tazi, Loïc Magne, Nils Reimers There's a problem with the way people are evaluating text embeddings. Right now, people are only testing a small set of data from one task. This makes it hard to know if the text embeddings will work well for other tasks, like clustering or reranking. To solve this problem, the authors created the Massive Text Embedding Benchmark (MTEB). MTEB spans 8 embedding tasks covering a total of 56 datasets and 112 languages. Through the benchmarking of 33 models on MTEB, they were able to establish the most comprehensive benchmark of text embeddings to date.*****He’s also worked as a Research Scientist at HuggingFace, (co-)founded several web companies, and worked as an AI consultant in the area of investment banking, media, and IoT. In our conversation, Nils gives us an introduction to the Sentence-BERT package and the large language models provided in it. He also shares some lessons from his experience in open-source development of such a popular package. Finally, Nils touches on his research collaborations on how to evaluate embeddings through works like MTEB: Massive Text Embedding Benchmark and BEIR. To go deeper into these tools, and other concepts around embeddings, watch the video and*****## Benchmarks We extensively benchmarked our new model to ensure the best performance across a wide range of applications, domains and languages. Specifically, we used: MTEB BEIR BEIR Mr. Tydi MIRACL Amazon MASSIVE dataset We compared our results against other state-of-the-art multilingual embedding models, specifically paraphrase-multilingual-mpnet-base-v2 (the best model from Sentence-Transformers ), LaBSE (from Google), and Universal Sentence Encoder cMLM (from Google). The following chart shows how they compare: The Cohere multilingual-22-12 model performs much better in all use cases. In particular, we see a robust improvement in multilingual search. The other models we tested against perform rather poorly, in many cases less effectively than keyword search.*****join the conversation on Discord. Stay tuned for more episodes in our Talking Language AI series!*****Sentence Transformers is one of the most popular Language AI/NLP tools. Tens of thousands of users rely on it to build systems for text classification, neural/semantic search, text clustering, and other language AI tasks. In Episode 3 of our series on applied NLP topics, tools, and people, we take a deep dive into this important tool with Nils Reimers, our Director and Principal Scientist of Machine Learning at Cohere. View the full episode (also embedded below). Feel free to post questions or comments in the thread on this episode in the Cohere Discord channel. Nils is the creator of Sentence-BERT and has authored several well-known research papers, including Sentence-BERT and the popular Sentence Transformers library.*****Now, compare them to the other kinds of inquiries, such as those related to airline information (see two examples below). Notice that while the embeddings about ground transportation inquiries look very similar to each other, they are distinctive from the rest. Here, the model was able to capture the context and meaning of each piece of text and it then represents them as embeddings. Each dimension of an embedding, called a feature , represents a certain universal characteristic about text according to how the model understands it. How is this possible? A large language model has been pre-trained with a vast amount of text data, where the training objective is set up in such a way to encourage the model to extract contextual information about a piece of text and store it as embeddings.*****# How It Works Embeddings are numerical representations of meaning in text. Because they are numbers, they can be compared to each other for similarity. They can also be plotted on a chart that shows which texts are similar to each other. Large language models produce highly nuanced embeddings.*****Prompt engineering is a fascinating topic. It is about figuring out the optimal way to prompt a model for a particular task, so we can shape the output to be how we want it to be. In this example, we have a startup idea generator. We want the endpoint to generate a startup idea and its name, given an industry/vertical as the input.*****To understand how model prompting works, let’s start by entering the following prompt into the playground. Once upon a time in a magical land called Clicking Generate gives us a continuation of the text (model-generated text is in ). This is the most basic form of prompting, which is simply asking the model to complete the text that we have entered. But this type of prompt is rather open-ended, and in more practical applications, you will need to make the prompt tighter, so that the output generated will be more predictable. With that, let’s now dive into how you can design more effective prompts.*****### Prompt Design Prompting is at the heart of working with LLMs. The prompt provides a context for the text that we want the model to generate. The prompts we create can be anything from simple instructions to more complex pieces of text, and they are used to encourage the model to produce a specific type of output. Coming up with a good prompt is a bit of both science and art. On the one hand, we know the broad patterns that enable us to construct a prompt that will generate the output that we want. But on the other hand, there is so much room for creativity and imagination in coming up with prompts that can get the best out of a model.*****## Conclusion In this article, we covered how to prompt a model — probably the most important, and definitely the most fun part of working with large language models. If you’d like to dive deeper into it, here are some resources you can go to for further reading: designing a prompt text generation Generate API reference tips and tricks In Part 2 , we will explore the range of use cases and areas where generative AI in language can be applied. And along the way, we’ll see the different ways a prompt can be designed. Cohere account*****At Cohere, our focus is on language. We want to enable developers to add language AI to their technology stack and build impactful applications with it. In this multi-part guide, we will go through everything that you need to know about generative AI with Cohere’s large language models (LLMs). Here is what we’ll cover throughout this series: Part 1: Model Prompting Part 2: Use Case Ideation Part 3: The Generate Endpoint Part 4: Creating Custom Models In this Part 1 article, we will cover the following topics: Getting Started with the Cohere Playground Prompting the Models Controlling the Model Output Saving and Sharing Your Prompts*****### Prompt Engineering Dive deeper into the key principles and techniques for writing prompts that generate the copy you need. Stay updated*****You can keep layering your instructions to be as specific as you want, and see the output generated by the model. And there is really no right or wrong way to design a prompt. It’s really about applying an idea and continuing to iterate the prompt until you get the outcome you are looking for.*****### The Two Types of Generative Models How a prompt is designed is dependent on the type of model you are using. There are two types of generative models available on the Cohere Platform: one where you prompt by instruction and another where you prompt by example . In summary, this is how the two types of models differ: Command-Xlarge XLarge Medium Now let’s learn more about designing these two types of prompts.*****### Prompting by Example Sometimes you want the model output to be more attuned to a specific pattern or nuance that you have in mind. For this, a better option might be to use the XLarge or Medium model and prompt it by example instead of by instruction. For example, say you want to generate product descriptions for a long list of products. You want each of the descriptions to be of similar length. And for each product description, you want to be able to input a few keywords to guide the content of the descriptions. A basic prompt format that generally works well is as follows.*****Cohere offers an API to add cutting-edge language processing to any system. Cohere trains massive language models and puts them behind a simple API. Moreover, through training, users can create massive models customized to their use case and trained on their data. This way, Cohere handles the complexities of collecting massive amounts of text data, the ever evolving neural network architectures, distributed training, and serving models around the clock. Two major categories of large language models are generative language models (like GPT2 and GPT3) and representation language models (like BERT). Cohere offers variants of both types.*****The best and brightest minds in machine learning transcend borders. That’s why we’re excited to announce Cohere For AI , a non-profit research lab and community dedicated to contributing fundamental research in machine learning, working to solve some of the field's most challenging problems. As part of our work, we're committed to fostering an inclusive and collaborative environment that creates more points of entry to participate in machine learning research. We’re excited to be able to revive the spirit of the original for.ai, an international research group founded by the two of us alongside Bryan Li and Sheldon Huang – just a few undergrads (+ a dropout) – as a way to contribute to machine learning research.*****📘 New to Cohere? Get Started now and get unprecedented access to world-class Generation and Representation models with billions of parameters.*****That same passion lives on through Cohere For AI, with an even more focused team and mission. We’re thrilled to welcome Sara Hooker to lead this effort as Head of Cohere For AI. With a long track-record of impactful research at Google Brain, she brings a wealth of knowledge from across machine learning. She also founded Delta Analytics, a non-profit that brings together researchers, data scientists, and software engineers to volunteer their skills for non-profits around the world. Throughout her career, Sara has led as a researcher, an educator, and a community builder – it’s such a privilege to have her leading Cohere For AI.*****## Get Started with Cohere Summarize API Accessing the Cohere Summarize API is easy and user-friendly. You can start exploring the API right away by accessing our Summarize playground , where you can input your own text and receive an instant summary generated by our language model. Alternatively, if you're a developer looking to integrate the Cohere Summarize API into your own application or workflow, you can access our documentation , which provides detailed instructions and code samples. You can also visit our Summarize product page . With our straightforward and accessible tools, getting started with Cohere Summarize has never been easier.*****The machine learning field currently has too few points of entry, especially depending on where you are in the world. With Cohere For AI, Sara’s vision is to change how, where and by whom research is done. Cohere For AI represents the opportunity to make an impact in ways that don’t just advance progress on machine learning research, but also create new points of entry into the field. Our lab will focus on solving today’s complex machine learning challenges––exploring the unknown, together. Our values will be based on the following principles: GitHub Cohere For AI is community-driven and motivated by the opportunity to create an inclusive, distributed community made up of brilliant researchers and engineers from across the globe.*****### Cohere gives every developer easy access to NLP We’ve made an API that works with every stack. No matter your level of developer experience, the Cohere Platform makes it easy to integrate machine learning into your applications and systems with our Python, Node, and Go SDKs. Our versatile NLP platform offers endpoints for generation, classification, or embedding text data at massive scale. Developers can then use Cohere's endpoints to handle specific tasks, such as text analysis, text classification, and topic labeling. Our platform can be plugged into any library, giving every developer access to NLP.*****## Unlock the power of NLP with the Cohere Platform Today we are announcing the general availability of Cohere’s natural language processing (NLP) platform. We've built a powerful, easy-to-deploy collection of APIs and tools designed for developers who want to create websites and apps that can read, write, and understand human language. We believe that broadening access to large language models (LLMs) will reduce the barriers to developing powerful product experiences rooted in language, shaping the future of how we interact with technology. Created for developers by developers, the Cohere platform is easily deployed with a few lines of code and is designed to make it easy to experiment, customize, and deploy NLP technology into your stack.*****### Summarize Large language models present a breakthrough in text generation. For the first time in history, we have software programs that can write text that sounds like it’s written by humans. These capabilities open doors to use cases like summarization or paraphrasing. A summarization prompt in the Cohere playground shows this output (in bold): Large language models can be adapted to new tasks with impressive speed. For tasks which appear in the training data (i.e. documents on the web), language models can successfully summarize text without being shown any examples at all. Summarization and paraphrasing both use the generate endpoint.*****Semantic search no longer restricts itself to queries and documents in the same language but also works across languages. For example, if we phrase the search query in Arabic (“ما هي عاصمة الولايات المتحدة؟”), we get the same results, while keyword search can obviously not retrieve any relevant documents. This enables interesting use cases, for example, in the financial domain, to quickly find relevant information irrespective of the language in which they have been published.*****## Use Cases Multilingual Semantic Search : Improve your search results regardless of the language. Aggregate Customer Feedback : Organize customer feedback across hundreds of languages, simplifying a major challenge for international operations. Cross-Lingual Zero-Shot Content Moderation : Identify harmful content in online communities is challenging, especially as users speak hundreds of languages. Train a model with a few English examples, then detect harmful content in 100+ languages.*****Cohere's multilingual text understanding model is now available! The multilingual-22-12 model can be used to semantically search within a single language, as well as across languages. Compared to keyword search, where you often need separate tokenizers and indices to handle different languages, the deployment of the multilingual model for search is trivial: no language-specific handling is needed — everything can be done by a single model within a single index. In addition to our new model, you can now detect the language of a data source using co.detect_language() endpoint. For more information, see our multilingual docs .*****The multilingual-22-12 model can be used to semantically search within a single language, as well as across languages. Compared to keyword search, where you often need separate tokenizers and indices to handle different languages, the deployment of the multilingual model for search is trivial: no language-specific handling is needed — everything can be done by a single model within a single index. We’re extremely proud of the performance of our multilingual understanding model. It outperformed the industry standard (the next best model) in search tasks by more than 230%.*****The multilingual flexibility of semantic search enables interesting use cases in industries like finance, where users need to quickly find information that may be published across multiple languages.*****### Single Model Environment Use a single multilingual model to handle both english and non-english queries. Identify the language of an incoming query and filter the results of your results by matching languages for monolingual retrieval with a multilingual model - in addition, you can specify which languages you want to filter for in a cross-lingual retrieval setup.*****By contrast, search quality can be greatly improved by using semantic search powered by Cohere’s multilingual model. In this example, the relevant article about Washington, D.C., is ranked at the top position of the search results. And such a gain in search quality is found not only with English queries, but also for a wide range of languages that we have tested (see benchmark results ). Semantic search does not restrict itself to queries and documents in the same language, but it also works across languages. For example, if we phrase the search query in Arabic (“ما هي عاصمة الولايات المتحدة؟”), we get the same results, while keyword search can obviously not retrieve any relevant documents.*****## What is a Multilingual Text Understanding Model? Multilingual text understanding models are powerful models that can derive insights from text data across languages. At Cohere, we’ve trained our model specifically to be used for and While many of these models are available for English, similar existing multilingual models only work well for short sentences and can’t capture the meaning behind longer text. This prevents them from being used for semantic search, which typically aims to match a short query with a longer, relevant document. In this blog post, we will cover three relevant use cases that showcase the power of Cohere’s new multilingual model:*****A lot of existing English training data has a focus on topics that are primarily interesting for U.S. citizens, for example, how to fill out specific U.S. tax forms. If these question/answer pairs are then translated into another language, e.g., Korean, the model learns in Korean how to file taxes in the U.S., but it doesn’t learn how to file taxes in Korea, a topic that is likely more relevant for Korean citizens. This makes prior models rather suboptimal for multilingual semantic search, as they don’t capture country-specific interests well. Our training process included actual authentic question/answer pairs from users across hundreds of languages from tens of thousands of websites from hundreds of countries.*****Customers who are part of the AWS Enterprise Discount Program can also draw down on their commitment by purchasing Cohere’s solutions through the AWS Marketplace. To get you started, we’re offering $75 in free credits for you and your team to use Cohere’s Playground and API. That’s enough to generate as many words as Shakespeare’s complete works (heigh-ho!). Simply sign in to your AWS account and select the Cohere Access Plan.*****So, we’re replacing our current $75 free credit program with a new freemium tier that gives you more hands-on experience with our API. To enable the free tier, we’re introducing two types of API keys that give users access to all of Cohere’s endpoints: Trial API keys and Production API keys. All current API keys will automatically be converted to a Trial API key, which is rate-limited to 100 API calls per minute and cannot be used in production scenarios. by signing up for an account . For developers looking for higher rate limits, or to serve Cohere to users in an application, you can upgrade to a Production API key where the throughput is 100 times higher (10,000 calls per minute).*****### Why Cohere Free Developer Tier Learn and iterate with the Cohere API free of charge until you go to production. Easy To Use No prior ML/AI experience required. Get started with just a few examples. Customizable Models Customize our models with your own data sets, and deploy them easily to production.*****## What’s Next? While LLMs are relatively new and incredibly powerful, not everyone has first-hand access to them. By sharing experimental code as part of Cohere's Sandbox, we want to empower you with the right open-source tools to discover what's possible with language AI and help you build and deploy, fast. This is just the beginning for Sandbox, as we plan to continue releasing new hands-on repositories to help with your use cases and deliver know-how and inspiration, over time. If you're feeling creative, consider supporting our open-source efforts by forking and experimenting with the projects, and feel free to share them with your friends and colleagues.*****You can request immediate access to Cohere’s NLP platform here . We are currently offering new users free credits for up to 300 million characters, usable over their first three months on the Cohere platform. More information on pricing can be found here .*****Today, we’re thrilled to announce an entirely reimagined platform pricing structure designed to provide more flexibility, control, and value to every developer and business building on the Cohere Platform. Our new pricing opens up the platform to allow you greater access and a simpler experience when choosing the right resources for your project. You’ll be able to experiment more for free, estimate costs more easily, and see less jargon from us. Read on for the full details! 1. Natural language processing (NLP) has become part of the public consciousness due to its rapid evolution and increasing number of applications. We want to make it easier for every developer to explore the vast potential of NLP and experiment with it on the Cohere Platform.*****Semantic search no longer restricts itself to queries and documents in the same language but also works across languages. For example, if we phrase the search query in Arabic (“ما هي عاصمة الولايات المتحدة؟”), we get the same results, while keyword search can obviously not retrieve any relevant documents. This enables interesting use cases, for example, in the financial domain, to quickly find relevant information irrespective of the language in which they have been published.*****## Use Cases Multilingual Semantic Search : Improve your search results regardless of the language. Aggregate Customer Feedback : Organize customer feedback across hundreds of languages, simplifying a major challenge for international operations. Cross-Lingual Zero-Shot Content Moderation : Identify harmful content in online communities is challenging, especially as users speak hundreds of languages. Train a model with a few English examples, then detect harmful content in 100+ languages.*****Cohere's multilingual text understanding model is now available! The multilingual-22-12 model can be used to semantically search within a single language, as well as across languages. Compared to keyword search, where you often need separate tokenizers and indices to handle different languages, the deployment of the multilingual model for search is trivial: no language-specific handling is needed — everything can be done by a single model within a single index. In addition to our new model, you can now detect the language of a data source using co.detect_language() endpoint. For more information, see our multilingual docs .*****The multilingual flexibility of semantic search enables interesting use cases in industries like finance, where users need to quickly find information that may be published across multiple languages.*****The multilingual-22-12 model can be used to semantically search within a single language, as well as across languages. Compared to keyword search, where you often need separate tokenizers and indices to handle different languages, the deployment of the multilingual model for search is trivial: no language-specific handling is needed — everything can be done by a single model within a single index. We’re extremely proud of the performance of our multilingual understanding model. It outperformed the industry standard (the next best model) in search tasks by more than 230%.*****## Benchmarks We extensively benchmarked our new model to ensure the best performance across a wide range of applications, domains and languages. Specifically, we used: MTEB BEIR BEIR Mr. Tydi MIRACL Amazon MASSIVE dataset We compared our results against other state-of-the-art multilingual embedding models, specifically paraphrase-multilingual-mpnet-base-v2 (the best model from Sentence-Transformers ), LaBSE (from Google), and Universal Sentence Encoder cMLM (from Google). The following chart shows how they compare: The Cohere multilingual-22-12 model performs much better in all use cases. In particular, we see a robust improvement in multilingual search. The other models we tested against perform rather poorly, in many cases less effectively than keyword search.*****## Differences Between English and Multilingual Embedding Models Unlike our English language embedding model, our multilingual model was trained using dot product calculations. Using dot products produces a non-normalized similarity score, reflecting the magnitude of the two compared vectors. When this dimension is incorporated, multilingual embeddings perform better than standard. For more information on how our English language model works (using cosine similarity), see our introductory guide to the Cohere platform . The dimensions of our multilingual embeddings is 768 dimensions.*****The multilingual-22-12 model can be used to semantically search within a single language, as well as across languages. Compared to keyword search, where you often need separate tokenizers and indices to handle different languages, the deployment of the multilingual model for search is trivial: no language-specific handling is needed — everything can be done by a single model within a single index. We’re extremely proud of the performance of our multilingual understanding model. It outperformed the industry standard (the next best model) in search tasks by more than 230%.*****## How Does the Multilingual Text Understanding Model Work? Cohere’s multilingual text understanding model maps text to a semantic vector space (also known as “ embeddings ”), positioning texts with a similar meaning in close proximity. This process unlocks a range of valuable use cases for multilingual settings. For example, one can map a query to this vector space during a search to locate relevant documents nearby. This often yields search results that are several times better than keyword search. To train multilingual models, you need large quantities (hundreds of millions) of suitable training pairs, like question/answer pairs. So far, such training data has been primarily available in English, and prior work tried to use machine translation to map it to other languages.*****## What is a Multilingual Text Understanding Model? Multilingual text understanding models are powerful models that can derive insights from text data across languages. At Cohere, we’ve trained our model specifically to be used for and While many of these models are available for English, similar existing multilingual models only work well for short sentences and can’t capture the meaning behind longer text. This prevents them from being used for semantic search, which typically aims to match a short query with a longer, relevant document. In this blog post, we will cover three relevant use cases that showcase the power of Cohere’s new multilingual model:*****To use our API, every developer must clearly outline their use case and have it approved by Cohere through our application process. The application requires thoroughly understanding of our models and their limitations, which will be updated as the models improve. Beyond these Usage Guidelines, you should refer to the Generation and Representation model cards for detailed information about each model. By understanding the language models that power our API endpoints, being aware of their limitations, and documenting your development practices, you can do great things with the Cohere Platform.*****Generation or summarization of long-form documents (max: 300 tokens/call). Generation of content sensitive politically, economically, medically, or culturally. : : Sharing positive generated content in order to direct attention away from harmful actions. : Tools that promote academic dishonesty. Usages which appear to violate our guidelines should be reported within 24 hours to Cohere by contacting us at responsibility@cohere.ai . Intentional stress testing of the API and adversarial attacks are allowable, but violative generations must be disclosed here, reported immediately , and must not be used for any purpose except for documenting the result of such attacks in a responsible manner.*****: Creating or promoting harmful false claims about government policies, or public figures, including applications founded on unscientific premises. : Spearphishing. : Model attacks to extract personal information. : Posting content to social platforms in an automated way. : Applications that do not disclose that the content is generated through automated means. : AI-based social scoring for general purposes done by public authorities: Using output toward larger decision-making systems that will influence actions, decisions, or policies without a human-in-the-loop. : Applications that classify and/or profile people based on protected characteristics, or infer those characteristics from text written about them or by them.*****Bullying, threatening, shaming, or doxxing. : Belittling victims of serious physical or emotional harm (even if unintentional). : Sharing of divisive generated content in order to turn a community against itself. : Perpetuating racism, or sexism (even if unintentional). Attempting to characterize gender, race, or ethnicity. : Distribution of sexually explicit acts, torture, or abuse. : Attempting to influence political decisions, or opinions. : Catfishing, phishing, or attempting to circumvent the law. : Sending unsolicited email and messages, or manipulating search engines. : Representing raw generations as coming from humans, using supervised generations with false identities, or a single person using generations with many identities that appear to be independent.*****### 1. Comprehend Cohere ( Qiu et al., 2020 ) describes the history, technical aspects, and applications of pre-trained language models like the ones which power the Cohere Platform. We recommend reading this survey and other language modeling research to learn what kinds of knowledge are encoded in language models and how to use their outputs responsibly in downstream tasks. Language models might encode the following: such as subject-verb agreement, part-of-speech, and other simple syntactic structures ( Liu et al., 2019 ; Hewitt et al., 2019 ). including relational and commonsense knowledge such as where famous individuals were born or the color of the sky, limited by what is contained in the training data.*****### Develop, test, and experiment with the industry’s first multilingual text understanding model that supports 100+ languages Humans speak over 7100 languages, yet the majority of language models only support the English language. This makes it incredibly challenging to build products and projects using multilingual language understanding. Cohere’s mission is to solve that by empowering our developers with technology that possesses the power of language. That’s why today we’re introducing our first multilingual text understanding model that supports over 100 languages and delivers 3X better performance than existing open-source models. This will enable new markets, countries, and global companies to better serve their customers across the globe.*****At Cohere, we are committed to breaking down barriers and expanding access to cutting-edge NLP technologies that power projects across the globe. By making our innovative multilingual language models available to all developers, we continue to move toward our goal of empowering developers, researchers, and innovators with state-of-the-art NLP technologies that push the boundaries of Language AI. Our Multilingual Model maps text to a semantic vector space, positioning text with a similar meaning in close proximity. This process unlocks a range of valuable use cases for multilingual settings. For example, one can map a query to this vector space during a search to locate relevant documents nearby.*****Cohere's multilingual text understanding model is now available! The multilingual-22-12 model can be used to semantically search within a single language, as well as across languages. Compared to keyword search, where you often need separate tokenizers and indices to handle different languages, the deployment of the multilingual model for search is trivial: no language-specific handling is needed — everything can be done by a single model within a single index. In addition to our new model, you can now detect the language of a data source using co.detect_language() endpoint. For more information, see our multilingual docs .*****## Get Started To get started using the multilingual embedding models, you can either query our endpoints or install our SDK to use the model within Python:*****The multilingual-22-12 model can be used to semantically search within a single language, as well as across languages. Compared to keyword search, where you often need separate tokenizers and indices to handle different languages, the deployment of the multilingual model for search is trivial: no language-specific handling is needed — everything can be done by a single model within a single index. We’re extremely proud of the performance of our multilingual understanding model. It outperformed the industry standard (the next best model) in search tasks by more than 230%.*****A lot of existing English training data has a focus on topics that are primarily interesting for U.S. citizens, for example, how to fill out specific U.S. tax forms. If these question/answer pairs are then translated into another language, e.g., Korean, the model learns in Korean how to file taxes in the U.S., but it doesn’t learn how to file taxes in Korea, a topic that is likely more relevant for Korean citizens. This makes prior models rather suboptimal for multilingual semantic search, as they don’t capture country-specific interests well. Our training process included actual authentic question/answer pairs from users across hundreds of languages from tens of thousands of websites from hundreds of countries.*****The co.classify endpoint now supports the use of Cohere's multilingual embedding model. The multilingual-22-12 model is now a valid model input in the co.classify call.*****## What is the Playground? The Cohere Playground is a visual interface for users to test Cohere's large language models without writing a single line of code. To familiarize yourself with our endpoints , we recommend clicking the Generate or Calculate button on each endpoint page and observing the outputs. Use the Playground to test your use cases and when you're ready to start building, simply click Export Code to add Cohere's functionality to your application.*****## Getting Started with the Cohere Playground Throughout the series, we will cover the full spectrum of working with generative AI to enable you to build applications with it. But to start with, let’s take the no-code route: we’ll show you how AI text generation works, and how you can experiment with it in the Cohere Playground . First, sign up for a Cohere account and then visit the Playground . The Playground UI consists of a few sections. The main window is where you enter your prompt and where the output, or response, is generated. A menu of saved prompts, or*****### UI or API - It's Your Choice You can generate copy using the Cohere Playground UI, or integrate Cohere's generation models into your own apps.*****## View a Demo of the Cohere Playground Watch this demonstration to learn how simple it is to work with and fine-tune Cohere models and endpoints for your startup’s needs. NLP offers a wealth of opportunities for startups building with language AI to create efficiencies, develop differentiated products, and get to market faster. With Cohere, you can achieve all of that and integrate NLP quickly with no model training required. Ready to get started building with LLMs? Learn more about developing content with Cohere and sign up for a free Cohere account to start building.*****### Summarize Large language models present a breakthrough in text generation. For the first time in history, we have software programs that can write text that sounds like it’s written by humans. These capabilities open doors to use cases like summarization or paraphrasing. A summarization prompt in the Cohere playground shows this output (in bold): Large language models can be adapted to new tasks with impressive speed. For tasks which appear in the training data (i.e. documents on the web), language models can successfully summarize text without being shown any examples at all. Summarization and paraphrasing both use the generate endpoint.*****#### Generate Produce text completions based on a given input. Model Price per Unit Default $2.5 per 1000 Generation Units* Custom $5.0 per 1000 Generation Units* More details*****# Meet your AI-generated content writer Generate is powered by a large language model that has read billions of words, learning the patterns and idiosyncrasies of sentences. Using this knowledge, it writes content, predicts outcomes or answers questions at your command.*****Free, rate limited Trial Keys for experimentation, testing, and playground usage Production keys with no rate limit for serving Cohere in production applications Flat rate pricing for Generate and Embed endpoints Reduced pricing for Classify endpoint New UI for dashboard including sign up and onboarding - everything except playground New use-case specific Quickstart Guides to learn about using Cohere API Replacing "Finetune" nomenclature with "Custom Model" Inviting team members is now more intuitive. Teams enable users to share custom models with each other Generative custom models now show accuracy and loss metrics alongside logs Embed and Classify custom models now show logs alongside accuracy, loss, precision, f1, recall*****## Reading billions of words, to write the ones you need. The Generate API is trained on vast amounts of text spanning all topics and industries. With Generate, you ‘instruct’ the model with your specific text generation ask. This could be a copywriting task, named entity recognition, or even paraphrasing or summarization.*****## Get Started Create an account instantly to get started. You can also contact us to design a custom package for your business. Stay updated*****# Classification Pricing  Classifications will cost $5 per 1000 text classified across the platform, regardless of model size (small, medium, large, xlarge) or usage of a finetuned model. Get an API Key Share what you're building on our co:mmunity forum or shoot us an email .*****#### What's possible with Generate Write ads and descriptions faster Use Generate to perform time-consuming and repetitive copywriting tasks, like product descriptions or email responses. Paraphrase sentences and paragraphs Generate allows you to re-word text to suit a specific reader, or reformat existing content into unique pieces. Make the world bite-sized Use Generate to automatically condense key information from texts into digestible summaries. Find what you're looking for Use our models to identify and extract specific data defined by your unique business needs.*****#### Summarize Summarize a given input text such as an article into a short paragraph or key bullet points Model Price per Unit Default $5.0 per 1000 Summarization Units* Custom $10.0 per 1000 Summarization Units* More details API Key Offering API Key Offering*****A lot of existing English training data has a focus on topics that are primarily interesting for U.S. citizens, for example, how to fill out specific U.S. tax forms. If these question/answer pairs are then translated into another language, e.g., Korean, the model learns in Korean how to file taxes in the U.S., but it doesn’t learn how to file taxes in Korea, a topic that is likely more relevant for Korean citizens. This makes prior models rather suboptimal for multilingual semantic search, as they don’t capture country-specific interests well. Our training process included actual authentic question/answer pairs from users across hundreds of languages from tens of thousands of websites from hundreds of countries.*****However, these models don’t capture the nuances behind language usage in different countries. Contrary to this approach, we collected a dataset of nearly 1.4 billion question/answer pairs across tens of thousands of websites in hundreds of languages. These are questions actually asked by speakers of said languages, allowing us to capture language- and country-specific nuances. “I strongly believe that embeddings are the future of search and recommendation. Thanks to the new Cohere multilingual model and the text2vec Cohere module in Weaviate, we can bring this to developers worldwide with a single command.” - Bob van Luijt, CEO at SeMI Technologies multilingual-22-12*****The main reason is that these models have just been trained at a sentence level, and they are not able to produce meaningful embeddings for longer text, like paragraphs.*****At Cohere, we are committed to breaking down barriers and expanding access to cutting-edge NLP technologies that power projects across the globe. By making our innovative multilingual language models available to all developers, we continue to move toward our goal of empowering developers, researchers, and innovators with state-of-the-art NLP technologies that push the boundaries of Language AI. Our Multilingual Model maps text to a semantic vector space, positioning text with a similar meaning in close proximity. This process unlocks a range of valuable use cases for multilingual settings. For example, one can map a query to this vector space during a search to locate relevant documents nearby.*****## What is a Multilingual Text Understanding Model? Multilingual text understanding models are powerful models that can derive insights from text data across languages. At Cohere, we’ve trained our model specifically to be used for and While many of these models are available for English, similar existing multilingual models only work well for short sentences and can’t capture the meaning behind longer text. This prevents them from being used for semantic search, which typically aims to match a short query with a longer, relevant document. In this blog post, we will cover three relevant use cases that showcase the power of Cohere’s new multilingual model:*****The co.classify endpoint now supports the use of Cohere's multilingual embedding model. The multilingual-22-12 model is now a valid model input in the co.classify call.*****Cohere's multilingual text understanding model is now available! The multilingual-22-12 model can be used to semantically search within a single language, as well as across languages. Compared to keyword search, where you often need separate tokenizers and indices to handle different languages, the deployment of the multilingual model for search is trivial: no language-specific handling is needed — everything can be done by a single model within a single index. In addition to our new model, you can now detect the language of a data source using co.detect_language() endpoint. For more information, see our multilingual docs .*****# Meet your AI-generated content writer Generate is powered by a large language model that has read billions of words, learning the patterns and idiosyncrasies of sentences. Using this knowledge, it writes content, predicts outcomes or answers questions at your command.*****#### What's possible with Generate Write ads and descriptions faster Use Generate to perform time-consuming and repetitive copywriting tasks, like product descriptions or email responses. Paraphrase sentences and paragraphs Generate allows you to re-word text to suit a specific reader, or reformat existing content into unique pieces. Make the world bite-sized Use Generate to automatically condense key information from texts into digestible summaries. Find what you're looking for Use our models to identify and extract specific data defined by your unique business needs.*****## Reading billions of words, to write the ones you need. The Generate API is trained on vast amounts of text spanning all topics and industries. With Generate, you ‘instruct’ the model with your specific text generation ask. This could be a copywriting task, named entity recognition, or even paraphrasing or summarization.*****#### Make it yours Our models have read billions and billions of words. But they can be made even more effective with a little training from you. Finetuning is when you teach a model niche knowledge through text examples — industry context, typical language and words used, common questions and answers, all of which improve the model’s accuracy and context.*****## Get Started Create an account instantly to get started. You can also contact us to design a custom package for your business. Stay updated*****### Integrate large language models into your builds We’ve made an API that can be used in different libraries that fit every stack. No matter your level of developer experience, Cohere makes it easy to build machine learning into your application with our Python, Node, and Go SDKs. We support all common languages through native SDK support or encapsulated REST calls, Our models have been trained on billions of words, allowing them to understand nuance and context*****## Final Thoughts In this blog post, we made our first foray into the Generate endpoint using the Python SDK. We got familiarized with how to get text generations via the API, and we created a simple function to help us experiment with a prompt idea. We have only covered the basics, though. In upcoming articles, we’ll look at how we can integrate the endpoint into proper applications, such as adding user interfaces, working with other endpoints in tandem, and more. In the meantime, get your free API key to start building with generative AI.*****## Understanding the Response Let’s understand what we get from the API response. The Generate endpoint accepts a text input, that is the prompt, and outputs a Generation object. Here’s an example response, with the text generated: The response contains: id text likelihood token_likelihoods If you want to keep it simple, you just need to get the text output and you’re good to go, like what we have done so far: response.generations[0].text . Here, we defined the the index of the generation (index 0, representing the first item) because the endpoint can actually generate more than one output in one go.*****# Training the Cohere Model For this tutorial, you’ll use a pre-trained text classification model and finetune it using a collection of example feedback that refers to four different product areas. You’ll do so using a simple two-column .csv spreadsheet containing the text sample and its label. You can find the sample data on GitHub . Start by clicking on the button in the Cohere Dashboard. Change the Model type to , select the radio button and enter this link . Then, click . On the next menu, check the option to exclude the top row of the .csv , click*****Second, the dataset should be in a CSV format with only two columns — one for examples and one for labels. Finally, it is recommended to have at least 250 examples in total to create your fine-tuned model. To conform to these guidelines, use a formatted version of the GoEmotions dataset . Now, go to the Cohere dashboard to create your fine-tuned model. Click . Select under model type and under model size. Then, upload your downloaded GoEmotions dataset. Click . This offers a visual sample of your dataset, allowing you to catch any mistakes. Click the checkbox and then .*****### The power of understanding Classify uses cutting-edge machine learning  to analyze and bucket text into specific categories. Build automated text classifiers into your application to do things like identify toxic language, automatically route customer queries, or detect breaking trends in product reviews.*****Visit Google Cloud Marketplace and click to get started using Cohere.*****#### What's possible with Classify Keep your community safe Use Classify to identify hate speech, abusive language, spam, profanity, or anything that meets user-provided filters. Harness intent recognition Leverage Classify to triage inbound chatbot or email requests to understand user intent and automatically issue responses. Serve your customers better Save time by tasking Classify to route inbound customer support requests to their respective teams. Access industry-leading sentiment analysis Develop a stronger customer affinity by classifying posts, reviews, etc to understand how they perceive your company/brand.*****# Organize information for more effective content moderation, analysis and chat bot experiences.  Access large language models that can understand text and take appropriate action — like highlight a post that violates your community guidelines, or trigger accurate chatbot responses. Just set your parameters, and Classify will do the rest.*****## Model Description The model outlined in this card provides It powers the Generate endpoint. : Generative Pretrained Transformer See release notes Medium, Extremely Large Cohere Safety Team & Responsibility Council coheretext-filtered dataset*****The student models can even outperform the teacher in some tasks while reducing model size requirements by several orders of magnitude. They conduct extensive ablation studies and sample studies to understand the reasoning capabilities of student models, and identify several important nuances that have been overlooked in concurrent fine-tuning works on CoT. SparseGPT: Massive Language Models Can Be Accurately Pruned in One-Shot Authors: Elias Frantar, Dan Alistarh This paper presents a new pruning method called SparseGPT that can reduce the number of weights in large-scale generative pre-trained transformer (GPT) models by at least 50% without any retraining and minimal loss of accuracy.*****### Large Language Models A recent breakthrough in artificial intelligence (AI) is the introduction of language processing technologies that enable us to build more intelligent systems with a richer understanding of language than ever before. Large pre-trained Transformer language models, or simply large language models, vastly extend the capabilities of what systems are able to do with text. Consider this: adding language models to empower Google Search was noted as “representing the biggest leap forward in the past five years, and one of the biggest leaps forward in the history of Search“. Microsoft also uses such models for every query in the Bing search engine.*****If you think of this as the process of building a house, pre-training can be compared to the process of building its foundation and basic building blocks. Just as a strong foundation is necessary for a house to stand, pre-training is necessary to build a solid foundation for a language model. Finetuning, on the other hand, focuses on customizing that house with specific features, which can differ based on the exact needs and preferences of a person. At Cohere, we refer to our pre-trained models as baseline models (at the time of writing, these are xlarge , medium , command-xlarge-nightly , and*****📘 This Guide Uses the Endpoint. You can find more information about the endpoint here . Extracting a piece of information from text is a common need in language processing systems. LLMs can at times extract entities which are harder to extract using other NLP methods (and where pre-training provides the model with some context on these entities). This is an overview of using generative LLMs to extract entities.*****## What are Custom Models? To understand what a custom model is and how it works, it’s good to know a couple of terms commonly used in LLMs: pre-training and finetuning. Pre-training is the process of training a language model on a large amount of text data to learn the general patterns and structures of language. By doing this, the model is able to learn how to generate text that is coherent. Finetuning, on the other hand, involves taking a pre-trained language model and training it on a smaller, more specific dataset to adapt it to a particular task. Finetuning allows the model to be customized for a specific use case, which can result in better performance on that task.*****Automated Essay Scoring Using Transformers Author: Kshitij Gupta Investigating automated essay scoring has been a long-standing focus in the natural language processing (NLP) community because of its potential applications in both education and business. Recent advances in large, pre-trained models and data augmentation have made significant progress in this area, but many challenges remain. This work demonstrates the effectiveness of transformer models and data augmentation for automated essay grading across a variety of topics. The findings show that transformer models are a promising approach for automated essay scoring, and they suggest avenues for further research. What Do Large Language Models Learn Beyond Language?*****The models may fail to generate the correct answer to a factual question if the information regarding that fact has recently changed since the models were trained. Model outputs are derived statistically rather than from any direct modeling of the meaning of words and phrases, and therefore they should not be interpreted as a grounded means for understanding text ( Bender, 2020 ). Language models capture the hegemonic viewpoint, reflecting and magnifying biases that exist on the internet ( Bender,2021 ). As a result, marginalized groups can be harmed by entrenching existing stereotypes, or producing demeaning portrayals ( Crawford, 2017 ).*****). As a result, performance will likely degrade on text about concepts, people and places from other regions, especially that of the Global South. The models may prefer phrases and ideas associated with Western ideals, or with wealthier and more technologically developed cultures. At any point, the model will only represent the concepts, events, places, and people from data on which it was trained. Information from after the dataset was gathered will not be represented. For example, if an event occurred today, the model would not be able to return a meaningful representation of the event name. Additionally, the model may amplify outdated societal biases about groups of people.*****The following factors may impact our language models’ performance. : Due to the lack of available training data and evaluation datasets for the majority of the world’s languages, the model is unlikely to perform well on languages other than the dominant dialects of ( Joshi, 2020 ; Dodge, 2021 ). We are actively working to increase the number of languages supported by the Cohere API. Our language models may fail to meaningfully represent non-English phrases. The majority of publicly available data used to train the model is from wealthier individuals in more developed countries, and is largely Western-centric ( Pew, 2021*****Despite our active efforts to mitigate these biases, we acknowledge that this is an ongoing research area ( Gonen, 2019 ). _ The models may associate gender, racial, and other identities with professions and concepts which are semantically unrelated to those identities. These associations are likely to reflect biases present in the historical data the model was trained on. For research on bias in embeddings, see, for example, ( Kurita et al., 2019 ); for a survey of the space of language model generation bias, see ( Sheng et al., 2021 ).*****The model may also fail to perform simple reasoning tasks involving basic arithmetics and chronology. If you’re getting an incoherent output from the model, give it a couple of more tries or experiment with different parameters or a different prompt. Recently developed methods in this field include chain-of-thought prompting , which significantly improves the model’s ability to handle these tasks. Your model has a limited “memory span” in which the context window is limited to a specific number of tokens. If you want your model to process text that is longer than the maximum text length it supports, it will split it and fail to connect its parts in a meaningful way.*****### Large Language Models A recent breakthrough in artificial intelligence (AI) is the introduction of language processing technologies that enable us to build more intelligent systems with a richer understanding of language than ever before. Large pre-trained Transformer language models, or simply large language models, vastly extend the capabilities of what systems are able to do with text. Consider this: adding language models to empower Google Search was noted as “representing the biggest leap forward in the past five years, and one of the biggest leaps forward in the history of Search“. Microsoft also uses such models for every query in the Bing search engine.*****here A large language model (LLM) is a type of machine learning model that can handle a wide range of natural language processing (NLP) use cases. But due to their versatility, LLMs can be a bit overwhelming for newcomers who are trying to understand when and where to use these models. In this blog series, we’ll simplify LLMs by mapping out the seven broad categories of use cases where you can apply them, with examples from Cohere's LLM platform. Hopefully, this can serve as a starting point as you begin working with the Cohere API , or even seed some ideas for the next thing you want to build.*****Despite the utility of these models, training and deploying them effectively is resource intensive in its requirements of data, compute, and engineering resources.*****here It can be a bit overwhelming for someone new to Large Language Models (LLMs) to understand when and where to use them in natural language processing (NLP) use cases. In this blog series, we simplify LLM application by mapping out the seven broad categories of use cases that you can address with Cohere’s LLM. In Part 1 of our series, we covered the first four use case categories: Generate, Summarize, Rewrite, and Extract. In this post, we will cover the other three: Search, Cluster, and Classify. Finally, we’ll look at how we can combine the different types, making their applications much more interesting and useful.*****The search engine must be able to know that the user is looking for taxis, car rentals, trains, or other similar services, even if the user doesn’t explicitly mention them. When we input a piece of text into a representation model, instead of generating more text, the model generates a set of numbers that represent the meaning or context of the input text. These numbers are called “ text embeddings ”. In LLMs, they tend to be a very long sequence of numbers, typically in the thousands, and the longer they are, the more information is stored about the text. With Cohere, you can access this type of model via the*****Here are some other example documents where LLM summarization will be useful:*****#### Generate Produce text completions based on a given input. Model Price per Unit Default $2.5 per 1000 Generation Units* Custom $5.0 per 1000 Generation Units* More details*****for more about toxic language model degeneration). We have put safeguards in place to avoid generating harmful text, but we highly recommend that developers build additional guardrails to ensure that text presented to end users is not toxic or harmful. Language models capture problematic associations and stereotypes prominent on the internet and society at large. They should not be used to make decisions about individuals or the groups they belong to. For example, it is dangerous to use Generation model outputs in CV ranking systems due to known biases ( Nadeem et al., 2020 ).*****Generated text used to provide the illusion of discourse or expression of opinion by members of the public on social media or any other channel. The generation of news or other articles which manipulate public opinion, or any content which aims to incite hate or mischaracterize a group of people. The generation of text about people, places, or events without a human-in-the-loop. This includes making automated decisions based on model-generated outputs which have real-world consequences on people, or posing as a human in any context where the end user is unaware that outputs are being generated by a language model. Footer*****## Potential for Misuse Guided by the NAACL Ethics Review Questions , we describe below the model-specific concerns around misuse of the Generation model. By documenting adverse use cases, we aim to encourage Cohere and its customers to prevent adversarial actors from leveraging our models to the following malicious ends. The examples in this section are and are only meant to illustrate our understanding of potential harms. The examples are meant to be more model-specific and tangible than those in the Usage Guidelines . Each of these malicious use cases violates our usage guidelines and Terms of Use, and Cohere reserves the right to restrict API access at any time.*****### Technical Notes This model provides completions for English text only. Generation quality is highly dependent on the sampling parameters. Please consult the documentation for details about each parameter and tune the values used for your application. Parameters may require re-tuning upon a new model release. Performance quality on generation tasks may increase when examples are provided in the context. See the prompt engineering wiki page for instructions regarding how to construct the best prompts for your task.*****### Model Toxicity and Bias Language models learn the statistical relationships present in training datasets, which may include toxic language and historical biases along race, gender, sexual orientation, ability, language, cultural, and intersectional dimensions. We recommend that developers using the Generation model take model toxicity and bias into account and design applications carefully to avoid the following: Despite our ongoing efforts to remove harmful text from the training corpus, models may generate toxic text. This may include obscenities, sexually explicit content, and messages which mischaracterize or stereotype groups of people based on problematic historical biases perpetuated by internet communities (see Gehman et al., 2020*****### Example This example uses the input: “ This curved gaming monitor delivers ... ” The output generated with a maximum token set of 4 and sorted by average token likelihood are: You can use these outputs in a number of ways, for example, by selecting the one with the highest likelihood as the final output or by presenting these as options in your application.*****## Intended Use Case Generations may be used for interactive autocomplete, augmenting human writing processes, summarization, text rephrasing, and other text-to-text tasks in non-sensitive domains. Outputs from Classify can be used for classification and analysis tasks, such as selecting the most likely completion for a sentence. Token likelihoods from Likelihood might be used to make fun claims about the “randomness” of your favourite author’s writing, or to explore the statistical differences between human-written and machine-generated text (see Gehrmann et al., 2019 ). Generations can address a large number of use cases. Check the presets for interesting examples.*****When you call the Generate endpoint, you have the option to generate multiple generations in a single call. This is done by setting the num_generations parameter . The model’s outputs will vary depending on the generation settings you have specified, such as temperature , top-k , and top-p . Each generation comes with its set of likelihood values, which consists of: The likelihood of each generated token The average likelihood of all generated tokens.*****Not all online platforms define toxicity the same way, and each will have different language nuances to accommodate. For example, a gaming platform, an online community for kids, and a social media platform—each would have a different interpretation of the exact same data. This is where model training can help, where a model can be customized to your specific needs.*****### When to Train a Model Training large language models is only required when you need to teach the model something extremely niche, like the different gaits of a horse or your company's unique knowledge base. Common knowledge, like the colour of the sky, does not require training. Training is also helpful for generating or understanding data in a specific writing style or format.*****#### Intuition Let's take a representation model as an example, where we finetune a model for a classification task with training data consisting of three classes. To get an idea of how a representation model performs, we can project the embeddings it generates on a 2-dimensional plot, as per the image below. This image was taken from actual model outputs in the Playground. The distance between two data points represents how semantically similar they are—the closer they are, the more similar they are, and vice versa. A good model will have a clear separation between classes. To test the model, here we have fifteen data points, five for each class, in which the classes are unknown to the model.*****With a baseline model (left plot), we get a good separation between classes, which shows that it can perform well in this task. But with a trained model (right plot), the separation becomes even more apparent. Similar data points are now pushed even closer together and further apart from the rest. This indicates that the model has adapted to the additional data it receives during training, hence is more likely to perform even better in this task. In real applications, this makes a huge difference. One example is a toxicity classifier to help content moderators automatically flag toxic content on their platforms.*****### An Overview of Model Training Cohere's platform gives you the ability to train a Large Language Model (LLM) and customize it with a dataset to excel at a specific task. Custom models can lead to some of the best performing NLP models for a wide number of tasks. In this article, we look at training a generation model. See here for training a representation model.*****## Training a Custom Model To train a custom model, there is only one thing we need to prepare, and that is the training dataset. The training dataset contains examples of what we want the model to output, given an input prompt. The format is the same as the "prompting by example" format we saw in Part 1. The difference here is that we will put all these examples in a `txt` file, and most importantly, we need to include a lot of examples. How many examples are needed? There’s no one-size-fits-all answer to that question as it depends on the type and complexity of your task.*****### Start Training Give your model a nickname! Now, everything is set for custom training to begin. Press Start Training to begin!*****## Training Data Training embedding models require data in a specific format. For example, question/answer pairs or title/document pairs. We can then learn which text pairs should be closer together in the vector space in order to enable applications like semantic search. To train Cohere’s new multilingual model, we processed and carefully cleaned terabytes of data from various sources: Wikipedia, news publications, scientific articles, and online communities across hundreds of languages. This resulted in a large training corpus of more than 900 million training pairs for English and 450 million training pairs for other languages. Other multilingual embedding models often rely on machine translation for training dataset creation, which creates an awkward bias for these models.*****## Use Cases for Embeddings What can you do with embeddings? Quite a lot. Our Embed endpoint takes a piece of text and creates a vector embedding, which represents the text as numbers that capture its meaning and context. Embedding transforms unstructured text data into a structured form that allows you to cluster, categorize, and semantically (contextually) search the text. Cohere’s powerful language models can find relationships between pieces of text (words, phrases, sentences, paragraphs, or documents) that would not surface with linear keyword search. Some examples of industries that could benefit from vector search include: Overall, any industry that relies on large amounts of information, and the ability to quickly and accurately search for that information, could benefit from vector search.*****# How It Works Embeddings are numerical representations of meaning in text. Because they are numbers, they can be compared to each other for similarity. They can also be plotted on a chart that shows which texts are similar to each other. Large language models produce highly nuanced embeddings.*****## How Does the Multilingual Text Understanding Model Work? Cohere’s multilingual text understanding model maps text to a semantic vector space (also known as “ embeddings ”), positioning texts with a similar meaning in close proximity. This process unlocks a range of valuable use cases for multilingual settings. For example, one can map a query to this vector space during a search to locate relevant documents nearby. This often yields search results that are several times better than keyword search. To train multilingual models, you need large quantities (hundreds of millions) of suitable training pairs, like question/answer pairs. So far, such training data has been primarily available in English, and prior work tried to use machine translation to map it to other languages.*****### Documents Understand relationships between words and ideas, and then uncover and visualize hidden trends in large documents using our powerful representation model and Embed endpoint. No model training required.*****For example, you can finetune (or customize) our base language models in the Cohere Playground to add additional language understanding to your dataset, such as unique product names, community slang, or industry jargon, and keep that customized or finetuned model updated. For example, the diagram below shows how a search pipeline would work by creating metadata for each document, creating and storing the vectors for the documents, and finally embedding the query to compare against the stored vectors. The task is a similarity matching where it retrieves the most similar image. Our multilingual support adds industry-leading accurate understanding of 100 languages to assist multinational companies with drawing insights and use from language data across the globe.*****The Cohere embedding does just this. Using transformers, attention mechanisms, and other cutting edge algorithms, this embedding sends every sentence to a vector formed by 4096 numbers, and this embedding works really well. As a small example, here is a heatmap of the first 10 entries of some sentences (writing the entire 4096 entries will take too much space, so we truncated it). Notice that these sentences are all very similar. In particular, the three highlighted sentences pretty much have the same meaning. If you look at their corresponding vectors, these are also really similar. That is exactly what an embedding should do.*****📘 This Guide Uses the Endpoint. You can find more information about the endpoint here . The Classify endpoint streamlines the task of running a text classification task. Via a single endpoint, you can deploy different kinds of content moderation use cases according to your needs. As online communities continue to grow, content moderators need a way to moderate user-generated content at scale . To appreciate the wide-ranging need for content moderation, we can refer to the paper A Unified Typology of Harmful Content by Banko et al. [ Source ]. It provides a unified typology of harmful content generated within online communities and a comprehensive list of examples, which can be grouped into four types:*****In this small example, the model got all classifications correct. We can then generate the equivalent code to access the Classify endpoint by exporting the code from the Playground. The following is the corresponding code snippet for the API call. From here, we can further build the content moderation solution according to the scale and integration needs.*****Hate and Harassment Self-Inflicted Harm Ideological Harm Exploitation There are publicly available datasets within the content moderation space which you can experiment with, for example: Social Media Toxicity dataset from Surge AI Wikipedia Comments dataset by Jigsaw/Conversation AI Civil Comments dataset by Jigsaw/Conversation AI Hate Speech Dataset by Derczynski et al.*****## Content Moderation The internet is dominated by user-generated content, which is a double-edged sword. While it provides an avenue for online platforms to grow and thrive, it is a bane for content moderators managing them. Take this for example. In 2021, 83% of adults (18-45) and 60% of teens (13-17) experienced harassment in online gaming [ Source ]. If online platforms are to provide a safe and pleasant user experience, they need to be effectively moderated. But the internet is a vast repository of content. Think about the various types of platforms available and the scale of their users: gaming, social media, dating, chat, online community, e-commerce, blog, video streaming, and the list goes on.*****In today’s world, content moderation remains a major challenge. As platforms like online games increasingly attract an international audience, the complexity of content moderation has grown as hateful content makes its way across multiple languages and has a greater probability of passing through content moderation tools. To tackle this challenge, we use multilingual embeddings to build a content moderation tool that works across 100+ languages and only requires training data in English. For content moderation, we just need a handful of training examples of harmful and acceptable content in one language. For example, in English, we can then train a classifier to find the decision boundary in the vector space that helps us determine which type of content is undesirable on the platform.*****The number of tokens per word depends on the complexity of the text. Simple text may approach 1 token per word on average, while complex texts may use less common words that require 3-4 tokens per word on average. Our representation models are currently limited to processing sequences with a maximum length of 4096 tokens. Generation models support up to 2048 tokens. Our vocabulary of tokens is created using Byte Pair Encoding .*****# Classification Pricing  Classifications will cost $5 per 1000 text classified across the platform, regardless of model size (small, medium, large, xlarge) or usage of a finetuned model. Get an API Key Share what you're building on our co:mmunity forum or shoot us an email .*****#### Summarize Summarize a given input text such as an article into a short paragraph or key bullet points Model Price per Unit Default $5.0 per 1000 Summarization Units* Custom $10.0 per 1000 Summarization Units* More details API Key Offering API Key Offering*****## Embedding Max Tokens Have Increased We have increased previous max tokens per text from 512 to 1024. For any text longer than 128 tokens, the text is spliced and the resulting embeddings of each component are averaged and returned.*****#### Embed Capture the semantic meaning of text by representing text as numbers. Model Price per Unit Default $1.0 per 1000 Embeddings Custom $2.0 per 1000 Embeddings More details*****Our language models understand "tokens" rather than characters or bytes. One token can be a part of a word, an entire word, or punctuation. Very common words like "water" will have their own unique tokens. A longer, less frequent word might be encoded into 2-3 tokens, e.g. "waterfall" gets encoded into two tokens, one for "water" and one for "fall". Note that tokenization is sensitive to whitespace and capitalization. Here are some references to calibrate how many tokens are in a text: one word tends to be about 2-3 tokens a verse of a song is about 128 tokens this short article has about 300 tokens*****The number of tokens per word depends on the complexity of the text. Simple text may approach 1 token per word on average, while complex texts may use less common words that require 3-4 tokens per word on average. Our representation models are currently limited to processing sequences with a maximum length of 4096 tokens. Generation models support up to 2048 tokens. Our vocabulary of tokens is created using Byte Pair Encoding .*****### How to pick max_tokens when sampling The easiest way to determine a good number of tokens is to guess and check using our playground. It is common to request more tokens than required and then run additional processing to retrieve the desired output.*****The method of picking output tokens is a key concept in text generation with language models. There are several methods (also called decoding strategies) for picking the output token and two of the leading ones are top-k sampling and top-p sampling. Let’s look at the example where the input to the model is the prompt The name of that country is the : The output token in this case, United , was picked in the last step of processing -- after the language model has processed the input and calculated a likelihood score for every token in its vocabulary. This score indicates the likelihood that it will be the next token in the sentence (based on all the text the model was trained on).*****Token likelihood is a useful tool for model evaluation. For instance, let's say you've trained a custom model and would like to know how much it's improved over the default model - you could use token likelihoods to compare the performance of the models on some held-out text. Here is a quick demonstration of how to use the return_likelihoods parameter from the Generate endpoint for model evaluation.*****Generation or summarization of long-form documents (max: 300 tokens/call). Generation of content sensitive politically, economically, medically, or culturally. : : Sharing positive generated content in order to direct attention away from harmful actions. : Tools that promote academic dishonesty. Usages which appear to violate our guidelines should be reported within 24 hours to Cohere by contacting us at responsibility@cohere.ai . Intentional stress testing of the API and adversarial attacks are allowable, but violative generations must be disclosed here, reported immediately , and must not be used for any purpose except for documenting the result of such attacks in a responsible manner.*****## 1. Pick the top token: greedy decoding You can see in this example that we picked the token with the highest likelihood, ‘United’. Greedy decoding is a reasonable strategy but has some drawbacks such as outputs with repetitive loops of text. Think of the suggestions in your smartphone's auto-suggest. When you continually pick the highest suggested word, it may devolve into repeated sentences.*****### Getting meaningful text representations with  Embed() The next step was to embed these titles so we can examine the dataset based on the meanings of the titles and not just the tokens they contain. Cohere’s embed endpoint gives us vector representations from a large embedding language model specifically tuned for text embedding (as opposed to word embedding or text generation). This gives us a matrix where each post title has a 1024 dimensional vector numerically containing its meaning.*****## Embedding Max Tokens Have Increased We have increased previous max tokens per text from 512 to 1024. For any text longer than 128 tokens, the text is spliced and the resulting embeddings of each component are averaged and returned.*****#### Generate Produce text completions based on a given input. Model Price per Unit Default $2.5 per 1000 Generation Units* Custom $5.0 per 1000 Generation Units* More details*****#### What's possible with Generate Write ads and descriptions faster Use Generate to perform time-consuming and repetitive copywriting tasks, like product descriptions or email responses. Paraphrase sentences and paragraphs Generate allows you to re-word text to suit a specific reader, or reformat existing content into unique pieces. Make the world bite-sized Use Generate to automatically condense key information from texts into digestible summaries. Find what you're looking for Use our models to identify and extract specific data defined by your unique business needs.*****#### Make it yours Our models have read billions and billions of words. But they can be made even more effective with a little training from you. Finetuning is when you teach a model niche knowledge through text examples — industry context, typical language and words used, common questions and answers, all of which improve the model’s accuracy and context.*****## Get Started Create an account instantly to get started. You can also contact us to design a custom package for your business. Stay updated*****## Reading billions of words, to write the ones you need. The Generate API is trained on vast amounts of text spanning all topics and industries. With Generate, you ‘instruct’ the model with your specific text generation ask. This could be a copywriting task, named entity recognition, or even paraphrasing or summarization.*****Our language models understand "tokens" rather than characters or bytes. One token can be a part of a word, an entire word, or punctuation. Very common words like "water" will have their own unique tokens. A longer, less frequent word might be encoded into 2-3 tokens, e.g. "waterfall" gets encoded into two tokens, one for "water" and one for "fall". Note that tokenization is sensitive to whitespace and capitalization. Here are some references to calibrate how many tokens are in a text: one word tends to be about 2-3 tokens a verse of a song is about 128 tokens this short article has about 300 tokens*****The number of tokens per word depends on the complexity of the text. Simple text may approach 1 token per word on average, while complex texts may use less common words that require 3-4 tokens per word on average. Our representation models are currently limited to processing sequences with a maximum length of 4096 tokens. Generation models support up to 2048 tokens. Our vocabulary of tokens is created using Byte Pair Encoding .*****### How to pick max_tokens when sampling The easiest way to determine a good number of tokens is to guess and check using our playground. It is common to request more tokens than required and then run additional processing to retrieve the desired output.*****The method of picking output tokens is a key concept in text generation with language models. There are several methods (also called decoding strategies) for picking the output token and two of the leading ones are top-k sampling and top-p sampling. Let’s look at the example where the input to the model is the prompt The name of that country is the : The output token in this case, United , was picked in the last step of processing -- after the language model has processed the input and calculated a likelihood score for every token in its vocabulary. This score indicates the likelihood that it will be the next token in the sentence (based on all the text the model was trained on).*****## Embedding Max Tokens Have Increased We have increased previous max tokens per text from 512 to 1024. For any text longer than 128 tokens, the text is spliced and the resulting embeddings of each component are averaged and returned.*****## Number of Tokens I said earlier that the language model builds a list of words and their probabilities as outputs. This is technically incorrect. It builds a list of tokens, which is roughly 4 characters, but not always. For example, a word like “water” might end up being one token, whereas larger words might be broken up into multiple tokens. At Cohere, we use byte-pair encoding to create tokens . You probably don’t want the language model to keep generating outputs ad infinitum, so the number of tokens parameters allows you to set a limit to how many tokens are generated.*****## 3. Pick from amongst the top tokens whose probabilities add up to 15%: top-p The difficulty of selecting the best top-k value opens the door for a popular decoding strategy that dynamically sets the size of the shortlist of tokens. This method, called Nucleus Sampling , shortlists the top tokens whose sum of likelihoods does not exceed a certain value. A toy example with a top-p value of 0.15 could look like this: Top-p is usually set to a high value (like 0.75) with the purpose of limiting the long tail of low-probability tokens that may be sampled. We can use both top-k and top-p together.*****### Getting meaningful text representations with  Embed() The next step was to embed these titles so we can examine the dataset based on the meanings of the titles and not just the tokens they contain. Cohere’s embed endpoint gives us vector representations from a large embedding language model specifically tuned for text embedding (as opposed to word embedding or text generation). This gives us a matrix where each post title has a 1024 dimensional vector numerically containing its meaning.*****## 1. Pick the top token: greedy decoding You can see in this example that we picked the token with the highest likelihood, ‘United’. Greedy decoding is a reasonable strategy but has some drawbacks such as outputs with repetitive loops of text. Think of the suggestions in your smartphone's auto-suggest. When you continually pick the highest suggested word, it may devolve into repeated sentences.*****here A large language model (LLM) is a type of machine learning model that can handle a wide range of natural language processing (NLP) use cases. But due to their versatility, LLMs can be a bit overwhelming for newcomers who are trying to understand when and where to use these models. In this blog series, we’ll simplify LLMs by mapping out the seven broad categories of use cases where you can apply them, with examples from Cohere's LLM platform. Hopefully, this can serve as a starting point as you begin working with the Cohere API , or even seed some ideas for the next thing you want to build.*****This means that teams embarking on this task will need to spend resources and time collecting the data needed to perform the task. And not all teams have the luxury to do so. Enter the Large Language Model (LLM). It is a type of machine learning system that is already pre-trained with a huge amount of text. It is a general-purpose model that performs well over a wide range of NLP tasks including text classification, and in our example, toxicity classification. With the LLM approach, the amount of labeled training data required to achieve a good text classification performance significantly drops from typically in the thousands with the traditional approach to just in the hundreds or even tens in certain cases.*****The search engine must be able to know that the user is looking for taxis, car rentals, trains, or other similar services, even if the user doesn’t explicitly mention them. When we input a piece of text into a representation model, instead of generating more text, the model generates a set of numbers that represent the meaning or context of the input text. These numbers are called “ text embeddings ”. In LLMs, they tend to be a very long sequence of numbers, typically in the thousands, and the longer they are, the more information is stored about the text. With Cohere, you can access this type of model via the*****## Some General Notes and Advice for Using LLMs: For instance, the above approaches won’t work very well if our intents are too specific or unique. The provided approaches will work best if your target users are a large chunk of existing social media users. Also, adding that data diversity in the given way does not represent your actual users. It is basically a stopgap that gives us a warmer start.*****## 5. Search/Similarity Any mention of LLMs will most likely spark discussion around their text generation capabilities, as we’ve seen in the previous four use cases. The less-talked-about, but equally powerful capability, is text representation. While text generation is about creating new text, text representation is about making sense of existing text. Think about the amount of unstructured text data being generated today that’s only accelerated by the increasingly ubiquitous internet. It would not be possible for humans to process this massive volume of information without NLP-powered automation. One such use case category for text representation is similarity search. Given a text query, the goal is to find documents that are most similar to the query.*****## 7. Classify Last but not least is the text classification category, and that’s because it is probably the most widely applicable use of NLP  today. You can think of it as similar to clustering, with a slight twist. Clustering is called an “unsupervised learning” algorithm. That’s because we don’t know what the clusters are beforehand — we assign a number of clusters (we can choose any number), and the algorithm will group the documents we give according to that number. On the other hand, classification is a “supervised learning” algorithm, because this time, we already know beforehand what those clusters, or more precisely*****Build trust with your users, and the general public Preserving the integrity of your brand and platform becomes more challenging as you increase your userbase. Leveraging Cohere allows you to crack down on toxic language in all its evolving forms — protecting your users, and your brand’s reputation. Supercharge your moderation efforts Content moderation teams are stretched thin by the ever-evolving problem of toxicity. Automating toxic language detection with Cohere enables you to reduce the burden on your team, gain deeper insight into trends, and take faster action to address harmful behavior.*****#### Vendor and Risk Management We undergo at least annual risk assessments to identify any potential threats, including considerations for fraud. Vendor risk is determined and the appropriate vendor reviews are performed prior to authorizing a new vendor.*****Our team members are required to review and accept all of the security policies. Our team members are required to go through employee security awareness training covering industry standard practices and information security topics such as phishing and password management. All team members are required to sign and adhere to an industry standard confidentiality agreement prior to their first day of work. We perform background checks on all new team members in accordance with local laws. All of our services are hosted with . They employ a robust security program with multiple certifications. For more information on our provider’s security processes, please visit*****All of our data is hosted on databases. These databases are all located in the United States. Please reference the above vendor specific documentation linked above for more information. All databases are encrypted at rest. Our applications encrypt in transit with TLS/SSL only. We perform vulnerability scanning and actively monitor for threats. We actively monitor and log various cloud services. We use our data hosting provider’s backup services to reduce any risk of data loss in the event of a hardware failure. We utilize monitoring services to alert the team in the event of any failures affecting users. We have a process for handling information security events which includes escalation procedures, rapid mitigation and communication.*****#### Organizational Security We have an Information Security Program in place that is communicated throughout the organization. Our Information Security Program follows the criteria set forth by the SOC 2 Framework. SOC 2 is a widely known information security auditing procedure created by the American Institute of Certified Public Accountants. Our organization undergoes independent third-party assessments to test our security and compliance controls. We perform an independent third-party penetration at least annually to ensure that the security posture of our services is uncompromised. Roles and responsibilities related to our Information Security Program and the protection of our customer’s data are well defined and documented.*****We encourage you to read the privacy policy of every website you visit. We have implemented reasonable administrative, technical and physical measures in an effort to safeguard the personal information in our custody and control against theft, loss and unauthorized access, use, modification and disclosure. We restrict access to personal information on a need-to-know basis to employees and authorized service providers who require access to fulfil their job requirements. If we receive a request from an individual to access or update personal information we maintain on behalf of a customer, we will direct that individual to the relevant customer. We will assist our customers wherever possible in responding to individual access requests.*****You can also obtain additional information on Google Analytics’ data privacy and security at the following links: and : Our Website may contain links to other websites that Cohere does not own or operate. We provide links to third party websites as a convenience to the user. These links are not intended as an endorsement of or referral to the linked websites. The linked websites have separate and independent privacy policies, notices and terms of use. We do not have any control over such websites, and therefore we have no responsibility or liability for the manner in which the organizations that operate such linked websites may collect, use or disclose, secure and otherwise treat personal information.*****We may use a third party such as Google Analytics to help us gather and analyze information about the areas visited on the Website (such as the pages most read, time spent, search terms and other engagement data) in order to evaluate, derive insights from and improve the user experience and the Website (including the organization from which you access the website). These third parties may use cookies and other tracking technologies. For more information about Google Analytics or to prevent the storage and processing of this data (including your IP address) by Google, you can download and install the browser plug-in available at the following link: https://tools.google.com/dlpage/gaoptout?hl=en .*****# Our process This requires anticipating and accounting for risks during our development process. We run adversarial attacks, filter our training data for harmful text, and measure our models against safety research benchmarks. We also evaluate evolving risks with monitoring tools designed to identify harmful model outputs. We recognize that misuse of powerful language models will disproportionately impact the most vulnerable, so we aim to balance safety considerations and equity of access. This is an ongoing process. As we release early versions of our technology, we’ll work closely with our partners and users to ensure its safe and responsible use.*****#### Generate Produce text completions based on a given input. Model Price per Unit Default $2.5 per 1000 Generation Units* Custom $5.0 per 1000 Generation Units* More details*****#### Classify Analyze and organize text into specific categories. Model Price per Unit Default $2.0 per 1000 Classifications Custom $2.0 per 1000 Classifications More details*****#### Summarize Summarize a given input text such as an article into a short paragraph or key bullet points Model Price per Unit Default $5.0 per 1000 Summarization Units* Custom $10.0 per 1000 Summarization Units* More details API Key Offering API Key Offering*****#### Embed Capture the semantic meaning of text by representing text as numbers. Model Price per Unit Default $1.0 per 1000 Embeddings Custom $2.0 per 1000 Embeddings More details*****Generated text used to provide the illusion of discourse or expression of opinion by members of the public on social media or any other channel. The generation of news or other articles which manipulate public opinion, or any content which aims to incite hate or mischaracterize a group of people. The generation of text about people, places, or events without a human-in-the-loop. This includes making automated decisions based on model-generated outputs which have real-world consequences on people, or posing as a human in any context where the end user is unaware that outputs are being generated by a language model. Footer*****### Model Toxicity and Bias Language models learn the statistical relationships present in training datasets, which may include toxic language and historical biases along race, gender, sexual orientation, ability, language, cultural, and intersectional dimensions. We recommend that developers using the Generation model take model toxicity and bias into account and design applications carefully to avoid the following: Despite our ongoing efforts to remove harmful text from the training corpus, models may generate toxic text. This may include obscenities, sexually explicit content, and messages which mischaracterize or stereotype groups of people based on problematic historical biases perpetuated by internet communities (see Gehman et al., 2020*****for more about toxic language model degeneration). We have put safeguards in place to avoid generating harmful text, but we highly recommend that developers build additional guardrails to ensure that text presented to end users is not toxic or harmful. Language models capture problematic associations and stereotypes prominent on the internet and society at large. They should not be used to make decisions about individuals or the groups they belong to. For example, it is dangerous to use Generation model outputs in CV ranking systems due to known biases ( Nadeem et al., 2020 ).*****## Intended Use Case Generations may be used for interactive autocomplete, augmenting human writing processes, summarization, text rephrasing, and other text-to-text tasks in non-sensitive domains. Outputs from Classify can be used for classification and analysis tasks, such as selecting the most likely completion for a sentence. Token likelihoods from Likelihood might be used to make fun claims about the “randomness” of your favourite author’s writing, or to explore the statistical differences between human-written and machine-generated text (see Gehrmann et al., 2019 ). Generations can address a large number of use cases. Check the presets for interesting examples.*****## Potential for Misuse Guided by the NAACL Ethics Review Questions , we describe below the model-specific concerns around misuse of the Generation model. By documenting adverse use cases, we aim to encourage Cohere and its customers to prevent adversarial actors from leveraging our models to the following malicious ends. The examples in this section are and are only meant to illustrate our understanding of potential harms. The examples are meant to be more model-specific and tangible than those in the Usage Guidelines . Each of these malicious use cases violates our usage guidelines and Terms of Use, and Cohere reserves the right to restrict API access at any time.*****## This Month’s Hot Topic: Discord Bots Next, our experiment-obsessed co-founder Nick Frosst spoke about his hot topic of the month: Discord bots. Not only is building Discord bots just plain fun, but the Discord platform makes it increasingly easier to trigger and share them. Nick introduced us to two projects that he’s been involved with: the Grounded QA bot for contextualized search (now open source, see GitHub repo ) and the Web LM bot for automating a browser through language. Feel free to contribute to either, and if you’re making your own Discord bot, apply to speak at co:lab or simply approach Sandra Kublik or Nick Frosst on Discord!*****This enables anyone to produce a nearly infinite set of creative designs, regardless of skill level or background. See the GitHub repo Health-E by Augusto Bonorio and Mostafa Azazy — The team shared their AI assistant for healthcare providers. By combining a custom conversant persona and grounded QA, Health-E app aims to aid in processing patients, extracting relevant information to fill out forms, and providing advice common knowledge advice on applicable scenarios if prompted with a question. See GitHub repo . SEM:ANTICS by — Faizah Sayyid, Sophia Abolore, Salwa Abdalla, and Majda Lojpur — The team showcased their app that helps people improve written communications to create positive impressions and maintain professional relationships.*****In our analyses, we show that language descriptions in demonstrations improve sample-efficiency and generalization across environments, and that dynamics modeling with expert demonstrations is more effective than with non-experts. Grounding Aleatoric Uncertainty for Unsupervised Environment Design Authors: Minqi Jiang, Michael Dennis, Jack Parker-Holder, Andrei Lupu, Heinrich Küttler, Edward Grefenstette, Tim Rocktäschel, Jakob Foerster Adaptive curricula in reinforcement learning (RL) have proven effective for producing policies robust to discrepancies between the train and test environment. Recently, the Unsupervised Environment Design (UED) framework generalized RL curricula to generating sequences of entire environments, leading to new methods with robust minimax regret properties. Problematically, in partially-observable or stochastic settings, optimal policies may depend on the ground-truth distribution over aleatoric parameters of the environment in the intended deployment setting, while curriculum learning necessarily shifts the training distribution.*****In the #grounded-qa-bot channel in the Cohere co:mmunity on Discord , there’s a bot that will answer your questions. Ask any question in the chat, add the question mark emoji, and the co_search bot will attempt to answer the question. This answer, however, is not simply a GPT model’s output to an input prompt. It is informed by a web search. There have been recent large language models (LLMs) from research labs that are trained on using a database or have the ability to search the web for information. Unlike those custom models, this is a bot that you, a developer who does not necessarily work at a giant tech company, can build today.*****In contrast to keyword search, the focus is on the actual semantic content of documents. For example, if the user searches for “United States,” semantic search will also find documents talking about “U.S.” and “U.S.A.” Similarly, when the lawyer searches for “NDA,” the language model that powers semantic search will find all contracts and paragraphs with relevant information on this topic.*****The most obvious example use case for this is search engines. As users, we expect the search results to return links and documents that are highly relevant to our query. What makes modern search engines work very well is their ability to match the query to the appropriate results not just via keyword-matching, but by semantic similarity . In simple words, they are able to perform matching based on meaning, context, themes, ideas — abstract concepts that may use different words altogether, but very much relate to each other. Let’s say a user enters the search string “ground transportation at the airport.”*****## New Features Interactive quickstart tutorials! Improved information architecture! UI refresh! Try out the new experience, and let us know what you think.*****## Use Cases Here are a few examples of language understanding systems that can be built on top of large language models.*****### Classify Classification is one of the most common use cases in language processing. Building systems on top of language models can automate language-based tasks and save time and energy. There's more than one way to build a classifier on top of Cohere's language models. It's worth experimenting to see which method works best for your use case. The simpler methods can get you quick results, while the more advanced methods need more data and will lead to better results. On the simpler side are methods like using the Classify endpoint for classification . More industrial-grade classifiers can be built by fitting a classifier on top of the embed endpoint (see:*****### Summarize Large language models present a breakthrough in text generation. For the first time in history, we have software programs that can write text that sounds like it’s written by humans. These capabilities open doors to use cases like summarization or paraphrasing. A summarization prompt in the Cohere playground shows this output (in bold): Large language models can be adapted to new tasks with impressive speed. For tasks which appear in the training data (i.e. documents on the web), language models can successfully summarize text without being shown any examples at all. Summarization and paraphrasing both use the generate endpoint.*****Cohere offers an API to add cutting-edge language processing to any system. Cohere trains massive language models and puts them behind a simple API. Moreover, through training, users can create massive models customized to their use case and trained on their data. This way, Cohere handles the complexities of collecting massive amounts of text data, the ever evolving neural network architectures, distributed training, and serving models around the clock. Two major categories of large language models are generative language models (like GPT2 and GPT3) and representation language models (like BERT). Cohere offers variants of both types.*****The Cohere platform builds natural language understanding and generation into your product with a few lines of code. Our large language models can solve a broad spectrum of natural language use cases, including classification, semantic search, paraphrasing, summarization, and content generation. By training a custom model , users can customize large language models to their use case and trained on their data. The models can be accessed through the playground , SDK and the CLI tool.*****## Creating a Cohere API key Cohere API keys authenticate requests to the Cohere endpoints. The Cohere web console and CLI provide features to manage the API key generated for your Cohere account. You’ll use your API key to connect to Cohere when you have a batch of tweets ready for analysis. Using your browser, navigate to the tab of your Cohere console to generate an API key. Click to launch a dialogue box for specifying the name of your API key. Enter your preferred text in the field to name the API key. Click on the dialog box to save the API key.*****#### Frequently Asked Questions 1. How do I get a Trial API Key? When an account is created, we automatically create an Trial API key for you. This API key will be available on the dashboard for you to copy, as well as in the dashboard section called “API Keys.” 2. How do I get a Production API key? To get a Production key, you'll need to have Owner privileges (or ask your organization Owner to complete the following steps). Navigate to the Billing and Usage page in your Cohere dashboard. Click on the button and fill out the Go to Production workflow.*****Other Questions What is the difference between a Trial API key and Production API key? Are there any account limitations upon signup? What is the difference between an organization and a personal account? Which model should I pick? When do I get billed?*****So, we’re replacing our current $75 free credit program with a new freemium tier that gives you more hands-on experience with our API. To enable the free tier, we’re introducing two types of API keys that give users access to all of Cohere’s endpoints: Trial API keys and Production API keys. All current API keys will automatically be converted to a Trial API key, which is rate-limited to 100 API calls per minute and cannot be used in production scenarios. by signing up for an account . For developers looking for higher rate limits, or to serve Cohere to users in an application, you can upgrade to a Production API key where the throughput is 100 times higher (10,000 calls per minute).*****# Getting Started You’ll start by creating a Cohere API key, which allows you to access large language models from your code. Log in to the Cohere Dashboard , scroll to the bottom to find the section, and click . Provide a name for the API Key, such as “Feedback Bot,” and click the button. Copy and save the newly generated key, as you’ll need it to use the Cohere API.*****### Getting Your API Key Next, you’ll need to get your API key. After signing up or logging in, go to the Cohere dashboard . Once you see it, click on the button at the bottom of the screen. After clicking on the button, proceed to enter the API name (enter any name of your choice) in the popup and click the button. Copy the API key and store it safely, as you’ll need it later.*****## Trial Key Limitations Trial keys for all endpoints are rate-limited at 100 calls per minute. If you’d like to use Cohere’s endpoints in a production application or require higher throughput from our endpoints for your usage, you can upgrade to a Production key. With a Trial Key: Organizations can still have unlimited trial keys in the free tier. There is a defined usage limit on all the development API keys per minute (all keys add up to that rate limit). When a developer/org reaches a rate limit, they will receive an error that they have exceeded the limit/minute. Playground usage counts toward your Trial key rate limit.*****The process takes less than 3 minutes to finish, and enables you to generate a Production key that you can use to serve Cohere APIs in production. If you deploy without completing the Go to Production workflow, your API key may be temporarily or permanently revoked.*****The holidays are a time for celebration and community, and what better way to wrap up 2022 than with the coolest demos from Cohere community makers! Our year-end co:lab fridays meetup included a bit of fun with prizes for the most festive community members and a Christmas trivia contest. Check out the recap below, or watch the full session .*****train the model. If you're interested in training a model, please submit a Full Access request from your Cohere Dashboard. Try asking the model to do any of the following: Summarize a paragraph of text Generate SEO tags for a blog post Produce some questions for your next trivia night Provide ideas of what to do in your city this weekend In each case, give the model a few examples your desired output. Additionally, note the Calculate Likelihood button on the bottom right-hand corner. This feature outputs the likelihood that each token would be generated by the model in the given sequence, as well as the average log-likelihood of each token in the input.*****## A Little Holiday Cheer In our December session, we were filled with holiday spirit, and hosted a little competition for the most festive attire. We then got hands on with Elaine’s Trivia Generator and had a Christmas-themed trivia showdown. What country is the home of Santa Claus? Watch the session to find out ! Co:lab friday community demos happen on the last Friday of every month. Be sure to Register for our next event on If you have a Cohere-powered demo you’d like to share at co:lab fridays, please fill out our application form . P.S. Also, join the co:mmunity conversation on Discord*****#### Generate Produce text completions based on a given input. Model Price per Unit Default $2.5 per 1000 Generation Units* Custom $5.0 per 1000 Generation Units* More details*****for more about toxic language model degeneration). We have put safeguards in place to avoid generating harmful text, but we highly recommend that developers build additional guardrails to ensure that text presented to end users is not toxic or harmful. Language models capture problematic associations and stereotypes prominent on the internet and society at large. They should not be used to make decisions about individuals or the groups they belong to. For example, it is dangerous to use Generation model outputs in CV ranking systems due to known biases ( Nadeem et al., 2020 ).*****## Potential for Misuse Guided by the NAACL Ethics Review Questions , we describe below the model-specific concerns around misuse of the Generation model. By documenting adverse use cases, we aim to encourage Cohere and its customers to prevent adversarial actors from leveraging our models to the following malicious ends. The examples in this section are and are only meant to illustrate our understanding of potential harms. The examples are meant to be more model-specific and tangible than those in the Usage Guidelines . Each of these malicious use cases violates our usage guidelines and Terms of Use, and Cohere reserves the right to restrict API access at any time.*****## Intended Use Case Generations may be used for interactive autocomplete, augmenting human writing processes, summarization, text rephrasing, and other text-to-text tasks in non-sensitive domains. Outputs from Classify can be used for classification and analysis tasks, such as selecting the most likely completion for a sentence. Token likelihoods from Likelihood might be used to make fun claims about the “randomness” of your favourite author’s writing, or to explore the statistical differences between human-written and machine-generated text (see Gehrmann et al., 2019 ). Generations can address a large number of use cases. Check the presets for interesting examples.*****### Model Toxicity and Bias Language models learn the statistical relationships present in training datasets, which may include toxic language and historical biases along race, gender, sexual orientation, ability, language, cultural, and intersectional dimensions. We recommend that developers using the Generation model take model toxicity and bias into account and design applications carefully to avoid the following: Despite our ongoing efforts to remove harmful text from the training corpus, models may generate toxic text. This may include obscenities, sexually explicit content, and messages which mischaracterize or stereotype groups of people based on problematic historical biases perpetuated by internet communities (see Gehman et al., 2020*****Aidan here, cofounder of Cohere. Creation runs on community and collaboration. And it’s community that will drive new capabilities on Cohere, get bugs fixed, and establish our Cohere SS22 swag -> dev community pipeline 😈. That’s why today, I’m stoked to announce our new Discord server where people can share discoveries, talk about what they’re working on, and chat with the team at Cohere. We’ll also be using the channel to share open positions at Cohere, upcoming events and product updates. Our team is excited at the opportunity to build in the open, share cool projects we’ve been working on internally, and answer all your questions in real-time.*****### Make online communities safe for everyone Toxicity is rife in many types of communities today. In multiplayer gaming, for example, four out of five players have experienced some form of harassment. By augmenting your content moderation stack with Cohere’s Classify, you can accelerate the fight against toxicity on your platform. As a result, you’ll be better equipped to build safer communities, minimize harmful content, and reduce churn. Prevent customers from leaving your platform People come to your platform to enjoy your content and build relationships, but toxic experiences drive many users away — often permanently. Cohere can help you increase retention by maintaining safe online spaces that make your users feel comfortable.*****Hackathons are where the magic of making cool stuff happens. Many startups are born at a hackathon, and it’s also how many friendships and lasting memories are made. Getting together and staying up all night to build something that earns you bragging rights is actually kinda awesome! The Cohere community is home to experienced hackers, and I'm super stoked to share that we've partnered with lablab.ai , a hackathon community that aims to support developers in building their skills within the artificial intelligence ecosystem. lablab.ai is an organization that hosts online and onsite hackathons leveraging state-on-the-art AI frameworks. Not only do they host awesome events, but they're also constantly innovating, so that their hackathons can have a broader impact.*****## Growing a Global Community The C4AI Community launched alongside Cohere For AI as a space for anyone interested in machine learning—from lifelong learners to seasoned engineers and all points in between—to connect and collaborate. In the span of six short months, the C4AI Community has grown to include over 800 members from 90+ countries around the world. Even more impressive than the numbers is the spirit of generosity, creativity, and collaboration that animates our community. From reading groups to mentorship to lightning talks, community members constantly share their skills to elevate and empower their fellow members.*****It’s a brand new year, and that means another chapter of creativity and innovation coming from the Cohere community. Our first co:lab fridays meetup of the year was hosted by Roy Lim, Marketing Events Manager for Dev Community at Cohere, and featured another exciting lineup of cool demos from community members around the world. Check out the recap below, or watch the full session .*****Hello Cohere Makers, We are so stoked to see you getting creative with our API! To showcase the amazing work of our community, we’ll be hosting live co:lab friday Community Demos sessions featuring your Cohere-powered projects. These special sessions will begin on and will happen on the during our regular co:lab friday meetup time, . The Community Demos session is your place to chat about what you are building, so you can get feedback on your project, well deserved love from the community, and practice your pitch game in case you need it later on! 🔥 Whenever you're ready, you can sign up to present your demo using the form linked below, and we will add you to the lineup for the upcoming session.*****The above code is where we hard-code our training examples as a prompt and package it up with other hyperparameters like temperature, max tokens, and so on. The endpoint has several hyperparameters, some of which are listed below: You can learn more about this here . You’ll also notice we pass ‘--’ for our stop sequence hyperparameter. If you look carefully at the prompt, you’ll notice we use this sequence between our examples. These let Cohere know when to stop generating. Let’s send this package, along with our authentication and metadata to the Cohere Generate Endpoint. And finally, let’s parse the response and output the summary.*****Let’s build our Classify function! This is a basic function definition taking in two parameters. The first one (s_args) is the range of cells for our training data. The second one (s_input) is the new text we want to label or classify. Let’s start adding our logic between the curly braces. First, we want to extract the training data from s_args and package it into something that the Cohere API can ingest. Our ‘data’ package is ready with a list of labeled ‘examples’ and the unlabeled ‘input’ we want to classify. Let’s call the Cohere API to classify this. We’re going to pass through our data package along with our authentication headers and other metadata to the Cohere Classify endpoint and then store the response in an aptly named ‘response’ object.*****The Cohere Platform CLI Tool is an alternative to our web interface, which allows you to login to your Cohere account, manage API Keys, and run finetunes. This CLI tool is POSIX compliant (you can expect arguments and flags to work the same as they do with other popular CLI tools). Don't forget to use co --help or co [COMMAND] --help if you don't want to check back to this page!*****🚧 This model is in active development. Our team is pushing hard to ensure command is safe, and helpful for our users. As you experiment with the model and run into issues, please help us by flagging it to our team by emailing us at team@cohere.com. Command is a generative model that responds well with instruction-like prompts, and is available in two sizes: medium and xlarge . The xlarge model demonstrates better performance, and medium is a great option for developers who require fast response, like those building chatbots. To reduce the turnaround time for releases, we have nightly versions of command available.*****### Ask a Generative Model Now that we have relevant results, we can add them to a prompt and pose the question to the generative model. Because we’re aiming for factual information, we set a low temperature value (0.3) to reduce the creativity of the model in generating this answer. The response generated by the model is posted as an answer alongside the URLs of the web pages that informed it.*****for more about toxic language model degeneration). We have put safeguards in place to avoid generating harmful text, but we highly recommend that developers build additional guardrails to ensure that text presented to end users is not toxic or harmful. Language models capture problematic associations and stereotypes prominent on the internet and society at large. They should not be used to make decisions about individuals or the groups they belong to. For example, it is dangerous to use Generation model outputs in CV ranking systems due to known biases ( Nadeem et al., 2020 ).*****### Generate output Finally, we’ll generate the output, giving us the generations response. Putting everything together, we get the code below. Here’s a sample output returned: The output is not bad, but it could be better. If we were writing a blog post for example, the tone of the generated output wouldn’t fit very well. Also, there is no natural ending to the generated text, which looks like it could continue for quite some time. We need to find a way to make the output tighter and closer to how we want it to be, which is where we leverage prompt engineering*****I also participated in the Accessibility Summit last year where we discussed approaches to make the software more accessible." Feedback: The response is well-written and provides some insight into how the candidate designs for accessibility. However, it could be longer and more detailed and include their experience at the Accessibility Summit, such as what they did, discussed, and learned. Question: "What do you know about distributed systems?" Response: "I have experience architecting applications that use message passing between different devices to solve complex problems." Feedback: This response is short and vague. The candidate should have gone into more detail about how their task was related to distributed systems and how their experience taught them about working with distributed systems.*****The second part of the tool handles cases where the answer to the customer’s question is not in the knowledge base or when the question is not relevant to the product. To do this, the tool uses the Generate endpoint to provide a reasonable response to the customer’s query. In this example, the customer’s statement has not specified the product for which they need information. In this instance, Cohere Generate returns the following response:*****Cohere's multilingual text understanding model is now available! The multilingual-22-12 model can be used to semantically search within a single language, as well as across languages. Compared to keyword search, where you often need separate tokenizers and indices to handle different languages, the deployment of the multilingual model for search is trivial: no language-specific handling is needed — everything can be done by a single model within a single index. In addition to our new model, you can now detect the language of a data source using co.detect_language() endpoint. For more information, see our multilingual docs .*****## How Does the Multilingual Text Understanding Model Work? Cohere’s multilingual text understanding model maps text to a semantic vector space (also known as “ embeddings ”), positioning texts with a similar meaning in close proximity. This process unlocks a range of valuable use cases for multilingual settings. For example, one can map a query to this vector space during a search to locate relevant documents nearby. This often yields search results that are several times better than keyword search. To train multilingual models, you need large quantities (hundreds of millions) of suitable training pairs, like question/answer pairs. So far, such training data has been primarily available in English, and prior work tried to use machine translation to map it to other languages.*****## What is a Multilingual Text Understanding Model? Multilingual text understanding models are powerful models that can derive insights from text data across languages. At Cohere, we’ve trained our model specifically to be used for and While many of these models are available for English, similar existing multilingual models only work well for short sentences and can’t capture the meaning behind longer text. This prevents them from being used for semantic search, which typically aims to match a short query with a longer, relevant document. In this blog post, we will cover three relevant use cases that showcase the power of Cohere’s new multilingual model:*****### Develop, test, and experiment with the industry’s first multilingual text understanding model that supports 100+ languages Humans speak over 7100 languages, yet the majority of language models only support the English language. This makes it incredibly challenging to build products and projects using multilingual language understanding. Cohere’s mission is to solve that by empowering our developers with technology that possesses the power of language. That’s why today we’re introducing our first multilingual text understanding model that supports over 100 languages and delivers 3X better performance than existing open-source models. This will enable new markets, countries, and global companies to better serve their customers across the globe.*****The multilingual-22-12 model can be used to semantically search within a single language, as well as across languages. Compared to keyword search, where you often need separate tokenizers and indices to handle different languages, the deployment of the multilingual model for search is trivial: no language-specific handling is needed — everything can be done by a single model within a single index. We’re extremely proud of the performance of our multilingual understanding model. It outperformed the industry standard (the next best model) in search tasks by more than 230%.*****## Getting Started with Cohere’s Multilingual Model To get started using Cohere’s multilingual model, just create a free account and get your API key . You can then either query our REST API endpoints or install our SDK to use the model within Python. The following video navigates through the Cohere Platform to select the multilingual model, and shows how the multilingual model can embed text from multiple languages into the embedding space. Additionally, we have added to the Cohere Sandbox , a collection of experimental, open-source GitHub repositories  that make building applications using large language models fast and easy with Cohere.*****This is what makes the Cohere multilingual model so powerful: it has seen thousands of topics in each language.*****## Training Data Training embedding models require data in a specific format. For example, question/answer pairs or title/document pairs. We can then learn which text pairs should be closer together in the vector space in order to enable applications like semantic search. To train Cohere’s new multilingual model, we processed and carefully cleaned terabytes of data from various sources: Wikipedia, news publications, scientific articles, and online communities across hundreds of languages. This resulted in a large training corpus of more than 900 million training pairs for English and 450 million training pairs for other languages. Other multilingual embedding models often rely on machine translation for training dataset creation, which creates an awkward bias for these models.*****## Final Thoughts At Cohere, we are committed to breaking down barriers and expanding access to cutting-edge NLP technologies that power projects across the globe. By making our innovative multilingual language model available to all developers, we continue to move toward our goal of empowering developers, researchers, and innovators with state-of-the-art NLP technologies that push the boundaries of language AI. Sign up and try our new multilingual model for free. If you would like to discuss your multilingual use case, please don’t hesitate to contact us . 1. “Ethnologue: Languages of the World,” Ethnologue , 2022 (accessed Nov. 25, 2022).*****#### Frequently Asked Questions 1. How do I get a Trial API Key? When an account is created, we automatically create an Trial API key for you. This API key will be available on the dashboard for you to copy, as well as in the dashboard section called “API Keys.” 2. How do I get a Production API key? To get a Production key, you'll need to have Owner privileges (or ask your organization Owner to complete the following steps). Navigate to the Billing and Usage page in your Cohere dashboard. Click on the button and fill out the Go to Production workflow.*****Other Questions What is the difference between a Trial API key and Production API key? Are there any account limitations upon signup? What is the difference between an organization and a personal account? Which model should I pick? When do I get billed?*****So, we’re replacing our current $75 free credit program with a new freemium tier that gives you more hands-on experience with our API. To enable the free tier, we’re introducing two types of API keys that give users access to all of Cohere’s endpoints: Trial API keys and Production API keys. All current API keys will automatically be converted to a Trial API key, which is rate-limited to 100 API calls per minute and cannot be used in production scenarios. by signing up for an account . For developers looking for higher rate limits, or to serve Cohere to users in an application, you can upgrade to a Production API key where the throughput is 100 times higher (10,000 calls per minute).*****## Trial Key Limitations Trial keys for all endpoints are rate-limited at 100 calls per minute. If you’d like to use Cohere’s endpoints in a production application or require higher throughput from our endpoints for your usage, you can upgrade to a Production key. With a Trial Key: Organizations can still have unlimited trial keys in the free tier. There is a defined usage limit on all the development API keys per minute (all keys add up to that rate limit). When a developer/org reaches a rate limit, they will receive an error that they have exceeded the limit/minute. Playground usage counts toward your Trial key rate limit.*****## Go to Production You must acknowledge Cohere’s SaaS Agreement and Terms of Service. Your organization must also read and recognize our Model Limitations, Model Cards, and Data Statement. You will be asked if your usage of Cohere API involves any of the sensitive use cases outlined in our Usage Guidelines. Following your acknowledgement of our terms, you will be able to generate and use a Production key immediately. However, if you indicate your usage involves a sensitive use case, your Production key will still be rate limited the same as a Trial key until our Safety team reaches out and manually approves your use case.*****## Generating Your API Key On the left menu, click and create a trial API key. You use this key to gain access to the Cohere API and Generate endpoint. It’s important to note that trial keys are free, but they’re rate-limited, meaning it’s not advisable to use them in production applications.*****If calls exceed the throttling we throw an error that says “Trial keys are throttled." Please upgrade your API key or contact us directly on Discord . Trial keys are free to use even after you upgrade to a Production key.*****## Intended Use Case Generations may be used for interactive autocomplete, augmenting human writing processes, summarization, text rephrasing, and other text-to-text tasks in non-sensitive domains. Outputs from Classify can be used for classification and analysis tasks, such as selecting the most likely completion for a sentence. Token likelihoods from Likelihood might be used to make fun claims about the “randomness” of your favourite author’s writing, or to explore the statistical differences between human-written and machine-generated text (see Gehrmann et al., 2019 ). Generations can address a large number of use cases. Check the presets for interesting examples.*****Large language models (LLMs) are versatile. Their general purpose nature makes them adept at handling a wide range of tasks. However, because of the same reason, sometimes it may not be immediately obvious as to how to apply LLMs for specific tasks. If you are thinking about the next thing you want to build with LLMs, hopefully this article will help you get up and running with generating use case ideas. In Part 1 , we looked at how to get started with text generation. In particular, we looked at two ways you can influence the output of a model: via prompts and model parameters.*****We also looked at a few examples, such as generating ad copy, summarizing emails, writing haikus, and more. In this Part 2 article, we’ll continue our exploration of the use cases for generative AI. We’ll first look into an ideation framework that was specifically designed for LLMs, and then look at some specific examples. While the applications of LLMs are broad, the key to ideating a new one is to get specific about what you want to achieve. And to be able to do that, it’s helpful to understand where and how LLMs can be applied. There are a number of ways that you can look at it, and here we propose three:*****### Building a Chatbot with a Persona This is an example of a conversational use case. Try the preset here . If you are looking for more examples, visit this page to get more use case ideas.*****“why” how” “what” Your idea is probably sitting at the intersection of these three areas. Getting specific in each of these areas can help you crystalize your target use case. The idea that you are looking for is probably sitting at the intersection of these three areas. Let’s now go deeper into each.*****Given a simple prompt, such as “cave monster,” the app uses Cohere Generate to create an image, description, and even a backstory for a new creature. The team built their app during a Cal Hacks 9.0 hackathon. See their GitHub repo .*****## What is Generative AI? Generative AI is a type of artificial intelligence that focuses on creating or generating new content or data. This can be in the form of language, images, videos, and more. Its market potential is significant as it has the potential to revolutionize many industries and drive innovation in a wide range of fields. For example, in creative arts, generative AI can be used to generate unique and engaging content, such as music or visual art, with minimal need for human input. In the business world, generative AI can be used to generate reports, presentations, and other business documents, reducing the need for manual data analysis and enhancing productivity.*****### Join Cohere We’re always looking for new people to join our team. Check out our listings below or send us an email at talent@cohere.com Stay updated*****## Get Started Create an account instantly to get started. You can also contact us to design a custom package for your business. Stay updated*****### Reflecting on the moments that make us the best place to work in NLP 2022 was a significant year for AI. Generative AI, in particular, captured the public’s imagination with its ability to create images from any imaginable text prompt and its ability to power chatbots with capabilities far beyond what was commonly thought possible. Now there are more eyes on our industry than ever before, and they’re looking for meaningful applications of generative and language technologies. At Cohere, we’re on a mission to deliver them. Throughout 2022, we focused on helping businesses of all sizes better take advantage of language AI in the real world, whether through predictive text generation like in copywriting or other functions like search, conversational AI, summarization and content moderation.*****## Maximize your Content Potential Using the Cohere Generate endpoint, your business teams can produce as much content as they need now, and have time to easily explore even more ways to make content deliver results for the business.*****The following image is a screenshot of Cohere's playground. In the following example, we are giving it the intent: Play Music . With that, we are supplying it with a bunch of examples. When we click the generate button, it will generate relevant text. The following image demonstrates another example of using the Cohere playground to generate text. Here, we are feeding it with an intent, for instance, setting an alarm or a reminder.*****# Meet your AI-generated content writer Generate is powered by a large language model that has read billions of words, learning the patterns and idiosyncrasies of sentences. Using this knowledge, it writes content, predicts outcomes or answers questions at your command.*****### Our Products Find out how developers are using NLP to power everyday apps and experiences. Classify Organize information at a gargantuan scale. Give Classify labels to bucket text into, then set it to work across tasks like content moderation or chatbot responses Generate Picture a large language model that can be used to write or summarize copy for just about any other application you can think of. That’s Generate. Embed Imagine tasking AI to read every single Reddit post about your company, then plot it into an easy-to-understand graph. You can do that, and more, with Embed.*****🚧 This model is in active development. Our team is pushing hard to ensure command is safe, and helpful for our users. As you experiment with the model and run into issues, please help us by flagging it to our team by emailing us at team@cohere.com. Command is a generative model that responds well with instruction-like prompts, and is available in two sizes: medium and xlarge . The xlarge model demonstrates better performance, and medium is a great option for developers who require fast response, like those building chatbots. To reduce the turnaround time for releases, we have nightly versions of command available.*****## This Month’s Hot Topic: The Command Nightly Model Luis Serrano, our Head of Developer Relations, walked us through Cohere’s newest model — Command Nightly . To reduce the turnaround time for releases, we’re making nightly versions of our Command model available, which means that every week you can expect the performance of Command Nightly to improve. Command is a generative model that responds well with instruction-like prompts, and is available in two sizes. X-Large demonstrates better performance and Medium produces a faster response.*****### Prompting by Instruction The Command-Xlarge model works best when we provide an instruction-based prompt. One way to do this is by using imperative verbs to tell the model what to do, for example: generate, write, list, provide, and other variations. Let’s say that we are creating social media ad copy for a wireless earbuds product. We can write the prompt as follows. Generate a social ad copy for the product: Wireless Earbuds. At this point, ensure that you select command-xlarge in the MODEL dropdown in the right pane. Then, click on Generate. This generates the following output. That’s not bad.*****## Selecting the Model We have only used one model so far: command-xlarge-nightly . But as we discussed in Part 1, you can prompt Cohere’s text generation model in two ways: by instruction and by example. So, the first thing that you want to define when calling the endpoint is the model type, depending on how you are constructing your prompt. Here are the available models at the time of writing: command-xlarge-nightly command-medium-nightly xlarge medium The sizes implied in the model names represent the parameter size of the models. So, which one do you choose? It depends on your use case, but as a rule of thumb, smaller models are faster, while larger models are generally more fluent and coherent.*****The machine learning field currently has too few points of entry, especially depending on where you are in the world. With Cohere For AI, Sara’s vision is to change how, where and by whom research is done. Cohere For AI represents the opportunity to make an impact in ways that don’t just advance progress on machine learning research, but also create new points of entry into the field. Our lab will focus on solving today’s complex machine learning challenges––exploring the unknown, together. Our values will be based on the following principles: GitHub Cohere For AI is community-driven and motivated by the opportunity to create an inclusive, distributed community made up of brilliant researchers and engineers from across the globe.*****# Meet your AI-generated content writer Generate is powered by a large language model that has read billions of words, learning the patterns and idiosyncrasies of sentences. Using this knowledge, it writes content, predicts outcomes or answers questions at your command.*****#### Generate Produce text completions based on a given input. Model Price per Unit Default $2.5 per 1000 Generation Units* Custom $5.0 per 1000 Generation Units* More details*****Free, rate limited Trial Keys for experimentation, testing, and playground usage Production keys with no rate limit for serving Cohere in production applications Flat rate pricing for Generate and Embed endpoints Reduced pricing for Classify endpoint New UI for dashboard including sign up and onboarding - everything except playground New use-case specific Quickstart Guides to learn about using Cohere API Replacing "Finetune" nomenclature with "Custom Model" Inviting team members is now more intuitive. Teams enable users to share custom models with each other Generative custom models now show accuracy and loss metrics alongside logs Embed and Classify custom models now show logs alongside accuracy, loss, precision, f1, recall*****Here’s a summary of the new pricing model per endpoint: We also want to make it easier for you to use our largest, best performing models by removing the pricing differences by model size — pricing of each endpoint will be standardized across sizes. This way, it’s a no brainer to get the best product Cohere has to offer! Custom Generate and Embed models will continue to be twice the price of their standard counterparts, whereas pricing for both Classify categories remain the same. Here’s a summary of the new costs: 3. For starters, we’re removing jargon in our product terminology.*****You can get started with as few as 32 examples (the minimum the platform accepts) but for the best performance, try experimenting in the region of hundreds or thousands of examples, if you have access to the data needed. Note that finetuning with the Cohere Platform is free, so you can create multiple custom models without worry. There is a price difference though for calling the model, but with the free developer trial key, you don’t have to worry about this either. This article comes with a Google Colaboratory notebook for reference.*****### Tempering excitement with care As social media gets swept up in posts that claim “ I made model X do impossible task Y 🤯 ”, it’s important to arm oneself with a discerning eye to filter these claims. One of the key questions to ask is whether a demonstrated capability is a 🍒 cherry-picked example that a model produces 40% of the time, or if it points to robust and reliable model behavior. Reliability is key for an AI capability to become part of a customer-facing product. Take for instance, the many capabilities attributed to large GPT models in the last few years.*****## What is Generative AI? Generative AI is a type of artificial intelligence that focuses on creating or generating new content or data. This can be in the form of language, images, videos, and more. Its market potential is significant as it has the potential to revolutionize many industries and drive innovation in a wide range of fields. For example, in creative arts, generative AI can be used to generate unique and engaging content, such as music or visual art, with minimal need for human input. In the business world, generative AI can be used to generate reports, presentations, and other business documents, reducing the need for manual data analysis and enhancing productivity.*****Other Questions What is the difference between a Trial API key and Production API key? Are there any account limitations upon signup? What is the difference between an organization and a personal account? Which model should I pick? When do I get billed?*****These models aren’t only enabling new features and products. In fact, entire new sectors of companies are based on these models as their foundation. One clear example here is the growing list of companies building AI writing assistants. This includes companies like HyperWrite , Jasper , Writer , copy.ai , and others. Another example is companies weaving model generations into interactive experiences like Latitude , Character AI , and Hidden Door .*****With a simple, one-line prompt, we already have a piece of ad copy that will make a digital marketer proud! But perhaps we want to be more specific in terms of what we want the output to look like. For this, we can layer additional instructions onto the model in the prompt. Let’s say that we want the model to provide the ad copy in the form of a well–known copywriting technique, called the AIDA Framework (which stands for Attention, Interest, Desire, and Action). In this case, we can append this specific instruction in the prompt as follows. Generate an ad copy for the product: Wireless Earbuds.*****### Our Products Find out how developers are using NLP to power everyday apps and experiences. Classify Organize information at a gargantuan scale. Give Classify labels to bucket text into, then set it to work across tasks like content moderation or chatbot responses Generate Picture a large language model that can be used to write or summarize copy for just about any other application you can think of. That’s Generate. Embed Imagine tasking AI to read every single Reddit post about your company, then plot it into an easy-to-understand graph. You can do that, and more, with Embed.*****Cohere offers an API to add cutting-edge language processing to any system. Cohere trains massive language models and puts them behind a simple API. Moreover, through training, users can create massive models customized to their use case and trained on their data. This way, Cohere handles the complexities of collecting massive amounts of text data, the ever evolving neural network architectures, distributed training, and serving models around the clock. Two major categories of large language models are generative language models (like GPT2 and GPT3) and representation language models (like BERT). Cohere offers variants of both types.*****## Deploy the SageMaker endpoint using a notebook Cohere has packaged Medium models, along with an optimized, low-latency inference framework, in containers that can be deployed as SageMaker inference endpoints. Cohere’s containers can be deployed on a range of different instances (including ml.p3.2xlarge, ml.g5.xlarge, and ml.g5.2xlarge) that offer different cost/performance trade-offs. These containers are currently available in two Regions: us-east-1 and eu-west-1. Cohere intends to expand its offering in the near future, including adding to the number and size of models available, the set of supported tasks (such as the endpoints built on top of these models), the supported instances, and the available Regions.*****### Power your content apps with NLP It’s easy to integrate Cohere into your apps using the platform’s simple API and set of language-specific SDKs.*****### News articles Build apps that extract and summarize insights from news articles across multiple industries, so your users can focus on higher level tasks. No model training required.*****# Unlock the power of words with accurate, affordable NLP Our pretrained text generation models let you quickly build apps that help you or your customers be productive or save time.*****### 2. Try multiple formulations of your prompt to get the best generations When using generate, it is useful to try a range of different prompts for the problem you are trying to solve. Different formulations of the same prompt which might sound similar to humans can lead to generations that are quite different from each other. This might happen, for instance, because our models have learned that the different formulations are actually used in very different contexts and for different purposes. Below we give a number of examples that we've found to work particularly well for different tasks. In the summarization example, if “In summary,” doesn’t lead to a good generation, we may want to try “To summarize in plain language,“ or “The main point to take from this article is that”.*****## Automate the process In actual applications, you will likely need to produce these text generations on an ongoing basis, given different inputs. Let’s simulate that with our example. Automating generations from multiple prompts First, we create a list of new topics, so we can iterate on them and get the paragraphs generated. We then make some tweaks to the earlier prompt: we create a base prompt containing the examples, and then we append it to the current prompt, which is the new topic. These steps are shown below. The list of topics: The base prompt: Set up the model: Iterate on the topics:*****We show that the translation quality and the domain of the in-context examples matter and that 1-shot noisy unrelated example can have a catastrophic impact on output quality. While concatenating multiple random examples reduces the effect of noise, a single good prompt optimized to maximize translation quality on the development dataset can elicit learned information from the pre-trained language model. Adding similar examples based on an n-gram overlap with the test source significantly and consistently improves the translation quality of the outputs, outperforming a strong kNN-MT baseline in 2 out of 4 out-of-domain datasets. Structured Prompting: Scaling In-Context Learning to 1,000 Examples*****A four-stage approach to teaching algorithmic reasoning to LLMs is identified and studied in this work: (1) formulating algorithms as skills, (2) teaching multiple skills simultaneously, (3) teaching skill composition, and (4) teaching skill utilization. The use of algorithmic prompting, which we call in-context learning, is shown to be an effective method of teaching algorithmic reasoning to LLMs. Language Models are Changing AI: The Need for Holistic Evaluation Authors: Rishi Bommasani, Percy Liang, Tony Lee We must be measured as language models generate excitement and fear. In order to be able to gain a better understanding of the technology and its societal impact, we need to know what it can and can't do, as well as what risks it poses.*****That was the idea behind the example tags I put in the prompt, which you can see on the Cohere Playground screenshot below: We call the endpoint by specifying a few settings, and it will generate the corresponding extractions.*****Get driving Every time a user enters a prompt into your chatbot, Cohere will classify the request by intent, allowing your bot to provide the most relevant answer back. With Cohere’s Classify, your AI-powered chatbots or conversational tools can consistently and accurately detect the intent behind user requests — including compound queries with multiple data points to consider. As a result, your bots are more intelligent and your customers are more satisfied with their interaction with your business. Ensure your chatbots speak the same language as your customers Making Cohere a part of your chatbot experience benefits both your customers and your team by*****### Being Creative vs. Predictable Probably the most useful set of parameters are the ones that we can tune to control the randomness of the output. The beauty of working with LLMs is, for the same prompt, the next generated token will not be the same every time. Rather, it is sampled from a long list of possible tokens. This is where the creative aspect of LLM comes from, allowing us to generate a variety of outputs, given the exact same prompt. But depending on your application, you may want to reduce, or increase, this level of randomness. You can do this by adjusting a number of parameters.*****Sampling from generation models incorporates randomness, so that the same prompt may yield different outputs each time you hit "generate". Temperature is a number used to tune the degree of randomness.*****To understand how model prompting works, let’s start by entering the following prompt into the playground. Once upon a time in a magical land called Clicking Generate gives us a continuation of the text (model-generated text is in ). This is the most basic form of prompting, which is simply asking the model to complete the text that we have entered. But this type of prompt is rather open-ended, and in more practical applications, you will need to make the prompt tighter, so that the output generated will be more predictable. With that, let’s now dive into how you can design more effective prompts.*****### Why Cohere Free Developer Tier Learn and iterate with the Cohere API free of charge until you go to production. Easy To Use No prior ML/AI experience required. Get started with just a few examples. Customizable Models Customize our models with your own data sets, and deploy them easily to production.*****## Trial Key Limitations Trial keys for all endpoints are rate-limited at 100 calls per minute. If you’d like to use Cohere’s endpoints in a production application or require higher throughput from our endpoints for your usage, you can upgrade to a Production key. With a Trial Key: Organizations can still have unlimited trial keys in the free tier. There is a defined usage limit on all the development API keys per minute (all keys add up to that rate limit). When a developer/org reaches a rate limit, they will receive an error that they have exceeded the limit/minute. Playground usage counts toward your Trial key rate limit.*****## Going Live Upon registration, every Cohere user receives a free, rate-limited Trial key to use with our endpoints. If you find that you are running against the Trial key rate limit or want to serve Cohere in production, this page details the process of upgrading to a Production key and going live.*****### 1. Comprehend Cohere ( Qiu et al., 2020 ) describes the history, technical aspects, and applications of pre-trained language models like the ones which power the Cohere Platform. We recommend reading this survey and other language modeling research to learn what kinds of knowledge are encoded in language models and how to use their outputs responsibly in downstream tasks. Language models might encode the following: such as subject-verb agreement, part-of-speech, and other simple syntactic structures ( Liu et al., 2019 ; Hewitt et al., 2019 ). including relational and commonsense knowledge such as where famous individuals were born or the color of the sky, limited by what is contained in the training data.*****Bullying, threatening, shaming, or doxxing. : Belittling victims of serious physical or emotional harm (even if unintentional). : Sharing of divisive generated content in order to turn a community against itself. : Perpetuating racism, or sexism (even if unintentional). Attempting to characterize gender, race, or ethnicity. : Distribution of sexually explicit acts, torture, or abuse. : Attempting to influence political decisions, or opinions. : Catfishing, phishing, or attempting to circumvent the law. : Sending unsolicited email and messages, or manipulating search engines. : Representing raw generations as coming from humans, using supervised generations with false identities, or a single person using generations with many identities that appear to be independent.*****If calls exceed the throttling we throw an error that says “Trial keys are throttled." Please upgrade your API key or contact us directly on Discord . Trial keys are free to use even after you upgrade to a Production key.*****#### Classify Analyze and organize text into specific categories. Model Price per Unit Default $2.0 per 1000 Classifications Custom $2.0 per 1000 Classifications More details*****# Classification Pricing  Classifications will cost $5 per 1000 text classified across the platform, regardless of model size (small, medium, large, xlarge) or usage of a finetuned model. Get an API Key Share what you're building on our co:mmunity forum or shoot us an email .*****Free, rate limited Trial Keys for experimentation, testing, and playground usage Production keys with no rate limit for serving Cohere in production applications Flat rate pricing for Generate and Embed endpoints Reduced pricing for Classify endpoint New UI for dashboard including sign up and onboarding - everything except playground New use-case specific Quickstart Guides to learn about using Cohere API Replacing "Finetune" nomenclature with "Custom Model" Inviting team members is now more intuitive. Teams enable users to share custom models with each other Generative custom models now show accuracy and loss metrics alongside logs Embed and Classify custom models now show logs alongside accuracy, loss, precision, f1, recall*****Here’s a summary of the new pricing model per endpoint: We also want to make it easier for you to use our largest, best performing models by removing the pricing differences by model size — pricing of each endpoint will be standardized across sizes. This way, it’s a no brainer to get the best product Cohere has to offer! Custom Generate and Embed models will continue to be twice the price of their standard counterparts, whereas pricing for both Classify categories remain the same. Here’s a summary of the new costs: 3. For starters, we’re removing jargon in our product terminology.*****#### Summarize Summarize a given input text such as an article into a short paragraph or key bullet points Model Price per Unit Default $5.0 per 1000 Summarization Units* Custom $10.0 per 1000 Summarization Units* More details API Key Offering API Key Offering*****#### What's possible with Classify Keep your community safe Use Classify to identify hate speech, abusive language, spam, profanity, or anything that meets user-provided filters. Harness intent recognition Leverage Classify to triage inbound chatbot or email requests to understand user intent and automatically issue responses. Serve your customers better Save time by tasking Classify to route inbound customer support requests to their respective teams. Access industry-leading sentiment analysis Develop a stronger customer affinity by classifying posts, reviews, etc to understand how they perceive your company/brand.*****#### Make it yours Our models have read billions and billions of words. But they can be made even more effective with a little input from you. Our finetuning feature allows you to tweak our base models to make them more applicable to your specific task or domain.*****## We are Cohere Cohere is here to give developers and businesses access to NLP, now. From our HQ in Toronto, our offices in Palo Alto and our new hub in London, we work at the cutting edge of machine learning to share the power of NLP with the world.*****Our service providers may be located in the U.S., Canada or other foreign jurisdictions. We and our Canadian, US and other foreign service providers may provide your personal information in response to a search warrant to other legally valid inquiry or order, or to another organization for the purposes of investigating a breach of an agreement or contravention of law or detecting, suppressing or preventing fraud, or as otherwise may be required or permitted by applicable Canadian, U.S. or other law or legal process, which may include lawful access by US or foreign courts, law enforcement or other government authorities. Your personal information may also be disclosed where necessary for the establishment, exercise or defence of legal claims and to investigate or prevent actual or suspected loss or harm to persons or property.*****## Making NLP a part of every application We work at the bleeding edge of machine learning. From our HQ in Toronto, our office in Palo Alto and our kitchen tables, we work to share NLP with the world.*****All of our data is hosted on databases. These databases are all located in the United States. Please reference the above vendor specific documentation linked above for more information. All databases are encrypted at rest. Our applications encrypt in transit with TLS/SSL only. We perform vulnerability scanning and actively monitor for threats. We actively monitor and log various cloud services. We use our data hosting provider’s backup services to reduce any risk of data loss in the event of a hardware failure. We utilize monitoring services to alert the team in the event of any failures affecting users. We have a process for handling information security events which includes escalation procedures, rapid mitigation and communication.*****“At Cohere, it’s our mission to change that by developing a solution that’s simple – yet effective – and available to any developer.” Cohere’s addition to Google Cloud Marketplace serves as another solution for businesses seeking cloud software that enhances and streamlines their operations. “Helping developers build NLP models into applications in more natural, streamlined ways has become a key focus area for enterprises today,” said Dai Vu, Managing Director, Marketplace & ISV GTM Initiatives, Google Cloud. “We’re thrilled that Cohere’s solution is available on Google Cloud Marketplace to help customers drive deeper insights from their language-processing operations.” We can’t wait to see how Google Cloud developers will solve problems or build innovations using Cohere.*****With many years of experience at the forefront of innovation in NLP, his research groups at DeepMind and Oxford have pioneered deep learning advances in machine translation, question answering, and language modelling. Phil will continue his research and teaching as a Professor of Computer Science at Oxford University and a fellow of St Hugh’s College. Ed joins as Head of Machine Learning to explore special projects aimed at expanding the functionality of Cohere’s platform. He brings a wealth of leadership experience and expertise in ML, having led initiatives across NLP, machine reasoning, grounded language acquisition, adaptive learning systems, and more at DeepMind and Facebook AI Research.*****#### Generate Produce text completions based on a given input. Model Price per Unit Default $2.5 per 1000 Generation Units* Custom $5.0 per 1000 Generation Units* More details*****#### Embed Capture the semantic meaning of text by representing text as numbers. Model Price per Unit Default $1.0 per 1000 Embeddings Custom $2.0 per 1000 Embeddings More details*****#### Summarize Summarize a given input text such as an article into a short paragraph or key bullet points Model Price per Unit Default $5.0 per 1000 Summarization Units* Custom $10.0 per 1000 Summarization Units* More details API Key Offering API Key Offering*****#### Classify Analyze and organize text into specific categories. Model Price per Unit Default $2.0 per 1000 Classifications Custom $2.0 per 1000 Classifications More details*****## Totally transparent pricing We're making it easy to explore, learn, and experiment with the Cohere Platform. All models Free Limit of 100 calls/minute Default models $ 1.00 per 1000 Embeddings Custom models $ 2.00 per 1000 Embeddings Examples Stay updated*****#### Frequently Asked Questions 1. How do I get a Trial API Key? When an account is created, we automatically create an Trial API key for you. This API key will be available on the dashboard for you to copy, as well as in the dashboard section called “API Keys.” 2. How do I get a Production API key? To get a Production key, you'll need to have Owner privileges (or ask your organization Owner to complete the following steps). Navigate to the Billing and Usage page in your Cohere dashboard. Click on the button and fill out the Go to Production workflow.*****Today, we’re thrilled to announce an entirely reimagined platform pricing structure designed to provide more flexibility, control, and value to every developer and business building on the Cohere Platform. Our new pricing opens up the platform to allow you greater access and a simpler experience when choosing the right resources for your project. You’ll be able to experiment more for free, estimate costs more easily, and see less jargon from us. Read on for the full details! 1. Natural language processing (NLP) has become part of the public consciousness due to its rapid evolution and increasing number of applications. We want to make it easier for every developer to explore the vast potential of NLP and experiment with it on the Cohere Platform.*****Other Questions What is the difference between a Trial API key and Production API key? Are there any account limitations upon signup? What is the difference between an organization and a personal account? Which model should I pick? When do I get billed?*****In recent tests, we conducted against three common open-source alternatives, the Cohere Multilingual Text Understanding Model is by far the best available multilingual embedding model, outperforming alternatives by significant margins.*****At Cohere, we are committed to breaking down barriers and expanding access to cutting-edge NLP technologies that power projects across the globe. By making our innovative multilingual language models available to all developers, we continue to move toward our goal of empowering developers, researchers, and innovators with state-of-the-art NLP technologies that push the boundaries of Language AI. Our Multilingual Model maps text to a semantic vector space, positioning text with a similar meaning in close proximity. This process unlocks a range of valuable use cases for multilingual settings. For example, one can map a query to this vector space during a search to locate relevant documents nearby.*****## Differences Between English and Multilingual Embedding Models Unlike our English language embedding model, our multilingual model was trained using dot product calculations. Using dot products produces a non-normalized similarity score, reflecting the magnitude of the two compared vectors. When this dimension is incorporated, multilingual embeddings perform better than standard. For more information on how our English language model works (using cosine similarity), see our introductory guide to the Cohere platform . The dimensions of our multilingual embeddings is 768 dimensions.*****## Benchmarks We extensively benchmarked our new model to ensure the best performance across a wide range of applications, domains and languages. Specifically, we used: MTEB BEIR BEIR Mr. Tydi MIRACL Amazon MASSIVE dataset We compared our results against other state-of-the-art multilingual embedding models, specifically paraphrase-multilingual-mpnet-base-v2 (the best model from Sentence-Transformers ), LaBSE (from Google), and Universal Sentence Encoder cMLM (from Google). The following chart shows how they compare: The Cohere multilingual-22-12 model performs much better in all use cases. In particular, we see a robust improvement in multilingual search. The other models we tested against perform rather poorly, in many cases less effectively than keyword search.*****## Multilingual Sentence Embeddings Most word and sentence embeddings are dependent on the language that the model is trained on. If you were to try to fit the French sentence “Bonjour, comment ça va?” (meaning: hello, how are you?) in the embedding from the previous section, it will struggle to understand that it should be close to the sentence “Hello, how are you?” in English. For the purpose of unifying many languages into one, and being able to understand text in all these languages, Cohere has trained a large multilingual model, that has showed wonderful results with more than 100 languages.*****#### What's possible with Embed Semantic Search Enable users to search using conversational language. Topic Modeling Cluster similar topics and discover thematic trends across a body of text sources. Recommendations Build a recommendation engine and engage your users with more relevant content. Multilingual embeddings Run topic modeling, semantic search, recommendations across 100+ languages with just one model. Read more .*****For example, you can finetune (or customize) our base language models in the Cohere Playground to add additional language understanding to your dataset, such as unique product names, community slang, or industry jargon, and keep that customized or finetuned model updated. For example, the diagram below shows how a search pipeline would work by creating metadata for each document, creating and storing the vectors for the documents, and finally embedding the query to compare against the stored vectors. The task is a similarity matching where it retrieves the most similar image. Our multilingual support adds industry-leading accurate understanding of 100 languages to assist multinational companies with drawing insights and use from language data across the globe.*****### Prompting by Instruction The Command-Xlarge model works best when we provide an instruction-based prompt. One way to do this is by using imperative verbs to tell the model what to do, for example: generate, write, list, provide, and other variations. Let’s say that we are creating social media ad copy for a wireless earbuds product. We can write the prompt as follows. Generate a social ad copy for the product: Wireless Earbuds. At this point, ensure that you select command-xlarge in the MODEL dropdown in the right pane. Then, click on Generate. This generates the following output. That’s not bad.*****It’s actually quite straightforward. We enter a prompt: And we get a response: Of course, there are more options available for you to define your call in a more precise way. In this article, we will cover that and more, including: An overview of the Generate endpoint Setting up Making the first API call Understanding the response Turning Playground prompts into code Selecting the model Understanding the other parameters Experimenting with a prompt But before going further, let’s take a quick step back and reflect on what this all means. Looking at the code snippet above, it’s easy to miss what’s at play here.*****🚧 This model is in active development. Our team is pushing hard to ensure command is safe, and helpful for our users. As you experiment with the model and run into issues, please help us by flagging it to our team by emailing us at team@cohere.com. Command is a generative model that responds well with instruction-like prompts, and is available in two sizes: medium and xlarge . The xlarge model demonstrates better performance, and medium is a great option for developers who require fast response, like those building chatbots. To reduce the turnaround time for releases, we have nightly versions of command available.*****Response: So, back to the Generate endpoint output, let’s say we get the following likelihood values for each token: So, the average likelihood of all generated tokens, in this case, is the average of the three tokens, which equals -1.3. Note that you need to enable the return_likelihoods parameter to return either GENERATION (output only) or ALL (output and prompt), otherwise, you will get None for the likelihood and token_likelihoods outputs. Response:*****## This Month’s Hot Topic: The Command Nightly Model Luis Serrano, our Head of Developer Relations, walked us through Cohere’s newest model — Command Nightly . To reduce the turnaround time for releases, we’re making nightly versions of our Command model available, which means that every week you can expect the performance of Command Nightly to improve. Command is a generative model that responds well with instruction-like prompts, and is available in two sizes. X-Large demonstrates better performance and Medium produces a faster response.*****## NEW Command Model (Beta) We’re also introducing a Beta of our new Command model, a generative model that’s conditioned to respond well to single-statement commands. Learn more about how to prompt command-xlarge-20221108 . You can expect to see command-xlarge-20221108 evolve dramatically in performance over the coming weeks.*****# Meet your AI-generated content writer Generate is powered by a large language model that has read billions of words, learning the patterns and idiosyncrasies of sentences. Using this knowledge, it writes content, predicts outcomes or answers questions at your command.*****## Selecting the Model We have only used one model so far: command-xlarge-nightly . But as we discussed in Part 1, you can prompt Cohere’s text generation model in two ways: by instruction and by example. So, the first thing that you want to define when calling the endpoint is the model type, depending on how you are constructing your prompt. Here are the available models at the time of writing: command-xlarge-nightly command-medium-nightly xlarge medium The sizes implied in the model names represent the parameter size of the models. So, which one do you choose? It depends on your use case, but as a rule of thumb, smaller models are faster, while larger models are generally more fluent and coherent.*****#### Generate Produce text completions based on a given input. Model Price per Unit Default $2.5 per 1000 Generation Units* Custom $5.0 per 1000 Generation Units* More details*****#### Embed Capture the semantic meaning of text by representing text as numbers. Model Price per Unit Default $1.0 per 1000 Embeddings Custom $2.0 per 1000 Embeddings More details*****#### Summarize Summarize a given input text such as an article into a short paragraph or key bullet points Model Price per Unit Default $5.0 per 1000 Summarization Units* Custom $10.0 per 1000 Summarization Units* More details API Key Offering API Key Offering*****#### Classify Analyze and organize text into specific categories. Model Price per Unit Default $2.0 per 1000 Classifications Custom $2.0 per 1000 Classifications More details*****## Totally transparent pricing We're making it easy to explore, learn, and experiment with the Cohere Platform. All models Free Limit of 100 calls/minute Default models $ 1.00 per 1000 Embeddings Custom models $ 2.00 per 1000 Embeddings Examples Stay updated*****#### Frequently Asked Questions 1. How do I get a Trial API Key? When an account is created, we automatically create an Trial API key for you. This API key will be available on the dashboard for you to copy, as well as in the dashboard section called “API Keys.” 2. How do I get a Production API key? To get a Production key, you'll need to have Owner privileges (or ask your organization Owner to complete the following steps). Navigate to the Billing and Usage page in your Cohere dashboard. Click on the button and fill out the Go to Production workflow.*****#### Summarize Summarize a given input text such as an article into a short paragraph or key bullet points Model Price per Unit Default $5.0 per 1000 Summarization Units* Custom $10.0 per 1000 Summarization Units* More details API Key Offering API Key Offering*****#### Generate Produce text completions based on a given input. Model Price per Unit Default $2.5 per 1000 Generation Units* Custom $5.0 per 1000 Generation Units* More details*****#### Embed Capture the semantic meaning of text by representing text as numbers. Model Price per Unit Default $1.0 per 1000 Embeddings Custom $2.0 per 1000 Embeddings More details*****#### Classify Analyze and organize text into specific categories. Model Price per Unit Default $2.0 per 1000 Classifications Custom $2.0 per 1000 Classifications More details*****Other Questions What is the difference between a Trial API key and Production API key? Are there any account limitations upon signup? What is the difference between an organization and a personal account? Which model should I pick? When do I get billed?*****Here’s a summary of the new pricing model per endpoint: We also want to make it easier for you to use our largest, best performing models by removing the pricing differences by model size — pricing of each endpoint will be standardized across sizes. This way, it’s a no brainer to get the best product Cohere has to offer! Custom Generate and Embed models will continue to be twice the price of their standard counterparts, whereas pricing for both Classify categories remain the same. Here’s a summary of the new costs: 3. For starters, we’re removing jargon in our product terminology.*****### 1. A prompt guides the model to generate useful output If you need a summary of an article, for example, a large language model trained on enough data can generate a summary if you guide it as such:*****## Main Principles We find that there are two main ideas to keep in mind while designing prompts for our models.*****One of the reasons why Large Language Models have taken the world by storm is their ability to manipulate or generate text for a wide variety of purposes without much instruction or training. Having been trained on large amounts of content already, the model can often produce the right outcomes based on a simple task description. In the world of LLMs and Generative AI , this task description is called a prompt. The prompt is one of the best ways you can influence the outcome of the LLM, and in this article, we’ll share some tips and tricks on how to get your prompts right.*****Additionally, you can also use the likelihood feature in the playground to see if there are particular words, phrases, or structures that the model has trouble understanding. However, keep in mind that the average likelihood of tokens will always be high at the beginning of the sequence. The model might assign low likelihood to the first time you introduce a novel concept or name, but once it has seen it once it can readily use it in the generation. You can also use the likelihood capability to see if there is any spelling or punctuation that is creating issues for tokenization.*****## Prompts 101 It’s quite expensive to build and train your own Large Language Models. Most people prefer to use a pre-trained model like Cohere, which you can access through our API. When calling the API, you need to pass in some parameters, like how random you want the output to be, how long you want it to be, and so on. One of those parameters is called the prompt and this is where you can describe to the model what you want it to do. Think of it like giving somebody an instruction, except in this case that somebody is a Language AI.*****Here, we discuss a few principles and techniques for writing prompts (inputs for our models) that will help you get the best generations for your task. Choosing the right temperature can also have a big influence on generation quality. We discuss temperature separately here .*****### The Generate Endpoint Large language models (LLM) have been pre-trained with a massive collection of text, which makes them capable of capturing the patterns of how humans use language. The outcome: just by giving them a simple prompt, these models can generate impressively original and coherent text. With the Cohere’s API, this capability is served by the Generate endpoint . Given an input (called a “prompt”), the endpoint will generate a new stream of text. The purpose of a prompt is to provide a context for the text that we want the model to generate. A prompt format for text generation that generally works well.*****## Conclusion The availability of Cohere natively on SageMaker via the AWS Marketplace represents a major milestone in the field of NLP. The Cohere model’s ability to generate high-quality, coherent text makes it a valuable tool for anyone working with text data. If you're interested in using Cohere for your own SageMaker projects, you can now access it on SageMaker JumpStart . Additionally, you can reference Cohere’s GitHub notebook for instructions on deploying the model and accessing it from the Cohere Generate endpoint .*****The company builds and continually improves its general-purpose large language models (LLMs), making them accessible via a simple-to-use platform. Companies can use the models out of the box or tailor them to their particular needs using their own custom data. Developers using SageMaker will have access to Cohere's Medium generation language model. The Medium generation model excels at tasks that require fast responses, such as question answering, copywriting, or paraphrasing. The Medium model is deployed in containers that enable low-latency inference on a diverse set of hardware accelerators available on AWS, providing different cost and performance advantages for SageMaker customers. “Amazon SageMaker provides the broadest and most comprehensive set of services that eliminate heavy lifting from each step of the machine learning process,” said Rajneesh Singh, General Manager AI/ML at Amazon Web Services.*****## Get started with Cohere in SageMaker Developers can use the visual interface of the Amazon SageMaker JumpStart foundation models to test Cohere's models without writing a single line of code. You can evaluate the model on your specific language understanding task and learn the basics of using generative language models. See Cohere’s documentation and blog for various tutorials and tips-and-tricks related to language modeling.*****The Cohere Medium generation language model available through SageMaker, provide developers with three key benefits:*****“We’re excited to offer Cohere’s general purpose large language model with Amazon SageMaker. Our joint customers can now leverage the broad range of Amazon SageMaker services and integrate Cohere’s model with their applications for accelerated time-to-value and faster innovation.” “As Cohere continues to push the boundaries of language AI, we are excited to join forces with Amazon SageMaker,” said Saurabh Baji, Senior Vice President of Engineering at Cohere. “This partnership will allow us to bring our advanced technology and innovative approach to an even wider audience, empowering developers and organizations around the world to harness the power of language AI and stay ahead of the curve in an increasingly competitive market."*****It’s an exciting day for the development community. Cohere’s state-of-the-art language AI is now available through Amazon SageMaker . This makes it easier for developers to deploy Cohere’s pre-trained generation language model to Amazon SageMaker, an end-to-end machine learning (ML) service. Developers, data scientists, and business analysts use Amazon SageMaker to build, train, and deploy ML models quickly and easily using its fully managed infrastructure, tools, and workflows. At Cohere, the focus is on language. The company’s mission is to enable developers and businesses to add language AI to their technology stack and build game-changing applications with it. Cohere helps developers and businesses automate a wide range of tasks, such as copywriting, named entity recognition, paraphrasing, text summarization, and classification.*****## Deploy the SageMaker endpoint using a notebook Cohere has packaged Medium models, along with an optimized, low-latency inference framework, in containers that can be deployed as SageMaker inference endpoints. Cohere’s containers can be deployed on a range of different instances (including ml.p3.2xlarge, ml.g5.xlarge, and ml.g5.2xlarge) that offer different cost/performance trade-offs. These containers are currently available in two Regions: us-east-1 and eu-west-1. Cohere intends to expand its offering in the near future, including adding to the number and size of models available, the set of supported tasks (such as the endpoints built on top of these models), the supported instances, and the available Regions.*****Our new and improved xlarge has better generation quality and a 4x faster prediction speed. This model now supports a maximum token length of 2048 tokens and frequency and presence penalties.*****directly in the Weaviate vector search engine as a vectorization module. The Pinecone vector database makes it easy to build high-performance vector search applications. Use Cohere to generate language embeddings, then store them in Pinecone and use them for Semantic Search. Cohere offers optimized containers that enable low latency inference on a diverse set of hardware accelerators available on AWS, providing different cost and performance points for Sagemaker customers. Integrate the Surge AI labeling platform into your Cohere workflow. Use Scale's labelled datasets with Cohere's Large Language Models.*****Visit Google Cloud Marketplace and click to get started using Cohere.*****In November of 2021, we announced a multi-year technology partnership with Google Cloud. As part of this collaboration, we made Cohere available via Google Cloud Marketplace . This gives developers and businesses who are building applications on Google Cloud easy access to natural language processing (NLP), powered by the latest generation of large language models. Cohere makes it easy for developers with any level of experience to build machine learning into their apps using Python, Node, and Go SDKs. Our versatile NLP platform offers three types of models that can generate, categorize, and organize text at massive scale. “Access to cutting-edge NLP technology has long held businesses back from one of the most exciting advancements in artificial intelligence,” said Cohere’s CEO and Cofounder Aidan Gomez.*****“At Cohere, it’s our mission to change that by developing a solution that’s simple – yet effective – and available to any developer.” Cohere’s addition to Google Cloud Marketplace serves as another solution for businesses seeking cloud software that enhances and streamlines their operations. “Helping developers build NLP models into applications in more natural, streamlined ways has become a key focus area for enterprises today,” said Dai Vu, Managing Director, Marketplace & ISV GTM Initiatives, Google Cloud. “We’re thrilled that Cohere’s solution is available on Google Cloud Marketplace to help customers drive deeper insights from their language-processing operations.” We can’t wait to see how Google Cloud developers will solve problems or build innovations using Cohere.*****Cohere offers an API to add cutting-edge language processing to any system. Cohere trains massive language models and puts them behind a simple API. Moreover, through training, users can create massive models customized to their use case and trained on their data. This way, Cohere handles the complexities of collecting massive amounts of text data, the ever evolving neural network architectures, distributed training, and serving models around the clock. Two major categories of large language models are generative language models (like GPT2 and GPT3) and representation language models (like BERT). Cohere offers variants of both types.*****Customers who are part of the AWS Enterprise Discount Program can also draw down on their commitment by purchasing Cohere’s solutions through the AWS Marketplace. To get you started, we’re offering $75 in free credits for you and your team to use Cohere’s Playground and API. That’s enough to generate as many words as Shakespeare’s complete works (heigh-ho!). Simply sign in to your AWS account and select the Cohere Access Plan.*****We may collect information about you when you: Before accessing or using the Services, you may be asked to complete the Cohere API Application. As part of this Application, we will collect your business contact information, information about your organization (including the type of organization and the jurisdiction in which it is located), and information about your intended use of the Services (including a description of your use case, your previous experiences using similar services, and estimates of the frequency with which you will use the Services). We use this information to understand the purposes for which you are seeking to use the Services, to assess whether the Services are right for your organization and the intended purposes, and to customize your user experience*****AWS developers — we have some exciting news! Cohere is now available through AWS Marketplace , making it even more accessible to developers and businesses of all kinds. Being a part of Amazon’s online store makes it easy for you to discover and explore Cohere’s language AI capabilities for your applications. The Cohere team is super excited about reaching a broad community of developers through this prominent channel. The Cohere API can be used in different libraries that fit every stack, and you can quickly build machine learning capabilities into your apps on AWS using our Python, Node, and Go SDKs.*****## Use Cases Here are a few examples of language understanding systems that can be built on top of large language models.*****### Classify Classification is one of the most common use cases in language processing. Building systems on top of language models can automate language-based tasks and save time and energy. There's more than one way to build a classifier on top of Cohere's language models. It's worth experimenting to see which method works best for your use case. The simpler methods can get you quick results, while the more advanced methods need more data and will lead to better results. On the simpler side are methods like using the Classify endpoint for classification . More industrial-grade classifiers can be built by fitting a classifier on top of the embed endpoint (see:*****## Use Cases for Embeddings What can you do with embeddings? Quite a lot. Our Embed endpoint takes a piece of text and creates a vector embedding, which represents the text as numbers that capture its meaning and context. Embedding transforms unstructured text data into a structured form that allows you to cluster, categorize, and semantically (contextually) search the text. Cohere’s powerful language models can find relationships between pieces of text (words, phrases, sentences, paragraphs, or documents) that would not surface with linear keyword search. Some examples of industries that could benefit from vector search include: Overall, any industry that relies on large amounts of information, and the ability to quickly and accurately search for that information, could benefit from vector search.*****The Cohere platform builds natural language understanding and generation into your product with a few lines of code. Our large language models can solve a broad spectrum of natural language use cases, including classification, semantic search, paraphrasing, summarization, and content generation. By training a custom model , users can customize large language models to their use case and trained on their data. The models can be accessed through the playground , SDK and the CLI tool.*****### Building a Chatbot with a Persona This is an example of a conversational use case. Try the preset here . If you are looking for more examples, visit this page to get more use case ideas.*****### Extracting Keywords from Emails This is an example of an extraction use case. Try the preset here .*****### Summarize Large language models present a breakthrough in text generation. For the first time in history, we have software programs that can write text that sounds like it’s written by humans. These capabilities open doors to use cases like summarization or paraphrasing. A summarization prompt in the Cohere playground shows this output (in bold): Large language models can be adapted to new tasks with impressive speed. For tasks which appear in the training data (i.e. documents on the web), language models can successfully summarize text without being shown any examples at all. Summarization and paraphrasing both use the generate endpoint.*****embeddings . Embeddings at their core allow computers to efficiently compare pieces of text and measure the degree of similarity between them. They do this by creating a long sequence of numbers that encode what words mean based on their context (e.g., bank of a river vs. money in the bank). These sequences are called “vectors,” and they allow the algorithm to see “nearest neighbors” to understand the similarity between words and their nuances. Vector search enables a contextual understanding that makes it far more accurate and effective than keyword- or metadata-based search. When a development team wants to explore the many uses of*****## Conclusion Pre-trained models save time, resources, and money compared to building and training your own model. And they are often as effective and more efficient than custom models. The best part is, that you can get started right away using our easy-to-use APIs, instead of waiting months to build and train your own model. Get started with Cohere for free, and test drive our pre-trained Large Language Models.*****Identifying a customer’s intent correctly and routing to the correct place is key to a positive customer support experience. When designing your customer support solution, there are three key components to consider: Salesforce Research Zendesk Hubspot Zendesk Conversational assistants have been widely deployed to solve this problem in 2022, experts estimate 80% of banking and healthcare queries will be answered by a conversational assistant ( CNBC , 2017). In addition, 87% of customers report neutral or positive experiences with conversational assistants ( Startup Bonsai , 2022) which indicates they have been successful in providing fast and scalable support. However, today’s conversational self-service assistants still do not have the best reputation for being able to understand a customer’s query, especially if their query is long, complex, or doesn’t contain certain keywords.*****### Summarization Generating summaries might sound like a trivial task but doing it at scale is not easy. Many financial companies and law firms find summaries of long reports to be quite useful. Without their own internal ML teams, it makes sense for them to use pre-trained models.*****## Usage Guidelines We require our users to abide by Cohere’s Usage Guidelines. Access will be revoked if these terms are not followed. If you spot the Cohere Platform being used in a harmful or otherwise unproductive way, please report it to us. Stay updated*****## The Power of Pre-Trained Pre-trained models can reduce the cost and effort required for deep learning because you do not have to spend time and money to gather and clean the data, not to mention the infrastructure and knowledge needed to train the models correctly.*****This similarity makes sense in the following ways: Why is the similarity between “bank” and “money” higher than the similarity between “bank” and “river”. We can imagine that “bank” gets used more often in the same context as “money”, than as “river”, and that explains the difference. We are simplifying this model quite a bit, it could be that the similarity between “the” and “of” is not zero, but 0.001. However, to simplify our calculations, we’ll use these numbers. Now, on to the next step. We are going to use the similarities to transform each of the words of this sentence.*****This gives a graphic like the one below. So in order to calculate the embeddings of Bank1 and Bank2, we simply do the math componentwise (that means, for each of the two components of the vector, separately). We get this: = [4.8, 4.8] + [0, 1] = [4.8, 5.8] = [4.2, 4.2] + [2.4, 0] = [6.6, 4.2] As you can see, “bank1” is closer to “river”, and “bank2” is closer to “money”. As a matter of fact, “bank1” is on the line between “bank” and “river”, 10% along the way. Similarly, “bank2” is on the line between “bank” and “money”, 20% along the way.*****This opens a huge untapped opportunity for semantic search for the enterprise. Semantic search can prove to be extremely beneficial across various industries, from manufacturing to finance. Any field that deals with internal documents can find significant value in better search capabilities. However, applying these technologies to get a performant semantic search engine can still be challenging, especially when documents become longer and more complex. So far, existing methods only work well on relatively short documents of around 200 words. Also, current methods work only on text, but a lot of documents like financial reports also contain images and graphs. Furthermore, information can also be stored in semi-structured formats like spreadsheets.*****## Totally transparent pricing We're making it easy to explore, learn, and experiment with the Cohere Platform. All models Free Limit of 100 calls/minute Default models $ 1.00 per 1000 Embeddings Custom models $ 2.00 per 1000 Embeddings Examples Stay updated*****#### Embed Capture the semantic meaning of text by representing text as numbers. Model Price per Unit Default $1.0 per 1000 Embeddings Custom $2.0 per 1000 Embeddings More details*****Free, rate limited Trial Keys for experimentation, testing, and playground usage Production keys with no rate limit for serving Cohere in production applications Flat rate pricing for Generate and Embed endpoints Reduced pricing for Classify endpoint New UI for dashboard including sign up and onboarding - everything except playground New use-case specific Quickstart Guides to learn about using Cohere API Replacing "Finetune" nomenclature with "Custom Model" Inviting team members is now more intuitive. Teams enable users to share custom models with each other Generative custom models now show accuracy and loss metrics alongside logs Embed and Classify custom models now show logs alongside accuracy, loss, precision, f1, recall*****The Embed endpoint takes a piece of text and turns it into a vector embedding. Embeddings represent text in the form of numbers that capture its meaning and context. This gives us the ability to turn unstructured text data into a structured form that can be processed and analyzed.*****### Why Cohere Free Developer Tier Learn and iterate with the Cohere API free of charge until you go to production. Easy To Use No prior ML/AI experience required. Get started with just a few examples. Customizable Models Customize our models with your own data sets, and deploy them easily to production.*****Here’s a summary of the new pricing model per endpoint: We also want to make it easier for you to use our largest, best performing models by removing the pricing differences by model size — pricing of each endpoint will be standardized across sizes. This way, it’s a no brainer to get the best product Cohere has to offer! Custom Generate and Embed models will continue to be twice the price of their standard counterparts, whereas pricing for both Classify categories remain the same. Here’s a summary of the new costs: 3. For starters, we’re removing jargon in our product terminology.*****# Uncover trends and patterns in text Turn text into numerical representations of language for deeper insights at scale. Embed makes it possible to algorithmically categorize and score text quickly to extract meaning.*****Embeddings are a way to represent the meaning of text as a list of numbers. This is useful because once text is in this form, it can be compared to other text for similarity. Using a simple comparison function, we can calculate a similarity score for two embeddings to figure out whether two texts are talking about similar things. In the example below, the embeddings for two phrases have a , and the embeddings for two phrases have a :*****## Benchmarks We extensively benchmarked our new model to ensure the best performance across a wide range of applications, domains and languages. Specifically, we used: MTEB BEIR BEIR Mr. Tydi MIRACL Amazon MASSIVE dataset We compared our results against other state-of-the-art multilingual embedding models, specifically paraphrase-multilingual-mpnet-base-v2 (the best model from Sentence-Transformers ), LaBSE (from Google), and Universal Sentence Encoder cMLM (from Google). The following chart shows how they compare: The Cohere multilingual-22-12 model performs much better in all use cases. In particular, we see a robust improvement in multilingual search. The other models we tested against perform rather poorly, in many cases less effectively than keyword search.*****## Differences Between English and Multilingual Embedding Models Unlike our English language embedding model, our multilingual model was trained using dot product calculations. Using dot products produces a non-normalized similarity score, reflecting the magnitude of the two compared vectors. When this dimension is incorporated, multilingual embeddings perform better than standard. For more information on how our English language model works (using cosine similarity), see our introductory guide to the Cohere platform . The dimensions of our multilingual embeddings is 768 dimensions.*****However, these models don’t capture the nuances behind language usage in different countries. Contrary to this approach, we collected a dataset of nearly 1.4 billion question/answer pairs across tens of thousands of websites in hundreds of languages. These are questions actually asked by speakers of said languages, allowing us to capture language- and country-specific nuances. “I strongly believe that embeddings are the future of search and recommendation. Thanks to the new Cohere multilingual model and the text2vec Cohere module in Weaviate, we can bring this to developers worldwide with a single command.” - Bob van Luijt, CEO at SeMI Technologies multilingual-22-12*****At Cohere, we are committed to breaking down barriers and expanding access to cutting-edge NLP technologies that power projects across the globe. By making our innovative multilingual language models available to all developers, we continue to move toward our goal of empowering developers, researchers, and innovators with state-of-the-art NLP technologies that push the boundaries of Language AI. Our Multilingual Model maps text to a semantic vector space, positioning text with a similar meaning in close proximity. This process unlocks a range of valuable use cases for multilingual settings. For example, one can map a query to this vector space during a search to locate relevant documents nearby.*****In recent tests, we conducted against three common open-source alternatives, the Cohere Multilingual Text Understanding Model is by far the best available multilingual embedding model, outperforming alternatives by significant margins.*****## Training Data Training embedding models require data in a specific format. For example, question/answer pairs or title/document pairs. We can then learn which text pairs should be closer together in the vector space in order to enable applications like semantic search. To train Cohere’s new multilingual model, we processed and carefully cleaned terabytes of data from various sources: Wikipedia, news publications, scientific articles, and online communities across hundreds of languages. This resulted in a large training corpus of more than 900 million training pairs for English and 450 million training pairs for other languages. Other multilingual embedding models often rely on machine translation for training dataset creation, which creates an awkward bias for these models.*****Cohere's multilingual text understanding model is now available! The multilingual-22-12 model can be used to semantically search within a single language, as well as across languages. Compared to keyword search, where you often need separate tokenizers and indices to handle different languages, the deployment of the multilingual model for search is trivial: no language-specific handling is needed — everything can be done by a single model within a single index. In addition to our new model, you can now detect the language of a data source using co.detect_language() endpoint. For more information, see our multilingual docs .*****The multilingual-22-12 model can be used to semantically search within a single language, as well as across languages. Compared to keyword search, where you often need separate tokenizers and indices to handle different languages, the deployment of the multilingual model for search is trivial: no language-specific handling is needed — everything can be done by a single model within a single index. We’re extremely proud of the performance of our multilingual understanding model. It outperformed the industry standard (the next best model) in search tasks by more than 230%.*****The co.classify endpoint now supports the use of Cohere's multilingual embedding model. The multilingual-22-12 model is now a valid model input in the co.classify call.*****This endpoint generates a succinct version of the original text that relays the most important information. Ideal use cases include, but are not limited to: news articles, blogs, chat transcripts, scientific articles, meeting notes, and any text that you should like to see a summary of! The endpoint can: Summarize a single document Control output length 🚧 Experimental Features These features are extremely experimental. Using these feature could lead to a substantial decrease in performance over the overall model. It is included as a feature based on user feedback — and our team is actively working on delivering a better solution.*****## 2. Summarize The second use case category, which also leverages prompt engineering, is text summarization. Think about the amount of text that we deal with on a typical day, such as reports, articles, meeting notes, emails, transcripts, and so on. We can have an LLM summarize a piece of text by prompting it with a few examples of a full document and its summary. The following is an example of article summarization, where we prepare the prompt to contain the full passage of an article and its one-line summary. You can test it out by accessing the saved preset .*****Because it is critical for some applications, we have exposed an experimental version. If you do try it out, we welcome your feedback. Ability to format chosen output Long document summaries Ability to provide additional instructions to focus the summary We recommend to leverage the playground for quick use cases, but for any repeated utilizations we strongly recommend the API. An example is provided below. In this example, we want to summarize a passage from a news article into its main point. Install the SDK, if you haven't already.*****### Summarize Large language models present a breakthrough in text generation. For the first time in history, we have software programs that can write text that sounds like it’s written by humans. These capabilities open doors to use cases like summarization or paraphrasing. A summarization prompt in the Cohere playground shows this output (in bold): Large language models can be adapted to new tasks with impressive speed. For tasks which appear in the training data (i.e. documents on the web), language models can successfully summarize text without being shown any examples at all. Summarization and paraphrasing both use the generate endpoint.*****### Summarizing Chat Transcripts This is an example of a summarization use case. Try the preset here .*****In this example, we want to summarize a passage from a news article into its main point. Install the SDK, if you haven't already.*****### Classify Classification is one of the most common use cases in language processing. Building systems on top of language models can automate language-based tasks and save time and energy. There's more than one way to build a classifier on top of Cohere's language models. It's worth experimenting to see which method works best for your use case. The simpler methods can get you quick results, while the more advanced methods need more data and will lead to better results. On the simpler side are methods like using the Classify endpoint for classification . More industrial-grade classifiers can be built by fitting a classifier on top of the embed endpoint (see:*****## Use Cases for Embeddings What can you do with embeddings? Quite a lot. Our Embed endpoint takes a piece of text and creates a vector embedding, which represents the text as numbers that capture its meaning and context. Embedding transforms unstructured text data into a structured form that allows you to cluster, categorize, and semantically (contextually) search the text. Cohere’s powerful language models can find relationships between pieces of text (words, phrases, sentences, paragraphs, or documents) that would not surface with linear keyword search. Some examples of industries that could benefit from vector search include: Overall, any industry that relies on large amounts of information, and the ability to quickly and accurately search for that information, could benefit from vector search.*****### Use Case #1: Building New Links The Generate endpoint suggesting potential new topics. The Generate endpoint can be applied in many different use cases, and one of them is in extracting information from a piece of text. The key is in the prompt, where we need to show a few examples of a piece of text and the kind of information to extract from the text. We’ll build an extraction step that takes each note — let’s call this the “parent” note — and suggests parts of that note that can be expanded into its own note. These new “child” notes would then become links and be linked back to the parent note.*****The company builds and continually improves its general-purpose large language models (LLMs), making them accessible via a simple-to-use platform. Companies can use the models out of the box or tailor them to their particular needs using their own custom data. Developers using SageMaker will have access to Cohere's Medium generation language model. The Medium generation model excels at tasks that require fast responses, such as question answering, copywriting, or paraphrasing. The Medium model is deployed in containers that enable low-latency inference on a diverse set of hardware accelerators available on AWS, providing different cost and performance advantages for SageMaker customers. “Amazon SageMaker provides the broadest and most comprehensive set of services that eliminate heavy lifting from each step of the machine learning process,” said Rajneesh Singh, General Manager AI/ML at Amazon Web Services.*****## Conclusion The availability of Cohere natively on SageMaker via the AWS Marketplace represents a major milestone in the field of NLP. The Cohere model’s ability to generate high-quality, coherent text makes it a valuable tool for anyone working with text data. If you're interested in using Cohere for your own SageMaker projects, you can now access it on SageMaker JumpStart . Additionally, you can reference Cohere’s GitHub notebook for instructions on deploying the model and accessing it from the Cohere Generate endpoint .*****directly in the Weaviate vector search engine as a vectorization module. The Pinecone vector database makes it easy to build high-performance vector search applications. Use Cohere to generate language embeddings, then store them in Pinecone and use them for Semantic Search. Cohere offers optimized containers that enable low latency inference on a diverse set of hardware accelerators available on AWS, providing different cost and performance points for Sagemaker customers. Integrate the Surge AI labeling platform into your Cohere workflow. Use Scale's labelled datasets with Cohere's Large Language Models.*****## Get started with Cohere in SageMaker Developers can use the visual interface of the Amazon SageMaker JumpStart foundation models to test Cohere's models without writing a single line of code. You can evaluate the model on your specific language understanding task and learn the basics of using generative language models. See Cohere’s documentation and blog for various tutorials and tips-and-tricks related to language modeling.*****The Cohere Medium generation language model available through SageMaker, provide developers with three key benefits:*****“We’re excited to offer Cohere’s general purpose large language model with Amazon SageMaker. Our joint customers can now leverage the broad range of Amazon SageMaker services and integrate Cohere’s model with their applications for accelerated time-to-value and faster innovation.” “As Cohere continues to push the boundaries of language AI, we are excited to join forces with Amazon SageMaker,” said Saurabh Baji, Senior Vice President of Engineering at Cohere. “This partnership will allow us to bring our advanced technology and innovative approach to an even wider audience, empowering developers and organizations around the world to harness the power of language AI and stay ahead of the curve in an increasingly competitive market."*****It’s an exciting day for the development community. Cohere’s state-of-the-art language AI is now available through Amazon SageMaker . This makes it easier for developers to deploy Cohere’s pre-trained generation language model to Amazon SageMaker, an end-to-end machine learning (ML) service. Developers, data scientists, and business analysts use Amazon SageMaker to build, train, and deploy ML models quickly and easily using its fully managed infrastructure, tools, and workflows. At Cohere, the focus is on language. The company’s mission is to enable developers and businesses to add language AI to their technology stack and build game-changing applications with it. Cohere helps developers and businesses automate a wide range of tasks, such as copywriting, named entity recognition, paraphrasing, text summarization, and classification.*****### Extracting Keywords from Emails This is an example of an extraction use case. Try the preset here .*****## 4. Extract Text extraction is another use case category that can leverage a generation LLM. The idea is to take a long piece of text and extract only the key information or words from the text. The following is the task of extracting relevant information from contracts. We prepare the prompt with a short bit of context about the task, followed by a couple of example contracts and the extracted text. You can test it out by accessing the saved preset . Some other use cases in this category include:*****# Conclusion Manually extracting data from invoices brings about many challenges. Some of them include inaccuracies, fatigue, slowness, and increased labor costs. With NLP, this can be automated and done quickly with little or no human intervention. This has been a demonstration of how to use the Cohere Platform to perform text extraction quickly and efficiently in an invoice use case. You saw that using Cohere, the complex capabilities of natural language processing can be quickly and easily incorporated into your Node.JS applications—all done using a form you created and linked with the backend. Learn more about Cohere’s Large Language Models*****### Use Case #1: Building New Links The Generate endpoint suggesting potential new topics. The Generate endpoint can be applied in many different use cases, and one of them is in extracting information from a piece of text. The key is in the prompt, where we need to show a few examples of a piece of text and the kind of information to extract from the text. We’ll build an extraction step that takes each note — let’s call this the “parent” note — and suggests parts of that note that can be expanded into its own note. These new “child” notes would then become links and be linked back to the parent note.*****## Use Case 2: Customer Feedback Aggregation When successful products like the iPhone launch, tens of thousands of users around the world post their feedback (in their own language) on eCommerce sites, social media, blogs, and elsewhere. Extracting insights from these reviews enables companies to quickly respond to the market, better understand their customer base, and improve their product roadmap. However, previous methods for content aggregation have only worked well for English, and they didn’t allow users to see patterns across languages or to compare feedback from different markets. Cohere’s multilingual model maps text in different languages to the same vector spaces, allowing users to derive insights across languages and find patterns for specific markets (e.g., which markets care about the picture quality of smartphones).*****## Creating the extraction prompt We'll create a prompt that demonstrates the task to the model. The prompt contains the examples above, and then presents the input text and asks the model to extract the movie name. So let's get a few example titles from the movies subreddit, label them, and make an extraction prompt out of them: Let's point out a few ideas in this prompt: The prompt is made up of six examples that demonstrate the task to the model before it encounters the input text we want to extract text from Each example demonstrates the task by showing an example input text and an example output text.*****## Benchmarks We extensively benchmarked our new model to ensure the best performance across a wide range of applications, domains and languages. Specifically, we used: MTEB BEIR BEIR Mr. Tydi MIRACL Amazon MASSIVE dataset We compared our results against other state-of-the-art multilingual embedding models, specifically paraphrase-multilingual-mpnet-base-v2 (the best model from Sentence-Transformers ), LaBSE (from Google), and Universal Sentence Encoder cMLM (from Google). The following chart shows how they compare: The Cohere multilingual-22-12 model performs much better in all use cases. In particular, we see a robust improvement in multilingual search. The other models we tested against perform rather poorly, in many cases less effectively than keyword search.*****## Differences Between English and Multilingual Embedding Models Unlike our English language embedding model, our multilingual model was trained using dot product calculations. Using dot products produces a non-normalized similarity score, reflecting the magnitude of the two compared vectors. When this dimension is incorporated, multilingual embeddings perform better than standard. For more information on how our English language model works (using cosine similarity), see our introductory guide to the Cohere platform . The dimensions of our multilingual embeddings is 768 dimensions.*****In recent tests, we conducted against three common open-source alternatives, the Cohere Multilingual Text Understanding Model is by far the best available multilingual embedding model, outperforming alternatives by significant margins.*****At Cohere, we are committed to breaking down barriers and expanding access to cutting-edge NLP technologies that power projects across the globe. By making our innovative multilingual language models available to all developers, we continue to move toward our goal of empowering developers, researchers, and innovators with state-of-the-art NLP technologies that push the boundaries of Language AI. Our Multilingual Model maps text to a semantic vector space, positioning text with a similar meaning in close proximity. This process unlocks a range of valuable use cases for multilingual settings. For example, one can map a query to this vector space during a search to locate relevant documents nearby.*****The multilingual-22-12 model can be used to semantically search within a single language, as well as across languages. Compared to keyword search, where you often need separate tokenizers and indices to handle different languages, the deployment of the multilingual model for search is trivial: no language-specific handling is needed — everything can be done by a single model within a single index. We’re extremely proud of the performance of our multilingual understanding model. It outperformed the industry standard (the next best model) in search tasks by more than 230%.*****However, these models don’t capture the nuances behind language usage in different countries. Contrary to this approach, we collected a dataset of nearly 1.4 billion question/answer pairs across tens of thousands of websites in hundreds of languages. These are questions actually asked by speakers of said languages, allowing us to capture language- and country-specific nuances. “I strongly believe that embeddings are the future of search and recommendation. Thanks to the new Cohere multilingual model and the text2vec Cohere module in Weaviate, we can bring this to developers worldwide with a single command.” - Bob van Luijt, CEO at SeMI Technologies multilingual-22-12*****## Training Data Training embedding models require data in a specific format. For example, question/answer pairs or title/document pairs. We can then learn which text pairs should be closer together in the vector space in order to enable applications like semantic search. To train Cohere’s new multilingual model, we processed and carefully cleaned terabytes of data from various sources: Wikipedia, news publications, scientific articles, and online communities across hundreds of languages. This resulted in a large training corpus of more than 900 million training pairs for English and 450 million training pairs for other languages. Other multilingual embedding models often rely on machine translation for training dataset creation, which creates an awkward bias for these models.*****#### Access Security Access to cloud infrastructure and other sensitive tools are limited to authorized employees who require it for their role. Where available we have Single Sign-on (SSO), 2-factor authentication (2FA) and strong password policies to ensure access to cloud services are protected. We follow the principle of least privilege with respect to identity and access management. We perform quarterly access reviews of all team members with access to sensitive systems. All team members are required to adhere to a minimum set of password requirements and complexity for access. All company issued laptops utilize a password manager for team members to manage passwords and maintain password complexity.*****: Creating or promoting harmful false claims about government policies, or public figures, including applications founded on unscientific premises. : Spearphishing. : Model attacks to extract personal information. : Posting content to social platforms in an automated way. : Applications that do not disclose that the content is generated through automated means. : AI-based social scoring for general purposes done by public authorities: Using output toward larger decision-making systems that will influence actions, decisions, or policies without a human-in-the-loop. : Applications that classify and/or profile people based on protected characteristics, or infer those characteristics from text written about them or by them.*****We encourage you to read the privacy policy of every website you visit. We have implemented reasonable administrative, technical and physical measures in an effort to safeguard the personal information in our custody and control against theft, loss and unauthorized access, use, modification and disclosure. We restrict access to personal information on a need-to-know basis to employees and authorized service providers who require access to fulfil their job requirements. If we receive a request from an individual to access or update personal information we maintain on behalf of a customer, we will direct that individual to the relevant customer. We will assist our customers wherever possible in responding to individual access requests.*****## Advice and Recommendations for Using LLMs: Dr. Rachael Tatman started her talk by strongly advising against serving raw-generated text to users for both UX and security reasons because of its unpredictability. She also mentions that most of the adversarial attacks on the LLMs require access to raw text output. So, if we don’t release the raw data, we won’t have to deal with adversarial attacks. She recommends human-in-the-loop data augmentation for a warmer start when training or fine-tuning a chatbot.*****All of our data is hosted on databases. These databases are all located in the United States. Please reference the above vendor specific documentation linked above for more information. All databases are encrypted at rest. Our applications encrypt in transit with TLS/SSL only. We perform vulnerability scanning and actively monitor for threats. We actively monitor and log various cloud services. We use our data hosting provider’s backup services to reduce any risk of data loss in the event of a hardware failure. We utilize monitoring services to alert the team in the event of any failures affecting users. We have a process for handling information security events which includes escalation procedures, rapid mitigation and communication.*****We also use third parties tools that use cookies and other technologies to collect information about your device and your behaviour on our Website in order to show us how you interact with our Website. The information collected includes information on what you see during your visit and actions you take when navigating our Website (e.g. clicks, mouse movements, hovers, page visits, scrolling, typing, form fills) as well as your device’s IP address, device screen size, device type, browser information, session duration, city and country, and preferred language. We use this information to optimize the user experience on our website, measure your engagement and help us identify errors on our Website.*****for more about toxic language model degeneration). We have put safeguards in place to avoid generating harmful text, but we highly recommend that developers build additional guardrails to ensure that text presented to end users is not toxic or harmful. Language models capture problematic associations and stereotypes prominent on the internet and society at large. They should not be used to make decisions about individuals or the groups they belong to. For example, it is dangerous to use Generation model outputs in CV ranking systems due to known biases ( Nadeem et al., 2020 ).*****#### Embed Capture the semantic meaning of text by representing text as numbers. Model Price per Unit Default $1.0 per 1000 Embeddings Custom $2.0 per 1000 Embeddings More details*****## Totally transparent pricing We're making it easy to explore, learn, and experiment with the Cohere Platform. All models Free Limit of 100 calls/minute Default models $ 1.00 per 1000 Embeddings Custom models $ 2.00 per 1000 Embeddings Examples Stay updated*****The Embed endpoint takes a piece of text and turns it into a vector embedding. Embeddings represent text in the form of numbers that capture its meaning and context. This gives us the ability to turn unstructured text data into a structured form that can be processed and analyzed.*****📘 This Guide Uses the Endpoint. You can find more information about the endpoint here . This notebook shows how to build a classifiers using Cohere's embeddings. You can find the code in the notebook and colab . The example classification task here will be sentiment analysis of film reviews. We'll train a simple classifier to detect whether a film review is negative (class 0) or positive (class 1). We'll go through the following steps: Install Cohere Get the dataset Get the embeddings of the reviews (for both the training set and the test set). Train a classifier using the training set*****### Technical Notes The model provides meaningful representations for English text only. Embeddings capture the state of the training data at the time it was scraped. Downstream classifiers will need to be validated or retrained upon release of new embedding models to ensure that they are still serving their intended purpose. embed outputs are the aggregation of contextualized word embeddings; hence, the embeddings of longer inputs may not capture the meaning accurately across the entire sequence length.*****Here’s a summary of the new pricing model per endpoint: We also want to make it easier for you to use our largest, best performing models by removing the pricing differences by model size — pricing of each endpoint will be standardized across sizes. This way, it’s a no brainer to get the best product Cohere has to offer! Custom Generate and Embed models will continue to be twice the price of their standard counterparts, whereas pricing for both Classify categories remain the same. Here’s a summary of the new costs: 3. For starters, we’re removing jargon in our product terminology.*****Our team members are required to review and accept all of the security policies. Our team members are required to go through employee security awareness training covering industry standard practices and information security topics such as phishing and password management. All team members are required to sign and adhere to an industry standard confidentiality agreement prior to their first day of work. We perform background checks on all new team members in accordance with local laws. All of our services are hosted with . They employ a robust security program with multiple certifications. For more information on our provider’s security processes, please visit*****#### Access Security Access to cloud infrastructure and other sensitive tools are limited to authorized employees who require it for their role. Where available we have Single Sign-on (SSO), 2-factor authentication (2FA) and strong password policies to ensure access to cloud services are protected. We follow the principle of least privilege with respect to identity and access management. We perform quarterly access reviews of all team members with access to sensitive systems. All team members are required to adhere to a minimum set of password requirements and complexity for access. All company issued laptops utilize a password manager for team members to manage passwords and maintain password complexity.*****If your Application is approved, you will be required to create an account in order to access and use the Services. To create your account, we collect your first and last name, email address and a password that you create. We use this information to create and administer your account and facilitate your access to and use of the Services. We strongly recommend that you do not disclose your password to anyone. We will never ask you for your password in any unsolicited communication (such as letters, phone calls or email messages). If you become aware of any unauthorized access to or use of your account, you are required to notify us immediately.*****We may retain this information to assist you in the future and to improve our customer service and service offerings. We do not sell or disclose your personal information to third parties without your consent, except as set forth below or as required or permitted by law. Your personal information will be transferred (or otherwise made available) to certain third parties that provide services on our behalf. We use service providers to provide services such as cloud computing, data storage, publishing, and analytics Our service providers are only provided with the information they need to perform their designated functions and are not authorized to use or disclose personal information for their own marketing or other purposes.*****We encourage you to read the privacy policy of every website you visit. We have implemented reasonable administrative, technical and physical measures in an effort to safeguard the personal information in our custody and control against theft, loss and unauthorized access, use, modification and disclosure. We restrict access to personal information on a need-to-know basis to employees and authorized service providers who require access to fulfil their job requirements. If we receive a request from an individual to access or update personal information we maintain on behalf of a customer, we will direct that individual to the relevant customer. We will assist our customers wherever possible in responding to individual access requests.*****You can also obtain additional information on Google Analytics’ data privacy and security at the following links: and : Our Website may contain links to other websites that Cohere does not own or operate. We provide links to third party websites as a convenience to the user. These links are not intended as an endorsement of or referral to the linked websites. The linked websites have separate and independent privacy policies, notices and terms of use. We do not have any control over such websites, and therefore we have no responsibility or liability for the manner in which the organizations that operate such linked websites may collect, use or disclose, secure and otherwise treat personal information.*****## Differences Between English and Multilingual Embedding Models Unlike our English language embedding model, our multilingual model was trained using dot product calculations. Using dot products produces a non-normalized similarity score, reflecting the magnitude of the two compared vectors. When this dimension is incorporated, multilingual embeddings perform better than standard. For more information on how our English language model works (using cosine similarity), see our introductory guide to the Cohere platform . The dimensions of our multilingual embeddings is 768 dimensions.*****### Getting meaningful text representations with  Embed() The next step was to embed these titles so we can examine the dataset based on the meanings of the titles and not just the tokens they contain. Cohere’s embed endpoint gives us vector representations from a large embedding language model specifically tuned for text embedding (as opposed to word embedding or text generation). This gives us a matrix where each post title has a 1024 dimensional vector numerically containing its meaning.*****The result is a custom model that produces outputs that are more attuned to the task you have at hand. As an example, let’s go back to our dataset. The first step we need to do is to prepare a dataset for finetuning. Finetuning requires a minimum of 250 data points, which we’ll take from the rest of the original dataset. With Cohere, this step is a simple one where you upload the dataset on the Playground and start the finetuning process from there. Once finetuning is complete, we’ll re-generate the embeddings, now using the finetuned model. The resulting embeddings, compressed to 2 dimensions and plotted on a chart, are as below:*****It uses Cohere to embed summaries (or abstracts) of each paper into an n-dimensional vector. See their GitHub repo . Baith al suroor (House of Happiness) by Arsalan Mohammad — Arslan presented a second demo, this time an interior design tool that helps people transform their space. The tool uses the Cohere API in combination with Stable Diffusion to create relevant images that bring a user’s interior design vision to life. Arsalan is looking for collaborators on both of his projects, so hit him up if you are interested!*****# How It Works Embeddings are numerical representations of meaning in text. Because they are numbers, they can be compared to each other for similarity. They can also be plotted on a chart that shows which texts are similar to each other. Large language models produce highly nuanced embeddings.*****### Embed Using Embed in the Playground enables users to assign numerical representations to strings and visualize comparative meaning on a 2-dimensional plane. Phrases similar in meaning should ideally be closer together on this visualization. Add a couple of your own phrases and see if the Playground visualization feels accurate to you. Cohere’s embeddings can be used to train a semantic classifier out of the box, saving users countless hours gathering data to train a model themselves.*****The Cohere embedding does just this. Using transformers, attention mechanisms, and other cutting edge algorithms, this embedding sends every sentence to a vector formed by 4096 numbers, and this embedding works really well. As a small example, here is a heatmap of the first 10 entries of some sentences (writing the entire 4096 entries will take too much space, so we truncated it). Notice that these sentences are all very similar. In particular, the three highlighted sentences pretty much have the same meaning. If you look at their corresponding vectors, these are also really similar. That is exactly what an embedding should do.*****### Getting meaningful text representations with  Embed() The next step was to embed these titles so we can examine the dataset based on the meanings of the titles and not just the tokens they contain. Cohere’s embed endpoint gives us vector representations from a large embedding language model specifically tuned for text embedding (as opposed to word embedding or text generation). This gives us a matrix where each post title has a 1024 dimensional vector numerically containing its meaning.*****## Differences Between English and Multilingual Embedding Models Unlike our English language embedding model, our multilingual model was trained using dot product calculations. Using dot products produces a non-normalized similarity score, reflecting the magnitude of the two compared vectors. When this dimension is incorporated, multilingual embeddings perform better than standard. For more information on how our English language model works (using cosine similarity), see our introductory guide to the Cohere platform . The dimensions of our multilingual embeddings is 768 dimensions.*****It uses Cohere to embed summaries (or abstracts) of each paper into an n-dimensional vector. See their GitHub repo . Baith al suroor (House of Happiness) by Arsalan Mohammad — Arslan presented a second demo, this time an interior design tool that helps people transform their space. The tool uses the Cohere API in combination with Stable Diffusion to create relevant images that bring a user’s interior design vision to life. Arsalan is looking for collaborators on both of his projects, so hit him up if you are interested!*****The result is a custom model that produces outputs that are more attuned to the task you have at hand. As an example, let’s go back to our dataset. The first step we need to do is to prepare a dataset for finetuning. Finetuning requires a minimum of 250 data points, which we’ll take from the rest of the original dataset. With Cohere, this step is a simple one where you upload the dataset on the Playground and start the finetuning process from there. Once finetuning is complete, we’ll re-generate the embeddings, now using the finetuned model. The resulting embeddings, compressed to 2 dimensions and plotted on a chart, are as below:*****The Cohere embedding does just this. Using transformers, attention mechanisms, and other cutting edge algorithms, this embedding sends every sentence to a vector formed by 4096 numbers, and this embedding works really well. As a small example, here is a heatmap of the first 10 entries of some sentences (writing the entire 4096 entries will take too much space, so we truncated it). Notice that these sentences are all very similar. In particular, the three highlighted sentences pretty much have the same meaning. If you look at their corresponding vectors, these are also really similar. That is exactly what an embedding should do.*****# Uncover trends and patterns in text Turn text into numerical representations of language for deeper insights at scale. Embed makes it possible to algorithmically categorize and score text quickly to extract meaning.*****# How It Works Embeddings are numerical representations of meaning in text. Because they are numbers, they can be compared to each other for similarity. They can also be plotted on a chart that shows which texts are similar to each other. Large language models produce highly nuanced embeddings.*****### Embed Using Embed in the Playground enables users to assign numerical representations to strings and visualize comparative meaning on a 2-dimensional plane. Phrases similar in meaning should ideally be closer together on this visualization. Add a couple of your own phrases and see if the Playground visualization feels accurate to you. Cohere’s embeddings can be used to train a semantic classifier out of the box, saving users countless hours gathering data to train a model themselves.*****## Totally transparent pricing We're making it easy to explore, learn, and experiment with the Cohere Platform. All models Free Limit of 100 calls/minute Default models $ 1.00 per 1000 Embeddings Custom models $ 2.00 per 1000 Embeddings Examples Stay updated*****#### Embed Capture the semantic meaning of text by representing text as numbers. Model Price per Unit Default $1.0 per 1000 Embeddings Custom $2.0 per 1000 Embeddings More details*****The Embed endpoint takes a piece of text and turns it into a vector embedding. Embeddings represent text in the form of numbers that capture its meaning and context. This gives us the ability to turn unstructured text data into a structured form that can be processed and analyzed.*****## Cohere project Cohere offers three categories of NLP models — Generate, Embed, and Classify — which allow you to obtain AI-generated text, create graphical representations of words and sentences, and segment pieces of text into defined buckets or classes. You can access all these NLP applications through a well-documented, intuitive user interface. It even offers a “Playground” feature, with examples of each use case. First, create a Cohere account and generate an API key. Cohere offers a $75 credit upon signup for use on API calls to baseline and finetuned models. API keys are found on the Cohere dashboard. With Cohere, you get an API with /generate, /embed, /classify and /tokenize endpoints, as well as Python, Node, and Go client SDKs.*****### Extract insights with an API call No matter your level of experience with ML/AI, the Cohere Platform makes it easy to integrate language comprehension into your application. We support all common languages through native SDK support or encapsulated REST calls, Our models have been trained on billions of words, allowing them to understand nuance and context.*****To upgrade your key now, simply complete the “Go to Production” workflow under the Usage tab in your dashboard (it takes less than 3 minutes). We hope that the new free tier will allow you to prototype using Cohere without worrying about having to upgrade in order to complete your experiment or proof of concept. 2. Thanks to your feedback and our relentless focus on improving efficiencies, we’ve simplified our pricing structure to make it easier for you to estimate the costs of using our platform. We’re also reducing the price of our embeddings (decreased by more than 10x depending on the length of your inputs) to enable and encourage a broader range of applications and tasks to be built with Cohere.*****### 1.2 Turn articles into embeddings The first thing we need to do is to turn each article's text into embeddings. An embedding is a list of numbers that our models use to represent a piece of text, capturing its context and meaning. We do this by calling Cohere’s Embed endpoint , which takes in texts as input and returns embeddings as output.*****Here’s a summary of the new pricing model per endpoint: We also want to make it easier for you to use our largest, best performing models by removing the pricing differences by model size — pricing of each endpoint will be standardized across sizes. This way, it’s a no brainer to get the best product Cohere has to offer! Custom Generate and Embed models will continue to be twice the price of their standard counterparts, whereas pricing for both Classify categories remain the same. Here’s a summary of the new costs: 3. For starters, we’re removing jargon in our product terminology.*****Good investment ideas can be found just about anywhere. This presents an opportunity, but also a challenge, since the sheer volume of public data available can be overwhelming. Every minute of every day, new data sources are published about any given company or industry, all of which potentially contain relevant information for investors. Some examples include: Asset managers and analysts struggle to ingest and parse all this data efficiently using manual processes. The result is not only information overload, but the thing they fear the most: missing out.*****In the #grounded-qa-bot channel in the Cohere co:mmunity on Discord , there’s a bot that will answer your questions. Ask any question in the chat, add the question mark emoji, and the co_search bot will attempt to answer the question. This answer, however, is not simply a GPT model’s output to an input prompt. It is informed by a web search. There have been recent large language models (LLMs) from research labs that are trained on using a database or have the ability to search the web for information. Unlike those custom models, this is a bot that you, a developer who does not necessarily work at a giant tech company, can build today.*****## Generating the Knowledge Base The app generates a knowledge base by extracting the product description from the company’s website and creating a vector embedding . It will use this embedding later for comparing the semantic similarity between the knowledge base and the customer’s message. For this project, the developers used information from Two Wests & Elliott , an eCommerce store selling equipment for greenhouses and gardens, to generate the knowledge base. Below is the webpage for the “Halls Standard Cold Frame” item, which lists product information, including the price, dimensions, materials, and delivery options. The app includes a prompt instructing the Generate endpoint to create Q&As using the product’s description.*****## 2- Make a distinction between impressive 🍒 cherry-picked demos, and reliable use cases that are ready for the marketplace Large text generation models are able to answer many questions correctly. But can they do it reliably ? Stack Overflow doesn’t think so. The popular forum where software developers ask questions has banned machine generated answers from being posted on the site “because the average rate of getting correct answers from ChatGPT is too low”. This is an example of a use case where some people expected the model to reliably be able to generate the exact correct generation for a complex set of problems.*****The step-by-step process behind this NLP-powered app is relatively simple. First, the app extracts product information from the company website and stores it in a knowledge base as text. It then uses the Cohere Embed and Cohere Generate endpoints to generate questions and answers (Q&As) from that text. Based on input from the customer support chatbot, the tool creates three appropriate responses to the customer’s message. The support agent can either reply using one of the responses as-is, edit one before sending it as a response, or answer the query manually. That is the app’s process in a nutshell. Next, let’s explore each step in more detail, starting with the knowledge base.*****### TL;DR: In this article, we go over how the Turing team used text generation with embedding endpoints to build a chatbot for customer support. This article’s title and TL;DR have been generated with Cohere. Get started with text generation Team Turing, consisting of Bence Gadanyi, Edwin Holst, Jonathan Fernandes, and Artur Gasparyan, presented their project as an NLP tool for customer chat support agents. The program uses a company’s knowledge base to generate automated responses to customer queries. When implemented successfully, this type of app could substantially reduce the costs and time associated with customer support training and increase overall staff efficiency.*****## Intended Use Case Embeddings may be used for purposes such as estimating semantic similarity between two sentences, choosing a sentence which is most likely to follow another sentence, sentiment analysis, topic extraction, or categorizing user feedback. Performance of embeddings will vary across use cases depending on the language, dialect, subject matter, and other qualities of the represented text.*****## Use Cases for Embeddings What can you do with embeddings? Quite a lot. Our Embed endpoint takes a piece of text and creates a vector embedding, which represents the text as numbers that capture its meaning and context. Embedding transforms unstructured text data into a structured form that allows you to cluster, categorize, and semantically (contextually) search the text. Cohere’s powerful language models can find relationships between pieces of text (words, phrases, sentences, paragraphs, or documents) that would not surface with linear keyword search. Some examples of industries that could benefit from vector search include: Overall, any industry that relies on large amounts of information, and the ability to quickly and accurately search for that information, could benefit from vector search.*****## Use Cases Multilingual Semantic Search : Improve your search results regardless of the language. Aggregate Customer Feedback : Organize customer feedback across hundreds of languages, simplifying a major challenge for international operations. Cross-Lingual Zero-Shot Content Moderation : Identify harmful content in online communities is challenging, especially as users speak hundreds of languages. Train a model with a few English examples, then detect harmful content in 100+ languages.*****And since these are sets of numbers, the ways you can process and extract insights from them are limited only by your imagination. What does this bring? It opens up many possible use cases that apply in the real world today. Embeddings power applications we interact with on a daily basis, such as modern search engines, eCommerce product recommendations, social media content moderation, email spam filtering, customer support conversational agents, and many more. In this article, we take a visual approach to understand: The examples from this article are taken from a Python notebook which you can try out here .*****When you hear about large language models (LLM), probably the first thing that comes to mind is the text generation capability, such as writing an essay or creating a marketing copy. But another thing you can get is text representation: a set of numbers that represent what the text means, and somehow capture the semantics of the text. These numbers are called text embeddings . Text Embeddings give you the ability to turn unstructured text data into a structured form. With embeddings, you can compare two or more pieces of text, be it single words, sentences, paragraphs, or even longer documents.*****Embeddings are a way to represent the meaning of text as a list of numbers. This is useful because once text is in this form, it can be compared to other text for similarity. Using a simple comparison function, we can calculate a similarity score for two embeddings to figure out whether two texts are talking about similar things. In the example below, the embeddings for two phrases have a , and the embeddings for two phrases have a :*****## 6. Cluster Clustering is another use case category that leverages text embeddings. The idea is to take a group of documents and make sense of how they are organized and how they are related to each other. In the previous use case, we visualized a set of documents on a plot to get a sense of how a set of documents are similar, or different, from each other. Clustering uses the same principles, but adds another step of organizing them into groups. This can be done via clustering algorithms, for example, k-means clustering , where we specify the number of clusters and the algorithm will return the appropriate cluster associated with each piece.*****## Use Case 3: Cross-Lingual Zero-Shot Content Moderation In today’s world, content moderation remains a major challenge. Social platforms like online gaming are attracting a wider international audience, which increases the complexity of content moderation efforts. As hateful content makes its way across multiple languages, it has a greater probability of passing through current content moderation tools that only catch English comments. To tackle this challenge, platforms can now use Cohere’s multilingual embeddings model to build a content moderation tool that works across 100+ languages and only requires training data in English. For the content moderation use case, the model just needs a handful of training examples in one language that demonstrate harmful and acceptable content.*****Now, compare them to the other kinds of inquiries, such as those related to airline information (see two examples below). Notice that while the embeddings about ground transportation inquiries look very similar to each other, they are distinctive from the rest. Here, the model was able to capture the context and meaning of each piece of text and it then represents them as embeddings. Each dimension of an embedding, called a feature , represents a certain universal characteristic about text according to how the model understands it. How is this possible? A large language model has been pre-trained with a vast amount of text data, where the training objective is set up in such a way to encourage the model to extract contextual information about a piece of text and store it as embeddings.*****# How It Works Embeddings are numerical representations of meaning in text. Because they are numbers, they can be compared to each other for similarity. They can also be plotted on a chart that shows which texts are similar to each other. Large language models produce highly nuanced embeddings.*****Embeddings are a way to represent the meaning of text as a list of numbers. This is useful because once text is in this form, it can be compared to other text for similarity. Using a simple comparison function, we can calculate a similarity score for two embeddings to figure out whether two texts are talking about similar things. In the example below, the embeddings for two phrases have a , and the embeddings for two phrases have a :*****embeddings . Embeddings at their core allow computers to efficiently compare pieces of text and measure the degree of similarity between them. They do this by creating a long sequence of numbers that encode what words mean based on their context (e.g., bank of a river vs. money in the bank). These sequences are called “vectors,” and they allow the algorithm to see “nearest neighbors” to understand the similarity between words and their nuances. Vector search enables a contextual understanding that makes it far more accurate and effective than keyword- or metadata-based search. When a development team wants to explore the many uses of*****## Use Cases for Embeddings What can you do with embeddings? Quite a lot. Our Embed endpoint takes a piece of text and creates a vector embedding, which represents the text as numbers that capture its meaning and context. Embedding transforms unstructured text data into a structured form that allows you to cluster, categorize, and semantically (contextually) search the text. Cohere’s powerful language models can find relationships between pieces of text (words, phrases, sentences, paragraphs, or documents) that would not surface with linear keyword search. Some examples of industries that could benefit from vector search include: Overall, any industry that relies on large amounts of information, and the ability to quickly and accurately search for that information, could benefit from vector search.*****The Embed endpoint takes a piece of text and turns it into a vector embedding. Embeddings represent text in the form of numbers that capture its meaning and context. This gives us the ability to turn unstructured text data into a structured form that can be processed and analyzed.*****## Deploy the SageMaker endpoint using a notebook Cohere has packaged Medium models, along with an optimized, low-latency inference framework, in containers that can be deployed as SageMaker inference endpoints. Cohere’s containers can be deployed on a range of different instances (including ml.p3.2xlarge, ml.g5.xlarge, and ml.g5.2xlarge) that offer different cost/performance trade-offs. These containers are currently available in two Regions: us-east-1 and eu-west-1. Cohere intends to expand its offering in the near future, including adding to the number and size of models available, the set of supported tasks (such as the endpoints built on top of these models), the supported instances, and the available Regions.*****AWS developers — we have some exciting news! Cohere is now available through AWS Marketplace , making it even more accessible to developers and businesses of all kinds. Being a part of Amazon’s online store makes it easy for you to discover and explore Cohere’s language AI capabilities for your applications. The Cohere team is super excited about reaching a broad community of developers through this prominent channel. The Cohere API can be used in different libraries that fit every stack, and you can quickly build machine learning capabilities into your apps on AWS using our Python, Node, and Go SDKs.*****## Conclusion The availability of Cohere natively on SageMaker via the AWS Marketplace represents a major milestone in the field of NLP. The Cohere model’s ability to generate high-quality, coherent text makes it a valuable tool for anyone working with text data. If you're interested in using Cohere for your own SageMaker projects, you can now access it on SageMaker JumpStart . Additionally, you can reference Cohere’s GitHub notebook for instructions on deploying the model and accessing it from the Cohere Generate endpoint .*****To help developers get started quickly, Cohere has provided Jupyter notebooks that make it easy to deploy these containers and run inference on the deployed endpoints. With the preconfigured set of constants in the notebook, deploying the endpoint can be easily done with only a couple of lines of code as shown in the following example: After the endpoint is deployed, users can use Cohere’s SDK to run inference. The SDK can be installed easily from PyPI as follows: It can also be installed from the source code in Cohere’s public SDK GitHub repository . After the endpoint is deployed, users can use the Cohere Generate endpoint to accomplish multiple generative tasks, such as text summarization, long-form content generation, entity extraction, or copywriting.*****The company builds and continually improves its general-purpose large language models (LLMs), making them accessible via a simple-to-use platform. Companies can use the models out of the box or tailor them to their particular needs using their own custom data. Developers using SageMaker will have access to Cohere's Medium generation language model. The Medium generation model excels at tasks that require fast responses, such as question answering, copywriting, or paraphrasing. The Medium model is deployed in containers that enable low-latency inference on a diverse set of hardware accelerators available on AWS, providing different cost and performance advantages for SageMaker customers. “Amazon SageMaker provides the broadest and most comprehensive set of services that eliminate heavy lifting from each step of the machine learning process,” said Rajneesh Singh, General Manager AI/ML at Amazon Web Services.*****### Tempering excitement with care As social media gets swept up in posts that claim “ I made model X do impossible task Y 🤯 ”, it’s important to arm oneself with a discerning eye to filter these claims. One of the key questions to ask is whether a demonstrated capability is a 🍒 cherry-picked example that a model produces 40% of the time, or if it points to robust and reliable model behavior. Reliability is key for an AI capability to become part of a customer-facing product. Take for instance, the many capabilities attributed to large GPT models in the last few years.*****Token likelihood is a useful tool for model evaluation. For instance, let's say you've trained a custom model and would like to know how much it's improved over the default model - you could use token likelihoods to compare the performance of the models on some held-out text. Here is a quick demonstration of how to use the return_likelihoods parameter from the Generate endpoint for model evaluation.*****The second part of the tool handles cases where the answer to the customer’s question is not in the knowledge base or when the question is not relevant to the product. To do this, the tool uses the Generate endpoint to provide a reasonable response to the customer’s query. In this example, the customer’s statement has not specified the product for which they need information. In this instance, Cohere Generate returns the following response:*****“We’re excited to offer Cohere’s general purpose large language model with Amazon SageMaker. Our joint customers can now leverage the broad range of Amazon SageMaker services and integrate Cohere’s model with their applications for accelerated time-to-value and faster innovation.” “As Cohere continues to push the boundaries of language AI, we are excited to join forces with Amazon SageMaker,” said Saurabh Baji, Senior Vice President of Engineering at Cohere. “This partnership will allow us to bring our advanced technology and innovative approach to an even wider audience, empowering developers and organizations around the world to harness the power of language AI and stay ahead of the curve in an increasingly competitive market."*****# Uncover trends and patterns in text Turn text into numerical representations of language for deeper insights at scale. Embed makes it possible to algorithmically categorize and score text quickly to extract meaning.*****# How It Works Embeddings are numerical representations of meaning in text. Because they are numbers, they can be compared to each other for similarity. They can also be plotted on a chart that shows which texts are similar to each other. Large language models produce highly nuanced embeddings.*****### Extract insights with an API call No matter your level of experience with ML/AI, the Cohere Platform makes it easy to integrate language comprehension into your application. We support all common languages through native SDK support or encapsulated REST calls, Our models have been trained on billions of words, allowing them to understand nuance and context.*****#### What's possible with Embed Semantic Search Enable users to search using conversational language. Topic Modeling Cluster similar topics and discover thematic trends across a body of text sources. Recommendations Build a recommendation engine and engage your users with more relevant content. Multilingual embeddings Run topic modeling, semantic search, recommendations across 100+ languages with just one model. Read more .*****#### Embed Capture the semantic meaning of text by representing text as numbers. Model Price per Unit Default $1.0 per 1000 Embeddings Custom $2.0 per 1000 Embeddings More details*****This Python notebook , also leveraging the Embed endpoint, goes into detail about how to make sense of three thousand “Ask HN” (Hacker News) posts. First, the text embeddings for each are generated. This is followed by clustering them into smaller groups by the theme or topic of the posts, supplemented by the keywords that represent the topic of each group. Finally, these posts are visualized on a plot, shown in the image below, where one color represents a topic cluster. Below you can see a few topics emerging, such as life, career, coding, startups, and computer science. This technique can be applied to number of different tasks, such as:*****At Cohere, we are committed to breaking down barriers and expanding access to cutting-edge NLP technologies that power projects across the globe. By making our innovative multilingual language models available to all developers, we continue to move toward our goal of empowering developers, researchers, and innovators with state-of-the-art NLP technologies that push the boundaries of Language AI. Our Multilingual Model maps text to a semantic vector space, positioning text with a similar meaning in close proximity. This process unlocks a range of valuable use cases for multilingual settings. For example, one can map a query to this vector space during a search to locate relevant documents nearby.*****## Deploy the SageMaker endpoint using a notebook Cohere has packaged Medium models, along with an optimized, low-latency inference framework, in containers that can be deployed as SageMaker inference endpoints. Cohere’s containers can be deployed on a range of different instances (including ml.p3.2xlarge, ml.g5.xlarge, and ml.g5.2xlarge) that offer different cost/performance trade-offs. These containers are currently available in two Regions: us-east-1 and eu-west-1. Cohere intends to expand its offering in the near future, including adding to the number and size of models available, the set of supported tasks (such as the endpoints built on top of these models), the supported instances, and the available Regions.*****Bullying, threatening, shaming, or doxxing. : Belittling victims of serious physical or emotional harm (even if unintentional). : Sharing of divisive generated content in order to turn a community against itself. : Perpetuating racism, or sexism (even if unintentional). Attempting to characterize gender, race, or ethnicity. : Distribution of sexually explicit acts, torture, or abuse. : Attempting to influence political decisions, or opinions. : Catfishing, phishing, or attempting to circumvent the law. : Sending unsolicited email and messages, or manipulating search engines. : Representing raw generations as coming from humans, using supervised generations with false identities, or a single person using generations with many identities that appear to be independent.*****It’s a complex problem that is currently being met with rudimentary AI or keyword search, allowing nuanced (e.g. an insult that requires an understanding of pop culture) or ‘nonuniform’ (e.g. a slur deliberately spelled incorrectly) hate to slip through the net. To tackle toxicity, businesses need to augment their existing solutions with advanced and customizable natural language processing (NLP) that can understand the context of posts. Today, that shift has begun, with many online platforms already leveraging Cohere’s LLMs and advanced AI to bolster their content moderation approach and reduce churn.*****### 3. Understand and Prevent Disallowed Use Cases The Cohere Platform may not be used for any of the following purposes. The description for each disallowed use case is illustrative but ; Cohere reserves the right to terminate access for harms which are not listed at our sole discretion. : Actions that threaten, encourage, or incite violence against anyone, directly or indirectly. : Promoting or glorifying acts of self-harm, such as cutting, eating disorders like anorexia or bulimia, and suicide. : Promoting or celebrating sexual exploitation, including the sexualization of minors. : Promoting hatred or glorifying abuse against people based on characteristics like race, ethnicity, national origin, religion, disability, disease, age, sexual orientation, gender, or gender identity.*****### 2. Document your Application We encourage careful consideration and documentation of the potential harms of any application developed using the Cohere Platform. If you build an application that uses model outputs, please provide your users a link to the corresponding model card, explaining how your application uses its output. For example, if you trained a downstream classifier using the embed endpoint, users should be provided with thorough documentation (such as model cards and data statements) of that classifier's training procedure and behaviour.*****## 1. Pick the top token: greedy decoding You can see in this example that we picked the token with the highest likelihood, ‘United’. Greedy decoding is a reasonable strategy but has some drawbacks such as outputs with repetitive loops of text. Think of the suggestions in your smartphone's auto-suggest. When you continually pick the highest suggested word, it may devolve into repeated sentences.*****and images . For builders, it’s important to know that those representations enable a wide variety of possibilities in addition to generation. One of these key possibilities is neural search. Neural search is the new breed of search systems that use language models to improve on simple keyword search. They enable searching by meaning . neural search in this video Neural search fits alongside text classification as use cases where AI produces reliable results for many industry use cases (some challenging areas include sarcasm classification).*****It’s a complex problem that is currently being met with rudimentary AI or keyword search, allowing nuanced (e.g. an insult that requires an understanding of pop culture) or ‘nonuniform’ (e.g. a slur deliberately spelled incorrectly) hate to slip through the net. To tackle toxicity, businesses need to augment their existing solutions with advanced and customizable natural language processing (NLP) that can understand the context of posts. Today, that shift has begun, with many online platforms already leveraging Cohere’s LLMs and advanced AI to bolster their content moderation approach and reduce churn.*****Then, using the Transformer architecture that underpins Google Search and Translate, Cohere’s content moderation Classify endpoint can be easily trained with these specific examples to identify toxic content. Here’s how it works: The toxic comments can automatically be removed, or flagged to a content moderator, depending on a company’s individual approach.*****## Streamlining NLP Development for Startups Cohere’s endpoints are specific operations that your developers can use to classify, search, summarize, generate, extract, and cluster text. They power a range of business processes, such as content moderation, customer support, and generating marketing copy, allowing your startup to build with NLP much more quickly and economically.*****### AI use cases that are reliable  now There are, however, other use cases (and workflows) where these models are capable of much more reliable results. Key amongst them are neural search (more in that in point #4 below), auto categorization of text (classification), and copywriting suggestions and brainstorming workflows for generation models (discussed in more detail in part three of this series). The amazing demos will keep rolling in. They’re part of a community discovery process for the limits and new possibilities of these models (more on community discovery of a model’s generative space and its product/economic value in part two).*****It’s a complex problem that is currently being met with rudimentary AI or keyword search. But these often allow nuanced (e.g., an insult that requires an understanding of pop culture) or nonuniform (e.g., a slur deliberately spelled incorrectly) hate speech to slip through the net. How Cohere can help*****Your result should appear similar to what’s shown below. You can go a step further to visualize the semantic search result in a scatterplot. You’ll use the same column created earlier, which represents if the similarity score is greater than 33%. The plot below shows that the search query, “graph network strategies,” is located closest to the AI papers about puzzles/games, path-finding, and bayesian probability. Below is another plot displaying the semantic search results for “language and translation.” Similar nodes are located near nodes about linguistics, neural networks, and image captions.*****## Conclusion This has been a demonstration of a Language AI system that shows some of the possibilities of pipelines using generation and embedding in successive steps. It’s an area we’re super excited about, and we would love to see what you build with it. Be sure to drop in to the Cohere co:mmunity on Discord to ask us questions or to show us anything cool you build with such systems.*****### Use Cohere’s Classify for automated text classification Starting from the same Transformer architecture as Google’s Search and Translate, Cohere’s Classify endpoint can read and understand text-based conversations, and identify toxicity. Behind the scenes, Cohere’s large language models are continuously learning toxic language patterns, and can recognize when language is used casually or in a malicious manner. Developers can further finetune the models with their own data set per their business or industry. Here’s how it works Set the road rules First, train Classify by inputting a few example comments from your community. Then, mark each as either “Toxic” or “Non-toxic,” or by any other label that makes sense to your content moderation workflow, like “Violates Policy” or “Permissible.”*****The number of tokens per word depends on the complexity of the text. Simple text may approach 1 token per word on average, while complex texts may use less common words that require 3-4 tokens per word on average. Our representation models are currently limited to processing sequences with a maximum length of 4096 tokens. Generation models support up to 2048 tokens. Our vocabulary of tokens is created using Byte Pair Encoding .*****There’s also a natural limit to the number of tokens the model can produce. Smaller models can go up to 1024 while larger models go up to 2048. It’s not recommended to hit those limits though. If you’re generating content using a large limit, the model may go off in a direction you’re not expecting. It’s generally recommended to generate in short bursts versus one long burst.*****## Embedding Max Tokens Have Increased We have increased previous max tokens per text from 512 to 1024. For any text longer than 128 tokens, the text is spliced and the resulting embeddings of each component are averaged and returned.*****## Number of Tokens I said earlier that the language model builds a list of words and their probabilities as outputs. This is technically incorrect. It builds a list of tokens, which is roughly 4 characters, but not always. For example, a word like “water” might end up being one token, whereas larger words might be broken up into multiple tokens. At Cohere, we use byte-pair encoding to create tokens . You probably don’t want the language model to keep generating outputs ad infinitum, so the number of tokens parameters allows you to set a limit to how many tokens are generated.*****So we can scale the number of exemplars with linear complexity instead of quadratic complexity with respect to length. Experimental results on a diverse set of tasks show that our approach improves end-task performance and reduces evaluation variance over conventional in-context learning as the number of demonstration examples increases. Code has been released at this URL . Parallel Context Windows Improve In-Context Learning of Large Language Models Authors: Nir Ratner, Yoav Levine, Yonatan Belinkov, Ori Ram, Omri Abend, Ehud Karpas, Amnon Shashua, Kevin Leyton-Brown, Yoav Shoham For applications that require processing large amounts of text at inference time, Large Language Models (LLMs) are handicapped by their limited context windows, which are typically 2048 tokens.*****We've retrained our small , medium , and large generation and representation models. Updated representation models now support contexts up to 4096 tokens (previously 1024 tokens). We recommend keeping text lengths below 512 tokens for optimal performance; for any text longer than 512 tokens, the text is spliced and the resulting embeddings of each component are then averaged and returned.*****## Main Principles We find that there are two main ideas to keep in mind while designing prompts for our models.*****Here, we discuss a few principles and techniques for writing prompts (inputs for our models) that will help you get the best generations for your task. Choosing the right temperature can also have a big influence on generation quality. We discuss temperature separately here .*****### 1. A prompt guides the model to generate useful output If you need a summary of an article, for example, a large language model trained on enough data can generate a summary if you guide it as such:*****So in this screenshot, I gave it the instruction “Write a sentence using the word ocean” and it responded with (the part in bold) “The ocean is vast and beautiful.” That was a pretty basic example. However, when you’re talking to a human, you can often be vague with your instructions and expect the other person understands. With a Language Model, sometimes you need to be a little clear with your instructions or word them in a certain way to get the best outcome. This is called prompt engineering.*****### 2. Try multiple formulations of your prompt to get the best generations When using generate, it is useful to try a range of different prompts for the problem you are trying to solve. Different formulations of the same prompt which might sound similar to humans can lead to generations that are quite different from each other. This might happen, for instance, because our models have learned that the different formulations are actually used in very different contexts and for different purposes. Below we give a number of examples that we've found to work particularly well for different tasks. In the summarization example, if “In summary,” doesn’t lead to a good generation, we may want to try “To summarize in plain language,“ or “The main point to take from this article is that”.*****### 3. Describe the task and the general setting Provide the model with enough context. For example, we can describe the summarization task in more detail before the article. Let's consider a few more aspects of this by looking at a different example. Suppose that you would like to use our models to assist your customer satisfaction department by automatically generating plausible responses to customer requests (note: the generations are not to be sent to the customers, this is only a simulation). A customer contacts your company with the following question:*****### Limit Output Size Since the language model is predicting the next word in a sequence, it may predict something that makes sense but takes it into a weird tangent. If you want to avoid that, don’t let the model generate for too long. Have it generate short amounts of text by limiting it with the “number of tokens” parameter. If you want longer outputs, you can feed the short output back into the model and have it continue from there. A better way to do this is to run some sort of post-processing on the first output, like removing repetition or irrelevant content, and then feeding the cleaned version back in.*****### Provide Guardrails Let’s go back to the translation task from earlier. I’m going to give it the same prompt but with a very small change. Uh oh, what happened here? At the core, a language model tries to predict the next word in a sequence. You may give it a task description that makes sense to a human but the language model doesn’t actually understand it. It’s simply just continuing the pattern. That’s why few-shot works so well. You’re building a pattern that the model can follow. In this new translation example, it’s also following a pattern, just not the pattern you were expecting.*****### Reflecting on the moments that make us the best place to work in NLP 2022 was a significant year for AI. Generative AI, in particular, captured the public’s imagination with its ability to create images from any imaginable text prompt and its ability to power chatbots with capabilities far beyond what was commonly thought possible. Now there are more eyes on our industry than ever before, and they’re looking for meaningful applications of generative and language technologies. At Cohere, we’re on a mission to deliver them. Throughout 2022, we focused on helping businesses of all sizes better take advantage of language AI in the real world, whether through predictive text generation like in copywriting or other functions like search, conversational AI, summarization and content moderation.*****and new events like co:lab Fridays and the Talking Language AI series . As a result, we watched thousands of developers build more than 300 apps on top of Cohere’s platform. These include standouts like a Discord bot that provides a summary of a channel's activity (try it on our Discord here ), and a tool that helps you search and summarize research papers . You can see more inspiring demos built by our community here . We collaborated with incredible groups across numerous hackathons, including Lablab, Hack the North and CalHacks, and we partnered with Major League Hacking to participate in over 30 hackathons.*****Meanwhile, we also had the privilege to host the Women & Non-Binary in Tech event series this past November and welcomed our very first batch of Community Champions in December. We welcomed incredible talent and partners. This past year, we expanded our exec team with Martin Kon joining from YouTube as Cohere’s President and COO, and Jaron Waldman, previously at Apple and Rakuten, taking on the role of Chief Product Officer. We also expanded on our impressive roster of technical talent, welcoming Phil Blunsom as Chief Scientist and Ed Grefenstette as our Head of Machine Learning, both of them opening the doors to our*****We’re making natural language processing more accessible to developers, including those without big tech resources or specialized teams. That means more businesses have been able to tap into the power of NLP. Looking back over the last 12 months, I’m incredibly proud of what our team has accomplished. We launched our co:mmunity Discord channel , an initiative that has inspired over 300 demos to be built with Cohere. We launched Cohere For AI (C4AI), a non-profit research lab that seeks to solve complex machine learning problems. We initiated Best Practices in using Large Language Models in partnership with OpenAI and AI21 Labs.*****new London HQ . And we were especially excited to have Nils Reimers join us as Director of Machine Learning to help us double down on semantic search . The brilliant Sara Hooker also joined us to spearhead Cohere For AI – a non-profit research lab that seeks to solve complex machine learning problems. Since that launch, the C4AI team has been hard at work publishing and promoting fundamental ML research, growing the community, and launching the Scholars Program designed to provide more entry points into ML research. You can read more about their highlights here . Meanwhile, in January, we kicked off a*****The multilingual-22-12 model can be used to semantically search within a single language, as well as across languages. Compared to keyword search, where you often need separate tokenizers and indices to handle different languages, the deployment of the multilingual model for search is trivial: no language-specific handling is needed — everything can be done by a single model within a single index. We’re extremely proud of the performance of our multilingual understanding model. It outperformed the industry standard (the next best model) in search tasks by more than 230%.*****Cohere's multilingual text understanding model is now available! The multilingual-22-12 model can be used to semantically search within a single language, as well as across languages. Compared to keyword search, where you often need separate tokenizers and indices to handle different languages, the deployment of the multilingual model for search is trivial: no language-specific handling is needed — everything can be done by a single model within a single index. In addition to our new model, you can now detect the language of a data source using co.detect_language() endpoint. For more information, see our multilingual docs .*****## Training Data Training embedding models require data in a specific format. For example, question/answer pairs or title/document pairs. We can then learn which text pairs should be closer together in the vector space in order to enable applications like semantic search. To train Cohere’s new multilingual model, we processed and carefully cleaned terabytes of data from various sources: Wikipedia, news publications, scientific articles, and online communities across hundreds of languages. This resulted in a large training corpus of more than 900 million training pairs for English and 450 million training pairs for other languages. Other multilingual embedding models often rely on machine translation for training dataset creation, which creates an awkward bias for these models.*****However, these models don’t capture the nuances behind language usage in different countries. Contrary to this approach, we collected a dataset of nearly 1.4 billion question/answer pairs across tens of thousands of websites in hundreds of languages. These are questions actually asked by speakers of said languages, allowing us to capture language- and country-specific nuances. “I strongly believe that embeddings are the future of search and recommendation. Thanks to the new Cohere multilingual model and the text2vec Cohere module in Weaviate, we can bring this to developers worldwide with a single command.” - Bob van Luijt, CEO at SeMI Technologies multilingual-22-12*****## How Does the Multilingual Text Understanding Model Work? Cohere’s multilingual text understanding model maps text to a semantic vector space (also known as “ embeddings ”), positioning texts with a similar meaning in close proximity. This process unlocks a range of valuable use cases for multilingual settings. For example, one can map a query to this vector space during a search to locate relevant documents nearby. This often yields search results that are several times better than keyword search. To train multilingual models, you need large quantities (hundreds of millions) of suitable training pairs, like question/answer pairs. So far, such training data has been primarily available in English, and prior work tried to use machine translation to map it to other languages.*****## What is a Multilingual Text Understanding Model? Multilingual text understanding models are powerful models that can derive insights from text data across languages. At Cohere, we’ve trained our model specifically to be used for and While many of these models are available for English, similar existing multilingual models only work well for short sentences and can’t capture the meaning behind longer text. This prevents them from being used for semantic search, which typically aims to match a short query with a longer, relevant document. In this blog post, we will cover three relevant use cases that showcase the power of Cohere’s new multilingual model:*****Language detection is a necessary first step for businesses that deal with multilingual user bases. Whether you are working with a single multi-lingual model or a multi model environment, understanding the language of a request (e.g. query, input) is paramount to a good user experience. Co.detect_language is an endpoint gives the following information for a text input: The full name ( language_name ) of the language the input is in. A language_code describing the language the input is in. For example, the ISO code for English is en .*****### Develop, test, and experiment with the industry’s first multilingual text understanding model that supports 100+ languages Humans speak over 7100 languages, yet the majority of language models only support the English language. This makes it incredibly challenging to build products and projects using multilingual language understanding. Cohere’s mission is to solve that by empowering our developers with technology that possesses the power of language. That’s why today we’re introducing our first multilingual text understanding model that supports over 100 languages and delivers 3X better performance than existing open-source models. This will enable new markets, countries, and global companies to better serve their customers across the globe.*****Developers can then train a classifier (in English or a language of their choice) to find the decision boundary in the vector space that helps determine which type of content — in 100+ languages — is undesirable on their platform. The following demo showcases Cohere’s multilingual model used to build a recommendation movie engine and obtain relevant results regardless of the language used in the search query or the content source. In addition, it demonstrates multilingual sentiment analysis classification across a wide variety of languages Test out the multilingual search and recommendation demo , multilingual sentiment analysis classification demo, and watch a demonstration:*****Our language models understand "tokens" rather than characters or bytes. One token can be a part of a word, an entire word, or punctuation. Very common words like "water" will have their own unique tokens. A longer, less frequent word might be encoded into 2-3 tokens, e.g. "waterfall" gets encoded into two tokens, one for "water" and one for "fall". Note that tokenization is sensitive to whitespace and capitalization. Here are some references to calibrate how many tokens are in a text: one word tends to be about 2-3 tokens a verse of a song is about 128 tokens this short article has about 300 tokens*****The number of tokens per word depends on the complexity of the text. Simple text may approach 1 token per word on average, while complex texts may use less common words that require 3-4 tokens per word on average. Our representation models are currently limited to processing sequences with a maximum length of 4096 tokens. Generation models support up to 2048 tokens. Our vocabulary of tokens is created using Byte Pair Encoding .*****### How to pick max_tokens when sampling The easiest way to determine a good number of tokens is to guess and check using our playground. It is common to request more tokens than required and then run additional processing to retrieve the desired output.*****Token likelihood is a useful tool for model evaluation. For instance, let's say you've trained a custom model and would like to know how much it's improved over the default model - you could use token likelihoods to compare the performance of the models on some held-out text. Here is a quick demonstration of how to use the return_likelihoods parameter from the Generate endpoint for model evaluation.*****Additionally, you can also use the likelihood feature in the playground to see if there are particular words, phrases, or structures that the model has trouble understanding. However, keep in mind that the average likelihood of tokens will always be high at the beginning of the sequence. The model might assign low likelihood to the first time you introduce a novel concept or name, but once it has seen it once it can readily use it in the generation. You can also use the likelihood capability to see if there is any spelling or punctuation that is creating issues for tokenization.*****This is where it will send feedback analysis. You can use the default channel to check messages for product feedback. Create a text channel on the server by right-clicking in an empty area of the channel list section, selecting , and naming it . If you want, you can create it as a private channel and organize it into its own category, so that it’s closer to a real scenario. In your Discord application, click on and navigate to . Toggle the to enable it and exit the settings. Right-click on the channel and choose to get the channel's ID. Then, save it with the other saved keys/tokens.*****Cohere's multilingual text understanding model is now available! The multilingual-22-12 model can be used to semantically search within a single language, as well as across languages. Compared to keyword search, where you often need separate tokenizers and indices to handle different languages, the deployment of the multilingual model for search is trivial: no language-specific handling is needed — everything can be done by a single model within a single index. In addition to our new model, you can now detect the language of a data source using co.detect_language() endpoint. For more information, see our multilingual docs .*****These results demonstrate that multilingual language models encode information along orthogonal language-sensitive and language-neutral axes, allowing the models to extract a variety of features for downstream tasks and cross-lingual transfer learning. UL2: Unifying Language Learning Paradigms Authors: Yi Tay, Mostafa Dehghani, Vinh Q. Tran, Xavier Garcia, Jason Wei, Xuezhi Wang, et al. Existing pre-trained models are generally geared towards a particular class of problems. To date, there seems to be still no consensus on what the right architecture and pre-training setup should be. This paper presents a unified framework for pre-training models that are universally effective across datasets and setups. We begin by disentangling architectural archetypes with pre-training objectives -- two concepts that are commonly conflated.*****At Cohere, we are committed to breaking down barriers and expanding access to cutting-edge NLP technologies that power projects across the globe. By making our innovative multilingual language models available to all developers, we continue to move toward our goal of empowering developers, researchers, and innovators with state-of-the-art NLP technologies that push the boundaries of Language AI. Our Multilingual Model maps text to a semantic vector space, positioning text with a similar meaning in close proximity. This process unlocks a range of valuable use cases for multilingual settings. For example, one can map a query to this vector space during a search to locate relevant documents nearby.*****## Final Thoughts At Cohere, we are committed to breaking down barriers and expanding access to cutting-edge NLP technologies that power projects across the globe. By making our innovative multilingual language model available to all developers, we continue to move toward our goal of empowering developers, researchers, and innovators with state-of-the-art NLP technologies that push the boundaries of language AI. Sign up and try our new multilingual model for free. If you would like to discuss your multilingual use case, please don’t hesitate to contact us . 1. “Ethnologue: Languages of the World,” Ethnologue , 2022 (accessed Nov. 25, 2022).*****## Differences Between English and Multilingual Embedding Models Unlike our English language embedding model, our multilingual model was trained using dot product calculations. Using dot products produces a non-normalized similarity score, reflecting the magnitude of the two compared vectors. When this dimension is incorporated, multilingual embeddings perform better than standard. For more information on how our English language model works (using cosine similarity), see our introductory guide to the Cohere platform . The dimensions of our multilingual embeddings is 768 dimensions.*****## Training Data Training embedding models require data in a specific format. For example, question/answer pairs or title/document pairs. We can then learn which text pairs should be closer together in the vector space in order to enable applications like semantic search. To train Cohere’s new multilingual model, we processed and carefully cleaned terabytes of data from various sources: Wikipedia, news publications, scientific articles, and online communities across hundreds of languages. This resulted in a large training corpus of more than 900 million training pairs for English and 450 million training pairs for other languages. Other multilingual embedding models often rely on machine translation for training dataset creation, which creates an awkward bias for these models.*****### Develop, test, and experiment with the industry’s first multilingual text understanding model that supports 100+ languages Humans speak over 7100 languages, yet the majority of language models only support the English language. This makes it incredibly challenging to build products and projects using multilingual language understanding. Cohere’s mission is to solve that by empowering our developers with technology that possesses the power of language. That’s why today we’re introducing our first multilingual text understanding model that supports over 100 languages and delivers 3X better performance than existing open-source models. This will enable new markets, countries, and global companies to better serve their customers across the globe.*****directly in the Weaviate vector search engine as a vectorization module. The Pinecone vector database makes it easy to build high-performance vector search applications. Use Cohere to generate language embeddings, then store them in Pinecone and use them for Semantic Search. Cohere offers optimized containers that enable low latency inference on a diverse set of hardware accelerators available on AWS, providing different cost and performance points for Sagemaker customers. Integrate the Surge AI labeling platform into your Cohere workflow. Use Scale's labelled datasets with Cohere's Large Language Models.*****Qdrant is an open-source vector similarity search engine and vector database. It provides a production-ready service with a convenient API to store, search, and manage points - vectors with an additional payload. Qdrant is tailored to extended filtering support. It makes it useful for all sorts of neural-network or semantic-based matching, faceted search, and other applications. Qdrant is written in Rust, which makes it fast and reliable even under high load. Weaviate is an open source vector search engine that stores both objects and vectors, allowing for combining vector search with structured filtering. The text2vec-cohere module allows you to use Cohere embeddings*****Ivan Zhang, co-founder of Cohere, has been leading the partnership effort and expressed his excitement for its possibilities. “NLP is the next frontier for the next generation of developers and researchers, and alongside Mila’s world-class research community, we’re excited to see what types of ideas will be exchanged and what new discoveries we’ll share. The partnership will allow more researchers to tap into the power of large language models, unlocking a massive amount of opportunities for research.” Personally, I’m thrilled that Cohere will have the opportunity to collaborate directly with Mila’s diverse community. Both Mila members and Cohere’s teams will be able to learn new perspectives from each other, as well as help develop talent in the field.*****Along the way, I look forward to uncovering research insights, as well as interesting new use cases for NLP. Industry collaboration is not new for us, in fact, it’s core to our DNA. Mila joins our cohort of valued partners, including the Vector Institute for Artificial Intelligence and Canadian accelerator Communitech . We believe that it's crucial to support like-minded organizations like Mila and our partners, and work together with them to build the foundations for the future of artificial intelligence. If your organization shares our mission and is interested in partnering with Cohere, let’s talk! Reach out via email at*****## A Match That Makes Sense A vector search database like Vertex Matching Engine isn’t useful without vectors — and the higher the quality vectors you have, the more efficient the database can be. Embeddings by themselves, no matter the quality, don’t add value unless you can efficiently compare them. This is why this match — using Cohere within VME — can help companies scale and access the full potential of embeddings for real-world use cases. To learn how Cohere and Vertex Machine Engine work, please contact us .*****## Why VME + Cohere Is a Game-Changer Beyond streamlining several different processes under one roof, so to speak, within the Google Cloud ecosystem, VME is massively scalable, really fast, and fully managed. A vector search database at its core, VME can search across billions of embedding vectors at high queries per second with very low latency. Oftentimes, you may need to sacrifice accuracy on the altar of speed and low latency. However, when tested on many real-world applications, VME retained a recall (metric that measures percentage of true nearest neighbors) of 95-98% while serving results with 90th percentile latency less than 10ms.******### Source Demographics The scraped data is similar in composition to many other large, Internet-sourced language modeling datasets, and hence reflects perspectives that skew young, white, and male ( Bender et al., 2021 ). Language models trained on such data encode the hegemonic viewpoint; Jo and Gebru, 2021 detail issues and solutions around this topic in-depth. Enhancing the diversity of our training data is a top priority as we continue to iterate our data collection process.*****Bullying, threatening, shaming, or doxxing. : Belittling victims of serious physical or emotional harm (even if unintentional). : Sharing of divisive generated content in order to turn a community against itself. : Perpetuating racism, or sexism (even if unintentional). Attempting to characterize gender, race, or ethnicity. : Distribution of sexually explicit acts, torture, or abuse. : Attempting to influence political decisions, or opinions. : Catfishing, phishing, or attempting to circumvent the law. : Sending unsolicited email and messages, or manipulating search engines. : Representing raw generations as coming from humans, using supervised generations with false identities, or a single person using generations with many identities that appear to be independent.*****Execute the command below to run the app.py script. Due to the large volume of API requests, it can take 10-20 minutes for the entire process to complete. After its completion, the values of the results list will print out to show how many frameworks had a positive sentiment and how many times they did so. In the image below, you will see that React is used more often, but Node has a higher percentage of positive comments, with 224 positive tweets. At this point, you now have a basic analysis bot that you can run multiple times to analyze data from the Twitter Search API.*****Next, add the code below to the existing code within the app.py file to create a class for the analysis bot. The use of classes helps you keep the bot code clean, organized, and readable. The constructor method of the AnalysisBot above uses the requests package to make a GET request to the search endpoint of Twitter APIs, fetching 100 tweets and storing them in the retrieved_tweets list. The code places the request in a while loop to execute the GET request 100 times, fetching 10,000 tweets at the end of the loop. Alongside the retrieved tweets, a token value is part of the API response for pagination purposes.*****Generation or summarization of long-form documents (max: 300 tokens/call). Generation of content sensitive politically, economically, medically, or culturally. : : Sharing positive generated content in order to direct attention away from harmful actions. : Tools that promote academic dishonesty. Usages which appear to violate our guidelines should be reported within 24 hours to Cohere by contacting us at responsibility@cohere.ai . Intentional stress testing of the API and adversarial attacks are allowable, but violative generations must be disclosed here, reported immediately , and must not be used for any purpose except for documenting the result of such attacks in a responsible manner.*****# Calling Your Finetuned Model Via FastAPI Using your previously named sentiment_classifier.py, tweak the predict_sentiment function to make two API calls generating two response sets, one from your baseline and one from the fine-tuned model. Spin up a Uvicorn server again, go to http://127.0.0.1:8000/docs and try out the model endpoint. This endpoint produces results from both models, classifying your inputs as positive, negative or neutral, as well as the 28 GoEmotions labels that include anger, surprise, admiration, and so on.*****Bullying, threatening, shaming, or doxxing. : Belittling victims of serious physical or emotional harm (even if unintentional). : Sharing of divisive generated content in order to turn a community against itself. : Perpetuating racism, or sexism (even if unintentional). Attempting to characterize gender, race, or ethnicity. : Distribution of sexually explicit acts, torture, or abuse. : Attempting to influence political decisions, or opinions. : Catfishing, phishing, or attempting to circumvent the law. : Sending unsolicited email and messages, or manipulating search engines. : Representing raw generations as coming from humans, using supervised generations with false identities, or a single person using generations with many identities that appear to be independent.*****## Creating a Twitter Application A Twitter application’s API credentials are within the tab on the application’s settings page. If your bearer token value is unknown, click to regenerate the and copy the token to a secure file. You’ll use the Bearer Token to authenticate HTTP requests for fetching data from Twitter. With your Cohere API Key and the Bearer Token for your Twitter app, the stage is set for you to begin building the tweet analysis bot with Python.*****You can now call our API in your Python code. Here's an example of what that call would look like. We'll use this code later on. This creates our largest Classification model instance that predicts the sentiments of two text inputs based on 15 examples and the Cohere baseline classification model. When run, it outputs the input texts, the predicted sentiments, and the confidence levels of the predictions.*****Summarizing documents with modern large language models is a powerful technique for language understanding and production. Through our extensive research, which draws on years of experience in the field of Natural Language Processing, coupled with valuable feedback from our early customers, we have identified three key advantages that individuals and companies can benefit from by utilizing our Summarize endpoint.*****## TL;DR: “Artificial intelligence firm Cohere has launched a text summarization endpoint. Built on the firm's foundational language model, the Cohere Summarize beta service allows the condensing of essential information from large documents. The system can process long documents of up to 50,000 characters and includes a series of customizable settings.” “This tl;dr was generated by the Cohere Summarize product . Don’t believe it, see it in action . In a world where information overload is the norm, the ability to quickly and accurately distill essential information from documents has become increasingly important for businesses and individuals alike. That's why we are excited to provide*****### Power your content apps with NLP It’s easy to integrate Cohere into your apps using the platform’s simple API and set of language-specific SDKs.*****early access to the Cohere Summarize – a new endpoint powered by our foundational language model, customized for text summarization. Summarize beta empowers users to efficiently distill crucial information from lengthy documents and articles. This functionality greatly increases productivity and saves valuable time, making it essential for anyone looking to extract the most essential information quickly and accurately. Gone are the days when users had to worry about crafting the perfect prompt or teaching a model how to generate a summary. With our endpoint, summarizing any article or text is as simple as inputting it into the system, which will automatically generate a summary for you.*****## Get Started with Cohere Summarize API Accessing the Cohere Summarize API is easy and user-friendly. You can start exploring the API right away by accessing our Summarize playground , where you can input your own text and receive an instant summary generated by our language model. Alternatively, if you're a developer looking to integrate the Cohere Summarize API into your own application or workflow, you can access our documentation , which provides detailed instructions and code samples. You can also visit our Summarize product page . With our straightforward and accessible tools, getting started with Cohere Summarize has never been easier.*****### Join Cohere We’re always looking for new people to join our team. Check out our listings below or send us an email at talent@cohere.com Stay updated*****## Conquer Information Overload To tackle information overload and prioritize key business objectives, teams can extract the gist of longer documents and articles by identifying their key concepts.*****support@cohere.com .*****“At Cohere, it’s our mission to change that by developing a solution that’s simple – yet effective – and available to any developer.” Cohere’s addition to Google Cloud Marketplace serves as another solution for businesses seeking cloud software that enhances and streamlines their operations. “Helping developers build NLP models into applications in more natural, streamlined ways has become a key focus area for enterprises today,” said Dai Vu, Managing Director, Marketplace & ISV GTM Initiatives, Google Cloud. “We’re thrilled that Cohere’s solution is available on Google Cloud Marketplace to help customers drive deeper insights from their language-processing operations.” We can’t wait to see how Google Cloud developers will solve problems or build innovations using Cohere.*****Our new and improved xlarge has better generation quality and a 4x faster prediction speed. This model now supports a maximum token length of 2048 tokens and frequency and presence penalties.*****## New & Improved Medium & Extremely Large The new and improved medium and x-large outperform our existing generation models on most downstream tasks, including summarization, paraphrasing, classification, and extraction, as measured by our internal task-based benchmarks. At this time, all baseline calls to x-large and medium will still route to previous versions of the models (namely, xlarge-20220609 and  medium-20220926). To access the new and improved versions, you’ll need to specify the release date in the Playground or your API call: xlarge-20221108 and medium-20221108 . Older versions of the models (xlarge-20220609 & medium-20220926) will be deprecated on December 2, 2022.*****Calls to Generate large-20220926 and xlarge-20220609 will route to the new and improved X-Large model (xlarge-20221108). Calls to Generate small-20220926 will route to the new and improved Medium model (medium-20221108). If you have any questions or concerns about this change, please don’t hesitate to contact us at: team@cohere.com.*****### Prompting by Instruction The Command-Xlarge model works best when we provide an instruction-based prompt. One way to do this is by using imperative verbs to tell the model what to do, for example: generate, write, list, provide, and other variations. Let’s say that we are creating social media ad copy for a wireless earbuds product. We can write the prompt as follows. Generate a social ad copy for the product: Wireless Earbuds. At this point, ensure that you select command-xlarge in the MODEL dropdown in the right pane. Then, click on Generate. This generates the following output. That’s not bad.*****# Classification Pricing  Classifications will cost $5 per 1000 text classified across the platform, regardless of model size (small, medium, large, xlarge) or usage of a finetuned model. Get an API Key Share what you're building on our co:mmunity forum or shoot us an email .*****## NEW Command Model (Beta) We’re also introducing a Beta of our new Command model, a generative model that’s conditioned to respond well to single-statement commands. Learn more about how to prompt command-xlarge-20221108 . You can expect to see command-xlarge-20221108 evolve dramatically in performance over the coming weeks.*****🚧 This model is in active development. Our team is pushing hard to ensure command is safe, and helpful for our users. As you experiment with the model and run into issues, please help us by flagging it to our team by emailing us at team@cohere.com. Command is a generative model that responds well with instruction-like prompts, and is available in two sizes: medium and xlarge . The xlarge model demonstrates better performance, and medium is a great option for developers who require fast response, like those building chatbots. To reduce the turnaround time for releases, we have nightly versions of command available.*****Nightly versions of our Common models are now available. This means that every week, you can expect the performance of command-nightly to improve as we continually retrain them. Command-nightly will be available in two sizes - medium and xlarge . The xlarge model demonstrates better performance, and medium is a great option for developers who require fast response, like those building chatbots. You can find more information here . If you were previously using the command-xlarge-20221108 model, you will now be redirected to the command-xlarge-nightly model. Please note that access to the command-xlarge-20221108 model will be discontinued after January 30, 2023.*****### Looking back at the moments that shaped the first six months of C4AI Cohere For AI launched in June of this year as a non-profit research lab and community dedicated to contributing fundamental research in machine learning. The vision for the lab was simple but ambitious: change how, where, and by whom research is done. Since that launch, the C4AI team has been hard at work publishing and promoting fundamental ML research, growing our community, and creating programs designed to provide more entry points into ML research. Now that it's the end of the year, we think it's a good time to pause and reflect on everything we've done so far.*****### About Cohere For AI Cohere For AI launched in June of this year. We are a non-profit research lab and community dedicated to contributing fundamental research in machine learning, working to solve some of the field's most challenging problems. Our programs include supporting community-driven research across fundamental machine learning topics, our full-time research positions and lab, and our speaker series, which provides a forum for important and timely discussions on machine learning topics. We welcome anyone to apply to join our community and register for our upcoming speaker series .*****The best and brightest minds in machine learning transcend borders. That’s why we’re excited to announce Cohere For AI , a non-profit research lab and community dedicated to contributing fundamental research in machine learning, working to solve some of the field's most challenging problems. As part of our work, we're committed to fostering an inclusive and collaborative environment that creates more points of entry to participate in machine learning research. We’re excited to be able to revive the spirit of the original for.ai, an international research group founded by the two of us alongside Bryan Li and Sheldon Huang – just a few undergrads (+ a dropout) – as a way to contribute to machine learning research.*****## 3. Pick from amongst the top tokens whose probabilities add up to 15%: top-p The difficulty of selecting the best top-k value opens the door for a popular decoding strategy that dynamically sets the size of the shortlist of tokens. This method, called Nucleus Sampling , shortlists the top tokens whose sum of likelihoods does not exceed a certain value. A toy example with a top-p value of 0.15 could look like this: Top-p is usually set to a high value (like 0.75) with the purpose of limiting the long tail of low-probability tokens that may be sampled. We can use both top-k and top-p together.*****to democratize NLP for developers. We enable users to “finetune” these large language models to a specific classification task, such as intent recognition, for the best natural language understanding ability available. To demonstrate our performance on intent recognition even against other large language models, we used a public dataset called Banking77 that contains 13,000+ banking queries. Each query is labeled into 1 of 77 intents. Our Large model outperforms the best natural language processing (NLP) models in the world:*****The Cohere Platform CLI Tool is an alternative to our web interface, which allows you to login to your Cohere account, manage API Keys, and run finetunes. This CLI tool is POSIX compliant (you can expect arguments and flags to work the same as they do with other popular CLI tools). Don't forget to use co --help or co [COMMAND] --help if you don't want to check back to this page!*****The Cohere platform builds natural language understanding and generation into your product with a few lines of code. Our large language models can solve a broad spectrum of natural language use cases, including classification, semantic search, paraphrasing, summarization, and content generation. By training a custom model , users can customize large language models to their use case and trained on their data. The models can be accessed through the playground , SDK and the CLI tool.*****# Get Started with the CLI 📘 This page shows some common CLI Commands. For more info about a specific command, see that command's page or type co [command] --help .*****## Overview of the Generate Endpoint The Cohere platform can be accessed via the Playground, SDK, or the CLI tool. In this article, we’ll learn how to work with the Python SDK. With the API, you can access a number of endpoints, such as Generate, Embed, and Classify. Different endpoints are used for different purposes and produce different outputs. For example, you would use the Classify endpoint when you want to perform text classification. Our focus for this series is, of course, text generation, so we’ll work with the Generate endpoint.*****## Prerequisites To follow this tutorial, ensure you have the following: Node.js documentation npm install -g @angular/cli@14.1.3 logging in creating an account*****## Setting up The first step is to install the Cohere Python SDK. Next, sign up for a Cohere account and create an API key, which you can generate from one of the following interfaces: dashboard CLI tool Once that is done, you can set up the Cohere client as follows.*****Turns a product description into a list of functional, emotional, and social benefits. Ad Headline Generate promotional ad copy variants Concept Relationships A is to B as C is to D relationships Product Classifier Given a set of sample products, identify if it is a type of power tool, screw, or cement board Startup Idea Generator Come up with a startup problem and solution for an industry Tweet Classification Given a set of tweets, determine if the tweet is about a demo, a piece of news, or an event Spelling and Grammar Check Given a list of text, it will remove improper capitalization and transform the sentence to Capitalized Text*****# Download the Package for your OS Use the following curl command to download the correct package, or use a download link below to get a tar.*****early access to the Cohere Summarize – a new endpoint powered by our foundational language model, customized for text summarization. Summarize beta empowers users to efficiently distill crucial information from lengthy documents and articles. This functionality greatly increases productivity and saves valuable time, making it essential for anyone looking to extract the most essential information quickly and accurately. Gone are the days when users had to worry about crafting the perfect prompt or teaching a model how to generate a summary. With our endpoint, summarizing any article or text is as simple as inputting it into the system, which will automatically generate a summary for you.*****#### Summarize Summarize a given input text such as an article into a short paragraph or key bullet points Model Price per Unit Default $5.0 per 1000 Summarization Units* Custom $10.0 per 1000 Summarization Units* More details API Key Offering API Key Offering*****## Reading billions of words, to write the ones you need. The Generate API is trained on vast amounts of text spanning all topics and industries. With Generate, you ‘instruct’ the model with your specific text generation ask. This could be a copywriting task, named entity recognition, or even paraphrasing or summarization.*****We make API calls for the two models through three temperature values (0.0, 0.5, and 1.0) and three generations each, and here are the responses. Baseline model: Custom model: With the baseline models, the output gets the correct response at lower temperature values, but as we start to bring it higher, it gets inconsistent. Whereas with the custom models, it gets the response correct even at higher temperatures. This indicates that the custom model option has greater predictability and can produce quality outputs consistently, something that’s much needed when deploying applications out there.*****#### What's possible with Generate Write ads and descriptions faster Use Generate to perform time-consuming and repetitive copywriting tasks, like product descriptions or email responses. Paraphrase sentences and paragraphs Generate allows you to re-word text to suit a specific reader, or reformat existing content into unique pieces. Make the world bite-sized Use Generate to automatically condense key information from texts into digestible summaries. Find what you're looking for Use our models to identify and extract specific data defined by your unique business needs.*****### Summarize Large language models present a breakthrough in text generation. For the first time in history, we have software programs that can write text that sounds like it’s written by humans. These capabilities open doors to use cases like summarization or paraphrasing. A summarization prompt in the Cohere playground shows this output (in bold): Large language models can be adapted to new tasks with impressive speed. For tasks which appear in the training data (i.e. documents on the web), language models can successfully summarize text without being shown any examples at all. Summarization and paraphrasing both use the generate endpoint.*****That same passion lives on through Cohere For AI, with an even more focused team and mission. We’re thrilled to welcome Sara Hooker to lead this effort as Head of Cohere For AI. With a long track-record of impactful research at Google Brain, she brings a wealth of knowledge from across machine learning. She also founded Delta Analytics, a non-profit that brings together researchers, data scientists, and software engineers to volunteer their skills for non-profits around the world. Throughout her career, Sara has led as a researcher, an educator, and a community builder – it’s such a privilege to have her leading Cohere For AI.*****## Exploring the Unknown, Together The field of machine learning moves fast, and the research published by Cohere For AI and Cohere technical staff is contributing to that momentum. Led by the work of Sara Hooker, Head of Cohere For AI, our lab has published a range of cutting-edge ML research papers, including metadata archeology , NLP efficiencies , and compression on multilingual models . This research is the result of collaborations across 20+ institutions and organizations, including the University of Toronto, the University of Waterloo, University College London, Cambridge University, and Google Research, to name a few! 2022 also saw Cohere For AI join Cohere’s technical staff in presenting research at several major ML conferences, including NAACL, ICML, Deep Learning Indaba, RIIAA, NeurIPS, and EMNLP.*****The best and brightest minds in machine learning transcend borders. That’s why we’re excited to announce Cohere For AI , a non-profit research lab and community dedicated to contributing fundamental research in machine learning, working to solve some of the field's most challenging problems. As part of our work, we're committed to fostering an inclusive and collaborative environment that creates more points of entry to participate in machine learning research. We’re excited to be able to revive the spirit of the original for.ai, an international research group founded by the two of us alongside Bryan Li and Sheldon Huang – just a few undergrads (+ a dropout) – as a way to contribute to machine learning research.*****The machine learning field currently has too few points of entry, especially depending on where you are in the world. With Cohere For AI, Sara’s vision is to change how, where and by whom research is done. Cohere For AI represents the opportunity to make an impact in ways that don’t just advance progress on machine learning research, but also create new points of entry into the field. Our lab will focus on solving today’s complex machine learning challenges––exploring the unknown, together. Our values will be based on the following principles: GitHub Cohere For AI is community-driven and motivated by the opportunity to create an inclusive, distributed community made up of brilliant researchers and engineers from across the globe.*****: Yeah, there's frustration there. We were promised that AI will change the world, and I’m just not seeing it. I’m a consumer too — I use all the same products that everyone else uses. I know the technology, I know what it’s capable of, but it’s not out there. For Cohere, the mission is to push it out further. The way we’re doing that is by trying to lower barriers. One of the largest barriers that I’m sure a lot of people are aware of is that the people who know how to do this stuff — MLEs or machine learning engineers — we can’t train enough of them.*****“We’re excited to offer Cohere’s general purpose large language model with Amazon SageMaker. Our joint customers can now leverage the broad range of Amazon SageMaker services and integrate Cohere’s model with their applications for accelerated time-to-value and faster innovation.” “As Cohere continues to push the boundaries of language AI, we are excited to join forces with Amazon SageMaker,” said Saurabh Baji, Senior Vice President of Engineering at Cohere. “This partnership will allow us to bring our advanced technology and innovative approach to an even wider audience, empowering developers and organizations around the world to harness the power of language AI and stay ahead of the curve in an increasingly competitive market."*****Cohere For AI is a research lab that seeks to solve complex machine learning problems. We believe the best minds transcend borders and that discoveries are often made off the beaten path. Today, I am excited to announce the launch of our Cohere For AI Scholars Program, designed to help change where, how, and by whom research is done. Progress in machine learning is moving at an incredible pace, and broadening access to participation in fundamental research is essential to pioneering new advancements. Still, there are very few settings to conduct research on cutting-edge NLP problems, and limited access to large-scale ML experimental settings.*****Research communities aren’t built overnight, and we’re excited to welcome participation and contributions from brilliant minds, regardless of where they are. To learn how to get involved, check out open research positions at jobs.lever.co/cohere , and visit Cohere For AI . You can also follow us on Twitter at @forai_ml . We can’t wait to explore together. Aidan and Ivan*****## Partnering with Cohere Quality is a theme that permeates HyperWrite’s experience with Cohere. The company tried working with a few managed language AI solutions before finally settling on Cohere for the long term. The deciding factors: accuracy, affordability, and great support. Shumer said, “Cohere’s models are amazing. The company excels at delivering cost-effective, low-latency models that are high-quality, which enables us to build some of our most difficult-to-implement features that can deliver excellent performance.” During development, the Cohere team provided valuable technical support that helped the HyperWrite team achieve their goals. When they experienced problems or needed help figuring out a use case, Cohere expertise was right there for them.*****At Cohere, every employee has 30 days off per year.