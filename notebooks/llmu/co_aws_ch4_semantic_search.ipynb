{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/cohere-ai/notebooks/blob/main/notebooks/llmu/co_aws_ch4_semantic_search.ipynb\"> <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Search Using Cohere Embed on Amazon Bedrock"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text embeddings are numerical representations created by language models that convert text into vectors. They capture and encode the context of a document. These vectors store a wealth of context about the documents they represent, opening up the possibility of a variety of applications, from semantic search and retrieval-augmented generation (RAG) to topic modeling and text classification.\n",
    "\n",
    "Cohere's Embed model, available on Amazon Bedrock, is a powerful text embeddings model that offers these capabilities. This model supports over 100 languages and is unique among text embedding models due to its emphasis on document quality for applications like semantic search.\n",
    "\n",
    "In this notebook, we'll explore how to use Cohere's Embed model on Amazon Bedrock. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's install and import the necessary libraries and set up our Cohere client using the cohere SDK. To use Bedrock, we create a BedrockClient by passing the necessary AWS credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install cohere pandas hnswlib -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import cohere\n",
    "import hnswlib\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cohere\n",
    "\n",
    "# Create Bedrock client via the native Cohere SDK\n",
    "# Contact your AWS administrator for the credentials\n",
    "co = cohere.BedrockClient(\n",
    "    aws_region=\"YOUR_AWS_REGION\",\n",
    "    aws_access_key=\"YOUR_AWS_ACCESS_KEY_ID\",\n",
    "    aws_secret_key=\"YOUR_AWS_SECRET_ACCESS_KEY\",\n",
    "    aws_session_token=\"YOUR_AWS_SESSION_TOKEN\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a dataset (MultiFIN) containing a list of real-world article headlines covering 15 languages (English, Turkish, Danish, Spanish, Polish, Greek, Finnish, Hebrew, Japanese, Hungarian, Norwegian, Russian, Italian, Icelandic, and Swedish). This is an open-source dataset curated for financial natural language processing (NLP) and is available on a [GitHub repository](https://github.com/RasmusKaer/MultiFin).\n",
    "\n",
    "In our case, we’ve created a CSV file with MultiFIN’s data, as well as a column with translations. We don’t use this column to feed the model; we use it to help us follow along when we print the results for those who don’t speak Danish or Spanish. We point to that CSV to create our dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "      <th>lang</th>\n",
       "      <th>id</th>\n",
       "      <th>translation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Revenue Recognition</td>\n",
       "      <td>['Accounting &amp; Assurance']</td>\n",
       "      <td>English</td>\n",
       "      <td>Israel-4145</td>\n",
       "      <td>Revenue Recognition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Más de la mitad de las empresas españolas fuer...</td>\n",
       "      <td>['Financial Crime']</td>\n",
       "      <td>Spanish</td>\n",
       "      <td>Spain-2044</td>\n",
       "      <td>More than half of the Spanish companies were v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Wynagrodzenie netto w Polsce to średnio 71% pe...</td>\n",
       "      <td>['Human Resource']</td>\n",
       "      <td>Polish</td>\n",
       "      <td>Poland-1567</td>\n",
       "      <td>The net salary in Poland is an average of 71% ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Time to talk: What has to change for women at ...</td>\n",
       "      <td>['Human Resource']</td>\n",
       "      <td>English</td>\n",
       "      <td>Turkey-5447</td>\n",
       "      <td>Time to talk: What has to change for women at ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Total Retail 2017</td>\n",
       "      <td>['Retail &amp; Consumers']</td>\n",
       "      <td>English</td>\n",
       "      <td>Spain-1981</td>\n",
       "      <td>Total Retail 2017</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0                                Revenue Recognition   \n",
       "1  Más de la mitad de las empresas españolas fuer...   \n",
       "2  Wynagrodzenie netto w Polsce to średnio 71% pe...   \n",
       "3  Time to talk: What has to change for women at ...   \n",
       "4                                  Total Retail 2017   \n",
       "\n",
       "                       labels     lang           id  \\\n",
       "0  ['Accounting & Assurance']  English  Israel-4145   \n",
       "1         ['Financial Crime']  Spanish   Spain-2044   \n",
       "2          ['Human Resource']   Polish  Poland-1567   \n",
       "3          ['Human Resource']  English  Turkey-5447   \n",
       "4      ['Retail & Consumers']  English   Spain-1981   \n",
       "\n",
       "                                         translation  \n",
       "0                                Revenue Recognition  \n",
       "1  More than half of the Spanish companies were v...  \n",
       "2  The net salary in Poland is an average of 71% ...  \n",
       "3  Time to talk: What has to change for women at ...  \n",
       "4                                  Total Retail 2017  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"https://raw.githubusercontent.com/cohere-ai/cohere-aws/main/notebooks/bedrock/multiFIN_train.csv\"\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "# Inspect dataset\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Process Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MultiFIN has over 6,000 records in 15 different languages. For our example use case, we focus on three languages: English, Spanish, and Danish.\n",
    "For this, we’ll need to do some pre-processing steps. First we remove the duplicates, remove the languages other than the three we need, and pick the top 80 articles for demonstration purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lang\n",
       "Spanish    33\n",
       "English    29\n",
       "Danish     18\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ensure there is no duplicated text in the headers\n",
    "def remove_duplicates(text):\n",
    "    return re.sub(r'((\\b\\w+\\b.{1,2}\\w+\\b)+).+\\1', r'\\1', text, flags=re.I)\n",
    "\n",
    "df ['text'] = df['text'].apply(remove_duplicates)\n",
    "\n",
    "# Keep only selected languages\n",
    "languages = ['English', 'Spanish', 'Danish']\n",
    "df = df.loc[df['lang'].isin(languages)]\n",
    "\n",
    "# Pick the top 80 longest articles\n",
    "df['text_length'] = df['text'].str.len()\n",
    "df.sort_values(by=['text_length'], ascending=False, inplace=True)\n",
    "top_80_df = df[:80]\n",
    "\n",
    "# Language distribution\n",
    "top_80_df['lang'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embed and index documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we want to embed our documents and store the embeddings. The embeddings are very large vectors that encapsulate the semantic meaning of our document. In particular, we use Cohere’s `embed-multilingual-v3.0` model, which creates embeddings with 1,024 dimensions.\n",
    "\n",
    "With the v3.0 embeddings models, we need to specify the input_type parameter to indicate the nature of the document. In semantic search applications, this is either `search_document`, which is for the documents to search, or `search_query`, which is for the search query that we’ll define later.\n",
    "We also keep track of the language and translation of the document to enrich the display of the results.\n",
    "\n",
    "Next, we create a search index using the `hnsw` vector library. This stores the embeddings in an index, which makes searching the documents more efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed documents\n",
    "docs = top_80_df['text'].to_list()\n",
    "docs_lang = top_80_df['lang'].to_list()\n",
    "translated_docs = top_80_df['translation'].to_list() #for reference when returning non-English results\n",
    "doc_embs = co.embed(texts=docs,\n",
    "                    model=\"cohere.embed-multilingual-v3\",\n",
    "                    input_type='search_document').embeddings\n",
    "\n",
    "# Create a search index\n",
    "index = hnswlib.Index(space='ip', dim=1024)\n",
    "index.init_index(max_elements=len(doc_embs), ef_construction=512, M=64)\n",
    "index.add_items(doc_embs, list(range(len(doc_embs))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we build a function that takes a query as input, embeds it, and finds the four headers more closely related to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieval of 4 closest docs to query\n",
    "def retrieval(query):\n",
    "    # Embed query and retrieve results\n",
    "    query_emb = co.embed(texts=[query],\n",
    "                         model=\"cohere.embed-multilingual-v3\",\n",
    "                         input_type=\"search_query\").embeddings\n",
    "    doc_ids = index.knn_query(query_emb, k=3)[0][0] # we will retrieve 4 closest neighbors\n",
    "    \n",
    "    # Print and append results\n",
    "    print(f\"QUERY: {query.upper()} \\n\")\n",
    "    retrieved_docs, translated_retrieved_docs = [], []\n",
    "    \n",
    "    for doc_id in doc_ids:\n",
    "        # Append results\n",
    "        retrieved_docs.append(docs[doc_id])\n",
    "        translated_retrieved_docs.append(translated_docs[doc_id])\n",
    "    \n",
    "        # Print results\n",
    "        print(f\"ORIGINAL ({docs_lang[doc_id]}): {docs[doc_id]}\")\n",
    "        if docs_lang[doc_id] != \"English\":\n",
    "            print(f\"TRANSLATION: {translated_docs[doc_id]} \\n----\")\n",
    "        else:\n",
    "            print(\"----\")\n",
    "    print(\"END OF RESULTS \\n\\n\")\n",
    "    return retrieved_docs, translated_retrieved_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s now try to query the index with a couple of examples, one each in English and Danish.\n",
    "\n",
    "As for results from the English query, notice how the retrieval system was able to surface documents similar in meaning, i.e., data science vs. AI. This is something that keyword-based search systems would not be able to capture.\n",
    "\n",
    "As for results from the Danish query, it highlights the ability to perform cross-lingual search with the Embed multilingual model. You can enter a query in one language and get relevant search results that span other languages.\n",
    "Another observation here is that the English acronym “PP&E” stands for “property, plant, and equipment,” and the model was able to connect it to the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUERY: CAN DATA SCIENCE HELP MEET SUSTAINABILITY GOALS? \n",
      "\n",
      "ORIGINAL (English): Using AI to better manage the environment could reduce greenhouse gas emissions, boost global GDP by up to 38m jobs by 2030\n",
      "----\n",
      "ORIGINAL (English): Quality of business reporting on the Sustainable Development Goals improves, but has a long way to go to meet and drive targets.\n",
      "----\n",
      "ORIGINAL (English): Only 10 years to achieve Sustainable Development Goals but businesses remain on starting blocks for integration and progress\n",
      "----\n",
      "END OF RESULTS \n",
      "\n",
      "\n",
      "QUERY: HVOR KAN JEG FINDE DEN SENESTE DANSKE BOLIGPLAN? \n",
      "\n",
      "ORIGINAL (Danish): Nyt fra CFOdirect: Ny PP&E-guide, FAQs om den nye leasingstandard, podcast om udfordringerne ved implementering af leasingstandarden og meget mere\n",
      "TRANSLATION: New from CFOdirect: New PP&E guide, FAQs on the new leasing standard, podcast on the challenges of implementing the leasing standard and much more \n",
      "----\n",
      "ORIGINAL (Danish): Lovforslag fremlagt om rentefri lån, udskudt frist for lønsumsafgift, førtidig udbetaling af skattekredit og loft på indestående på skattekontoen\n",
      "TRANSLATION: Bills presented on interest -free loans, deferred deadline for payroll tax, early payment of tax credit and ceiling on the balance in the tax account \n",
      "----\n",
      "ORIGINAL (Danish): Nyt fra CFOdirect: Shareholder-spørgsmål til ledelsen, SEC cybersikkerhedsguide, den amerikanske skattereform og meget mere\n",
      "TRANSLATION: New from CFOdirect: Shareholder questions for management, the SEC cybersecurity guide, US tax reform and more \n",
      "----\n",
      "END OF RESULTS \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "queries = [\n",
    "    \"Can data science help meet sustainability goals?\", # English example\n",
    "    \"Hvor kan jeg finde den seneste danske boligplan?\" # Danish example - \"Where can I find the latest Danish property plan?\"\n",
    "]\n",
    "\n",
    "for query in queries:\n",
    "    retrieval(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Semantic search applications, enabled by text embeddings, offer a significantly more effective approach to retrieving and analyzing information. Cohere's Embed model can do this across over 100 languages. Its application in fields like financial analysis, as demonstrated in this chapter, shows how it can transform data retrieval and processing tasks, saving time and improving accuracy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
