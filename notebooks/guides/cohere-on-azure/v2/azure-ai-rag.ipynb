{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval Augmented Generation (RAG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Large Language Models (LLMs) excel at generating text and maintaining conversational context in chat applications. However, LLMs can sometimes hallucinate - producing responses that are factually incorrect. This is particularly important to mitigate in enterprise environments where organizations work with proprietary information that wasn't part of the model's training data.\n",
    "\n",
    "Retrieval-augmented generation (RAG) addresses this limitation by enabling LLMs to incorporate external knowledge sources into their response generation process. By grounding responses in retrieved facts, RAG significantly reduces hallucinations and improves the accuracy and reliability of the model's outputs.\n",
    "\n",
    "In this tutorial, we'll cover:\n",
    "- Setting up the Cohere client\n",
    "- Building a RAG application by combining retrieval and chat capabilities\n",
    "- Managing chat history and maintaining conversational context\n",
    "- Handling direct responses vs responses requiring retrieval\n",
    "- Generating citations for retrieved information\n",
    "\n",
    "In the next tutorial, we'll explore how to leverage Cohere's tool use features to build agentic applications.\n",
    "\n",
    "We'll use Cohere's Command, Embed, and Rerank models deployed on Azure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, you will need to deploy the Command, Embed, and Rerank models on Azure via Azure AI Foundry. The deployment will create a serverless API with pay-as-you-go token based billing. You can find more information on how to deploy models in the [Azure documentation](https://learn.microsoft.com/en-us/azure/ai-studio/how-to/deploy-models-serverless?tabs=azure-ai-studio).\n",
    "\n",
    "Once the model is deployed, you can access it via Cohere's Python SDK. Let's now install the Cohere SDK and set up our client.\n",
    "\n",
    "To create a client, you need to provide the API key and the model's base URL for the Azure endpoint. You can get these information from the Azure AI Foundry platform where you deployed the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install cohere hnswlib unstructured\n",
    "\n",
    "import cohere\n",
    "\n",
    "co_chat = cohere.ClientV2(\n",
    "    api_key=\"AZURE_API_KEY_CHAT\",\n",
    "    base_url=\"AZURE_ENDPOINT_CHAT\" # example: \"https://cohere-command-r-plus-08-2024-xyz.eastus.models.ai.azure.com/\"\n",
    ")\n",
    "\n",
    "co_embed = cohere.ClientV2(\n",
    "    api_key=\"AZURE_API_KEY_EMBED\",\n",
    "    base_url=\"AZURE_ENDPOINT_EMBED\" # example: \"https://cohere-embed-v3-multilingual-xyz.eastus.models.ai.azure.com/\"\n",
    ")\n",
    "\n",
    "co_rerank = cohere.ClientV2(\n",
    "    api_key=\"AZURE_API_KEY_RERANK\",\n",
    "    base_url=\"AZURE_ENDPOINT_RERANK\" # example: \"https://cohere-rerank-v3-multilingual-xyz.eastus.models.ai.azure.com/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A quick example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin with a simple example to explore how RAG works.\n",
    " \n",
    "The foundation of RAG is having a set of documents for the LLM to reference. Below, we'll work with a small collection of basic documents. While RAG systems usually involve retrieving relevant documents based on the user's query (which we'll explore later), for now we'll keep it simple and use this entire small set of documents as context for the LLM.\n",
    "\n",
    "We have seen how to use the Chat endpoint in the text generation chapter. To use the RAG feature, we simply need to add one additional parameter, `documents`, to the endpoint call. These are the documents we want to provide as the context for the model to use in its response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "    {\n",
    "        \"title\": \"Tall penguins\",\n",
    "        \"text\": \"Emperor penguins are the tallest.\"},\n",
    "    {\n",
    "        \"title\": \"Penguin habitats\",\n",
    "        \"text\": \"Emperor penguins only live in Antarctica.\"},\n",
    "    {\n",
    "        \"title\": \"What are animals?\",\n",
    "        \"text\": \"Animals are different from plants.\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how the model responds to the question \"What are the tallest living penguins?\"\n",
    "\n",
    "The model leverages the provided documents as context for its response. Specifically, when mentioning that Emperor penguins are the tallest species, it references `doc_0` - the document which states that \"Emperor penguins are the tallest.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RESPONSE:\n",
      "\n",
      "The tallest living penguins are the Emperor penguins. They only live in Antarctica.\n",
      "\n",
      "CITATIONS:\n",
      "\n",
      "start=36 end=53 text='Emperor penguins.' sources=[DocumentSource(type='document', id='doc:0', document={'id': 'doc:0', 'text': 'Emperor penguins are the tallest.', 'title': 'Tall penguins'})] type=None\n",
      "start=59 end=83 text='only live in Antarctica.' sources=[DocumentSource(type='document', id='doc:1', document={'id': 'doc:1', 'text': 'Emperor penguins only live in Antarctica.', 'title': 'Penguin habitats'})] type=None\n"
     ]
    }
   ],
   "source": [
    "message = \"What are the tallest living penguins?\"\n",
    "\n",
    "response = co_chat.chat(\n",
    "    model=\"model\", # Pass a dummy string\n",
    "    messages=[{\"role\": \"user\", \"content\": message}],\n",
    "    documents=[{\"data\": doc} for doc in documents]\n",
    ")\n",
    "\n",
    "print(\"\\nRESPONSE:\\n\")\n",
    "print(response.message.content[0].text)\n",
    "\n",
    "if response.message.citations:\n",
    "    print(\"\\nCITATIONS:\\n\") \n",
    "    for citation in response.message.citations:\n",
    "        print(citation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A more comprehensive example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we’ve covered a basic RAG implementation, let’s look at a more comprehensive example of RAG that includes:\n",
    "\n",
    "- Creating a retrieval system that converts documents into text embeddings and stores them in an index\n",
    "- Building a query generation system that transforms user messages into optimized search queries\n",
    "- Implementing a chat interface to handle LLM interactions with users\n",
    "- Designing a response generation system capable of handling various query types\n",
    "\n",
    "First, let’s import the necessary libraries for this project. This includes `hnswlib` for the vector library and `unstructured` for chunking the documents (more details on these later).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "import yaml\n",
    "import hnswlib\n",
    "from typing import List, Dict\n",
    "from unstructured.partition.html import partition_html\n",
    "from unstructured.chunking.title import chunk_by_title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we’ll define the documents we’ll use for RAG. We’ll use a few pages from the Cohere documentation that discuss prompt engineering. Each entry is identified by its title and URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_documents = [\n",
    "    {\n",
    "        \"title\": \"Crafting Effective Prompts\",\n",
    "        \"url\": \"https://docs.cohere.com/docs/crafting-effective-prompts\"},\n",
    "    {\n",
    "        \"title\": \"Advanced Prompt Engineering Techniques\",\n",
    "        \"url\": \"https://docs.cohere.com/docs/advanced-prompt-engineering-techniques\"},\n",
    "    {\n",
    "        \"title\": \"Prompt Truncation\",\n",
    "        \"url\": \"https://docs.cohere.com/docs/prompt-truncation\"},\n",
    "    {\n",
    "        \"title\": \"Preambles\",\n",
    "        \"url\": \"https://docs.cohere.com/docs/preambles\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create vectorstore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Vectorstore class handles the ingestion of documents into embeddings (or vectors) and the retrieval of relevant documents given a query.\n",
    "\n",
    "It includes a few methods:\n",
    "\n",
    "- `load_and_chunk`: Loads the raw documents from the URL and breaks them into smaller chunks\n",
    "- `embed`: Generates embeddings of the chunked documents\n",
    "- `index`: Indexes the document chunk embeddings to ensure efficient similarity search during retrieval\n",
    "- `retrieve`: Uses semantic search to retrieve relevant document chunks from the index, given a query. It involves two steps: first, dense retrieval from the index via the Embed endpoint, and second, a reranking via the Rerank endpoint to boost the search results further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vectorstore:\n",
    "\n",
    "    def __init__(self, raw_documents: List[Dict[str, str]]):\n",
    "        self.raw_documents = raw_documents\n",
    "        self.docs = []\n",
    "        self.docs_embs = []\n",
    "        self.retrieve_top_k = 10\n",
    "        self.rerank_top_k = 3\n",
    "        self.load_and_chunk()\n",
    "        self.embed()\n",
    "        self.index()\n",
    "\n",
    "\n",
    "    def load_and_chunk(self) -> None:\n",
    "        \"\"\"\n",
    "        Loads the text from the sources and chunks the HTML content.\n",
    "        \"\"\"\n",
    "        print(\"Loading documents...\")\n",
    "\n",
    "        for raw_document in self.raw_documents:\n",
    "            elements = partition_html(url=raw_document[\"url\"])\n",
    "            chunks = chunk_by_title(elements)\n",
    "            for chunk in chunks:\n",
    "                self.docs.append(\n",
    "                    {\n",
    "                        \"data\": {\n",
    "                            \"title\": raw_document[\"title\"],\n",
    "                            \"text\": str(chunk),\n",
    "                            \"url\": raw_document[\"url\"],\n",
    "                        }\n",
    "                    }\n",
    "                )\n",
    "\n",
    "    def embed(self) -> None:\n",
    "        \"\"\"\n",
    "        Embeds the document chunks using the Cohere API.\n",
    "        \"\"\"\n",
    "        print(\"Embedding document chunks...\")\n",
    "\n",
    "        batch_size = 90\n",
    "        self.docs_len = len(self.docs)\n",
    "        for i in range(0, self.docs_len, batch_size):\n",
    "            batch = self.docs[i : min(i + batch_size, self.docs_len)]\n",
    "            texts = [item[\"data\"][\"text\"] for item in batch]\n",
    "            docs_embs_batch = co_embed.embed(\n",
    "                texts=texts,\n",
    "                model=\"embed-multilingual-v3.0\",\n",
    "                input_type=\"search_document\",\n",
    "                embedding_types=[\"float\"]\n",
    "            ).embeddings.float\n",
    "            self.docs_embs.extend(docs_embs_batch)\n",
    "\n",
    "    def index(self) -> None:\n",
    "        \"\"\"\n",
    "        Indexes the document chunks for efficient retrieval.\n",
    "        \"\"\"\n",
    "        print(\"Indexing document chunks...\")\n",
    "\n",
    "        self.idx = hnswlib.Index(space=\"ip\", dim=1024)\n",
    "        self.idx.init_index(max_elements=self.docs_len, ef_construction=512, M=64)\n",
    "        self.idx.add_items(self.docs_embs, list(range(len(self.docs_embs))))\n",
    "\n",
    "        print(f\"Indexing complete with {self.idx.get_current_count()} document chunks.\")\n",
    "\n",
    "    def retrieve(self, query: str) -> List[Dict[str, str]]:\n",
    "        \"\"\"\n",
    "        Retrieves document chunks based on the given query.\n",
    "\n",
    "        Parameters:\n",
    "        query (str): The query to retrieve document chunks for.\n",
    "\n",
    "        Returns:\n",
    "        List[Dict[str, str]]: A list of dictionaries representing the retrieved document chunks, with 'title', 'text', and 'url' keys.\n",
    "        \"\"\"\n",
    "\n",
    "        # Dense retrieval\n",
    "        query_emb = co_embed.embed(\n",
    "            texts=[query],\n",
    "            model=\"embed-multilingual-v3.0\",\n",
    "            input_type=\"search_query\",\n",
    "            embedding_types=[\"float\"]\n",
    "        ).embeddings.float\n",
    "        \n",
    "        doc_ids = self.idx.knn_query(query_emb, k=self.retrieve_top_k)[0][0]\n",
    "\n",
    "        # Reranking\n",
    "        docs_to_rerank = [self.docs[doc_id][\"data\"] for doc_id in doc_ids]\n",
    "        yaml_docs = [yaml.dump(doc, sort_keys=False) for doc in docs_to_rerank] \n",
    "        rerank_results = co_rerank.rerank(\n",
    "            query=query,\n",
    "            documents=yaml_docs,\n",
    "            model=\"model\", # Pass a dummy string\n",
    "            top_n=self.rerank_top_k\n",
    "        )\n",
    "\n",
    "        doc_ids_reranked = [doc_ids[result.index] for result in rerank_results.results]\n",
    "\n",
    "        docs_retrieved = []\n",
    "        for doc_id in doc_ids_reranked:\n",
    "            docs_retrieved.append(self.docs[doc_id][\"data\"])\n",
    "            \n",
    "        return docs_retrieved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the Vectorstore set up, we can process the documents, which will involve chunking, embedding, and indexing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading documents...\n",
      "Embedding document chunks...\n",
      "Indexing document chunks...\n",
      "Indexing complete with 137 document chunks.\n"
     ]
    }
   ],
   "source": [
    "# Create an instance of the Vectorstore class with the given sources\n",
    "vectorstore = Vectorstore(raw_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can test if the retrieval is working by entering a search query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'Advanced Prompt Engineering Techniques',\n",
       "  'text': 'Few-shot Prompting\\n\\nUnlike the zero-shot examples above, few-shot prompting is a technique that provides a model with examples of the task being performed before asking the specific question to be answered. We can steer the LLM toward a high-quality solution by providing a few relevant and diverse examples in the prompt. Good examples condition the model to the expected response type and style.',\n",
       "  'url': 'https://docs.cohere.com/docs/advanced-prompt-engineering-techniques'},\n",
       " {'title': 'Crafting Effective Prompts',\n",
       "  'text': 'Incorporating Example Outputs\\n\\nLLMs respond well when they have specific examples to work from. For example, instead of asking for the salient points of the text and using bullet points “where appropriate”, give an example of what the output should look like.',\n",
       "  'url': 'https://docs.cohere.com/docs/crafting-effective-prompts'},\n",
       " {'title': 'Advanced Prompt Engineering Techniques',\n",
       "  'text': 'In addition to giving correct examples, including negative examples with a clear indication of why they are wrong can help the LLM learn to distinguish between correct and incorrect responses. Ordering the examples can also be important; if there are patterns that could be picked up on that are not relevant to the correctness of the question, the model may incorrectly pick up on those instead of the semantics of the question itself.',\n",
       "  'url': 'https://docs.cohere.com/docs/advanced-prompt-engineering-techniques'}]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorstore.retrieve(\"Prompting by giving examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now run the chatbot. For this, we create a `run_chatbot` function that accepts the user message and the `messages` list containing the conversation history, if available.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_chatbot(query, messages=None):\n",
    "    if messages is None:\n",
    "        messages = []\n",
    "\n",
    "    messages.append({\"role\": \"user\", \"content\": query})\n",
    "\n",
    "    # Retrieve document chunks and format\n",
    "    documents = vectorstore.retrieve(query)\n",
    "    documents_formatted = []\n",
    "    for doc in documents:\n",
    "        documents_formatted.append({\n",
    "            \"data\": doc\n",
    "        })\n",
    "\n",
    "    # Use document chunks to respond\n",
    "    response = co_chat.chat(\n",
    "        model=\"model\", # Pass a dummy string\n",
    "        messages=messages,\n",
    "        documents=documents_formatted\n",
    "    )\n",
    "        \n",
    "    # Print the chatbot response, citations, and documents\n",
    "    print(\"\\nRESPONSE:\\n\")\n",
    "    print(response.message.content[0].text)\n",
    "        \n",
    "    if response.message.citations:\n",
    "        print(\"\\nCITATIONS:\\n\")           \n",
    "        for citation in response.message.citations:\n",
    "            print(\"-\"*20)\n",
    "            print(\"start:\", citation.start, \"end:\", citation.end, \"text:\", citation.text)\n",
    "            print(\"SOURCES:\")\n",
    "            print(citation.sources)\n",
    "            \n",
    "    # Add assistant response to messages\n",
    "    messages.append({\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": response.message.content[0].text\n",
    "    })\n",
    "\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a sample conversation consisting of a few turns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RESPONSE:\n",
      "\n",
      "Hello! How can I help you today?\n"
     ]
    }
   ],
   "source": [
    "messages = run_chatbot(\"Hello, I have a question\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RESPONSE:\n",
      "\n",
      "There are a few ways to provide examples in prompts.\n",
      "\n",
      "One way is to provide a few relevant and diverse examples in the prompt. This can help steer the LLM towards a high-quality solution. Good examples condition the model to the expected response type and style.\n",
      "\n",
      "Another way is to provide specific examples to work from. For example, instead of asking for the salient points of the text and using bullet points “where appropriate”, give an example of what the output should look like.\n",
      "\n",
      "In addition to giving correct examples, including negative examples with a clear indication of why they are wrong can help the LLM learn to distinguish between correct and incorrect responses.\n",
      "\n",
      "CITATIONS:\n",
      "\n",
      "--------------------\n",
      "start: 68 end: 126 text: provide a few relevant and diverse examples in the prompt.\n",
      "SOURCES:\n",
      "[DocumentSource(type='document', id='doc:0', document={'id': 'doc:0', 'text': 'Few-shot Prompting\\n\\nUnlike the zero-shot examples above, few-shot prompting is a technique that provides a model with examples of the task being performed before asking the specific question to be answered. We can steer the LLM toward a high-quality solution by providing a few relevant and diverse examples in the prompt. Good examples condition the model to the expected response type and style.', 'title': 'Advanced Prompt Engineering Techniques', 'url': 'https://docs.cohere.com/docs/advanced-prompt-engineering-techniques'})]\n",
      "--------------------\n",
      "start: 136 end: 187 text: help steer the LLM towards a high-quality solution.\n",
      "SOURCES:\n",
      "[DocumentSource(type='document', id='doc:0', document={'id': 'doc:0', 'text': 'Few-shot Prompting\\n\\nUnlike the zero-shot examples above, few-shot prompting is a technique that provides a model with examples of the task being performed before asking the specific question to be answered. We can steer the LLM toward a high-quality solution by providing a few relevant and diverse examples in the prompt. Good examples condition the model to the expected response type and style.', 'title': 'Advanced Prompt Engineering Techniques', 'url': 'https://docs.cohere.com/docs/advanced-prompt-engineering-techniques'})]\n",
      "--------------------\n",
      "start: 188 end: 262 text: Good examples condition the model to the expected response type and style.\n",
      "SOURCES:\n",
      "[DocumentSource(type='document', id='doc:0', document={'id': 'doc:0', 'text': 'Few-shot Prompting\\n\\nUnlike the zero-shot examples above, few-shot prompting is a technique that provides a model with examples of the task being performed before asking the specific question to be answered. We can steer the LLM toward a high-quality solution by providing a few relevant and diverse examples in the prompt. Good examples condition the model to the expected response type and style.', 'title': 'Advanced Prompt Engineering Techniques', 'url': 'https://docs.cohere.com/docs/advanced-prompt-engineering-techniques'})]\n",
      "--------------------\n",
      "start: 282 end: 321 text: provide specific examples to work from.\n",
      "SOURCES:\n",
      "[DocumentSource(type='document', id='doc:1', document={'id': 'doc:1', 'text': 'Incorporating Example Outputs\\n\\nLLMs respond well when they have specific examples to work from. For example, instead of asking for the salient points of the text and using bullet points “where appropriate”, give an example of what the output should look like.', 'title': 'Crafting Effective Prompts', 'url': 'https://docs.cohere.com/docs/crafting-effective-prompts'})]\n",
      "--------------------\n",
      "start: 335 end: 485 text: instead of asking for the salient points of the text and using bullet points “where appropriate”, give an example of what the output should look like.\n",
      "SOURCES:\n",
      "[DocumentSource(type='document', id='doc:1', document={'id': 'doc:1', 'text': 'Incorporating Example Outputs\\n\\nLLMs respond well when they have specific examples to work from. For example, instead of asking for the salient points of the text and using bullet points “where appropriate”, give an example of what the output should look like.', 'title': 'Crafting Effective Prompts', 'url': 'https://docs.cohere.com/docs/crafting-effective-prompts'})]\n",
      "--------------------\n",
      "start: 527 end: 679 text: including negative examples with a clear indication of why they are wrong can help the LLM learn to distinguish between correct and incorrect responses.\n",
      "SOURCES:\n",
      "[DocumentSource(type='document', id='doc:2', document={'id': 'doc:2', 'text': 'In addition to giving correct examples, including negative examples with a clear indication of why they are wrong can help the LLM learn to distinguish between correct and incorrect responses. Ordering the examples can also be important; if there are patterns that could be picked up on that are not relevant to the correctness of the question, the model may incorrectly pick up on those instead of the semantics of the question itself.', 'title': 'Advanced Prompt Engineering Techniques', 'url': 'https://docs.cohere.com/docs/advanced-prompt-engineering-techniques'})]\n"
     ]
    }
   ],
   "source": [
    "messages = run_chatbot(\"How to provide examples in prompts\", messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RESPONSE:\n",
      "\n",
      "I'm sorry, I could not find any information about 5G networks.\n"
     ]
    }
   ],
   "source": [
    "messages = run_chatbot(\"What do you know about 5G networks?\", messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'role': 'user', 'content': 'Hello, I have a question'} \n",
      "\n",
      "{'role': 'assistant', 'content': 'Hello! How can I help you today?'} \n",
      "\n",
      "{'role': 'user', 'content': 'How to provide examples in prompts'} \n",
      "\n",
      "{'role': 'assistant', 'content': 'There are a few ways to provide examples in prompts.\\n\\nOne way is to provide a few relevant and diverse examples in the prompt. This can help steer the LLM towards a high-quality solution. Good examples condition the model to the expected response type and style.\\n\\nAnother way is to provide specific examples to work from. For example, instead of asking for the salient points of the text and using bullet points “where appropriate”, give an example of what the output should look like.\\n\\nIn addition to giving correct examples, including negative examples with a clear indication of why they are wrong can help the LLM learn to distinguish between correct and incorrect responses.'} \n",
      "\n",
      "{'role': 'user', 'content': 'What do you know about 5G networks?'} \n",
      "\n",
      "{'role': 'assistant', 'content': \"I'm sorry, I could not find any information about 5G networks.\"} \n",
      "\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "for message in messages:\n",
    "    print(message, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few observations worth pointing out:\n",
    "\n",
    "- Direct response: For user messages that don’t require retrieval (“Hello, I have a question”), the chatbot responds directly without requiring retrieval.\n",
    "- Citation generation: For responses that do require retrieval (\"What's the difference between zero-shot and few-shot prompting\"), the endpoint returns the response together with the citations. These are fine-grained citations, which means they refer to specific spans of the generated text.\n",
    "- Response synthesis: The model can decide if none of the retrieved documents provide the necessary information to answer a user message. For example, when asked the question, “What do you know about 5G networks”, the chatbot retrieves external information from the index. However, it doesn’t use any of the information in its response as none of it is relevant to the question.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we learned about:\n",
    "- How to set up the Cohere client to use the Command model deployed on Azure AI Foundry for chat\n",
    "- How to build a RAG application by combining retrieval and chat capabilities\n",
    "- How to manage chat history and maintain conversational context\n",
    "- How to handle direct responses vs responses requiring retrieval\n",
    "- How citations are automatically generated for retrieved information\n",
    "\n",
    "In the next tutorial, we'll explore how to leverage Cohere's tool use features to build agentic applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
