{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_q-7fEZxmIHm"
      },
      "source": [
        "Large Language Models (LLMs) are becoming increasingly capable of comprehending text, among others excelling in document analysis. The new Cohere model, [Command-R](https://huggingface.co/CohereForAI/c4ai-command-r-v01), boasts a context length of 128k, which makes it particularly effective for such tasks. Nevertheless, even with the extended context window, some documents might be too lengthy to accommodate in full.\n",
        "\n",
        "In this cookbook, we'll explore techniques to address cases when relevant information doesn't fit in the model context window.\n",
        "\n",
        "We'll show you three potential mitigation strategies: truncating the document, query-based retrieval, and a \"text rank\" approach we use internally at Cohere.\n",
        "\n",
        "### Table of content:\n",
        "1. [Getting started](#getting-started)\n",
        "2. [Approach 1: Truncate](#truncate)\n",
        "3. [Approach 2: Query Based Retrieval](#query-based-retrieval)\n",
        "4. [Approach 3: Text Rank](#text-rank)\n",
        "\n",
        "### Summary\n",
        "\n",
        "| Approach     | Description               | Pros                                      | Cons                       | When to use?                       |\n",
        "|-----------------------|-------------------------------------------|-------------------------------------------|-------------------------------------------|-------------------------------------------|\n",
        "| Truncation            | Truncate the document to fit the context window.                    | - Simplicity of implementation<br>(does not rely on extrenal infrastructure)| - Loses information at the end of the document | Utilize when all relevant information is contained<br> at the beginning of the document. |\n",
        "| Query Based Retrieval| Utilize semantic similarity to retrieve text chunks<br> that are most relevant to the query. | - Focuses on sections directly relevant to<br> the query | - Relies on a semantic similarity algorithm.<br>- Might lose broader context | Employ when seeking specific<br> information within the text. |\n",
        "| Text Rank             | Apply graph theory to generate a cohesive set<br> of chunks that effectively represent the document. | - Preserves the broader picture.          | - Might lose detailed information. | Utilize in summaries and when the question<br> requires broader context. |\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lwgm08jzah5c"
      },
      "source": [
        "<a id=\"getting-started\"></a>\n",
        "<a name=\"getting-started\"></a>\n",
        "# Getting Started"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "wuQ1PO8FadQf",
        "outputId": "c6cc36f7-6368-4cdc-ce57-4d95249e409f"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install cohere\n",
        "!pip install python-dotenv\n",
        "!pip install tokenizers\n",
        "!pip install langchain\n",
        "!pip install nltk\n",
        "!pip install networkx\n",
        "!pip install pypdf2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UFi1QH3sZUEZ",
        "outputId": "3e72afe2-2c60-472e-b838-949676897e13"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /Users/meor/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import requests\n",
        "from collections import deque\n",
        "from typing import List, Tuple\n",
        "\n",
        "import cohere\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import PyPDF2\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "from tokenizers import Tokenizer\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')  # Download the necessary data for sentence tokenization\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "import networkx as nx\n",
        "from getpass import getpass\n",
        "from IPython.display import HTML, display"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "6BtV3iJ_UQ2w",
        "outputId": "76303020-4c08-4f5e-aa5a-03762686455d"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "def set_css():\n",
        "  display(HTML('''\n",
        "  <style>\n",
        "    pre {\n",
        "        white-space: pre-wrap;\n",
        "    }\n",
        "  </style>\n",
        "  '''))\n",
        "get_ipython().events.register('pre_run_cell', set_css)\n",
        "\n",
        "set_css()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K603hzyKda2f",
        "outputId": "611374e9-1e51-4012-c99c-9ecb17a17e19"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Set up Cohere client\n",
        "co_model = 'command-a-03-2025'\n",
        "co_api_key = getpass(\"Enter your Cohere API key: \")\n",
        "co = cohere.Client(api_key=co_api_key)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "kI0inogLmIHp"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "def load_long_pdf(file_path):\n",
        "    \"\"\"\n",
        "    Load a long PDF file and extract its text content.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): The path to the PDF file.\n",
        "\n",
        "    Returns:\n",
        "        str: The extracted text content of the PDF file.\n",
        "    \"\"\"\n",
        "    with open(file_path, 'rb') as file:\n",
        "        pdf_reader = PyPDF2.PdfReader(file)\n",
        "        num_pages = len(pdf_reader.pages)\n",
        "        full_text = ''\n",
        "        for page_num in range(num_pages):\n",
        "            page = pdf_reader.pages[page_num]\n",
        "            full_text += page.extract_text()\n",
        "    return full_text\n",
        "\n",
        "def save_pdf_from_url(pdf_url, save_path):\n",
        "    try:\n",
        "        # Send a GET request to the PDF URL\n",
        "        response = requests.get(pdf_url, stream=True)\n",
        "        response.raise_for_status()  # Raise an exception for HTTP errors\n",
        "\n",
        "        # Open the local file for writing in binary mode\n",
        "        with open(save_path, 'wb') as file:\n",
        "            # Write the content of the response to the local file\n",
        "            for chunk in response.iter_content(chunk_size=8192):\n",
        "                file.write(chunk)\n",
        "\n",
        "        print(f\"PDF saved successfully to '{save_path}'\")\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error downloading PDF: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lw1ttnX_VM7N"
      },
      "source": [
        "In this example we use the Proposal for a Regulation of the European Parliament and of the Council defining rules on Artificial Intelligence from 26 January 2024, [link](https://data.consilium.europa.eu/doc/document/ST-5662-2024-INIT/en/pdf)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "4JgYYTg7Shvq",
        "outputId": "bc24d5a7-8bde-4d2a-e569-8a6941612629"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PDF saved successfully to 'example.pdf'\n",
            "Document length - #tokens: 134184\n"
          ]
        }
      ],
      "source": [
        "# Download the PDF file from the URL\n",
        "pdf_url = \"https://data.consilium.europa.eu/doc/document/ST-5662-2024-INIT/en/pdf\"\n",
        "save_path = \"example.pdf\"\n",
        "save_pdf_from_url(pdf_url, save_path)\n",
        "\n",
        "# Load the PDF file and extract its text content\n",
        "long_text = load_long_pdf(save_path)\n",
        "long_text = long_text.replace(\"\\n\", \" \")\n",
        "\n",
        "# Print the length of the document\n",
        "print(\n",
        "    \"Document length - #tokens:\",\n",
        "    len(co.tokenize(\n",
        "        text=long_text, \n",
        "        model=\"command-a-03-2025\"\n",
        "        ).tokens\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wu9MjCfUmIHp"
      },
      "source": [
        "## Summarizing the text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "PAa9xaU6-8ls",
        "outputId": "7e60436a-d8a2-42ec-df85-6ae3402bc106"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "def generate_response(message, max_tokens=300, temperature=0.2, k=0):\n",
        "  \"\"\"\n",
        "  A wrapper around the Cohere API to generate a response based on a given prompt.\n",
        "\n",
        "  Args:\n",
        "    messsage (str): The input message for generating the response.\n",
        "    max_tokens (int, optional): The maximum number of tokens in the generated response. Defaults to 300.\n",
        "    temperature (float, optional): Controls the randomness of the generated response. Higher values (e.g., 1.0) make the output more random, while lower values (e.g., 0.2) make it more deterministic. Defaults to 0.2.\n",
        "    k (int, optional): Controls the diversity of the generated response. Higher values (e.g., 5) make the output more diverse, while lower values (e.g., 0) make it more focused. Defaults to 0.\n",
        "\n",
        "  Returns:\n",
        "    str: The generated response.\n",
        "\n",
        "  \"\"\"\n",
        "  response = co.chat(\n",
        "    model = co_model,\n",
        "    message=message,\n",
        "    max_tokens=max_tokens,\n",
        "    temperature=temperature,\n",
        "    return_prompt=True\n",
        "    )\n",
        "  return response.text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "80fgeyAhZDZD",
        "outputId": "3a756564-c6d9-44d6-ac97-ffb8c33d09f6"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Example summary prompt.\n",
        "prompt_template = \"\"\"\n",
        "## Instruction\n",
        "Summarize the following Document in 3-5 sentences. Only answer based on the information provided in the document.\n",
        "\n",
        "## Document\n",
        "{document}\n",
        "\n",
        "## Summary\n",
        "\"\"\".strip()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tq9Di1gImIHq"
      },
      "source": [
        "If you run the cell below, an error will occur. Therefore, in the following sections, we will explore some techniques to address this limitation.\n",
        "\n",
        "Error: :`CohereAPIError: too many tokens:`\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "TECMEd6rdYN_"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "prompt = prompt_template.format(document=long_text)\n",
        "# print(generate_response(message=prompt))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FLwHEBQojIXX"
      },
      "source": [
        "Therefore, in the following sections, we will explore some techniques to address this limitation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AH-tKADHmIHq"
      },
      "source": [
        "<a name=\"truncate\"></a>\n",
        "# Approach 1 - Truncate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OGxtSKUEmIHq"
      },
      "source": [
        "First we try to truncate the document so that it meets the length constraints. This approach is simple to implement and understand. However, it drops potentially important information contained towards the end of the document."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "JrvVls3smIHq",
        "outputId": "6ec69e1b-0d6d-4b96-e8ae-650ebc049a7a"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# The new Cohere model has a context limit of 128k tokens. However, for the purpose of this exercise, we will assume a smaller context window.\n",
        "# Employing a smaller context window also has the additional benefit of reducing the cost per request, especially if billed by the number of tokens.\n",
        "\n",
        "MAX_TOKENS = 40000\n",
        "\n",
        "def truncate(long: str, max_tokens: int) -> str:\n",
        "    \"\"\"\n",
        "    Shortens `long` by brutally truncating it to the first `max_tokens` tokens.\n",
        "    This can break up sentences, passages, etc.\n",
        "    \"\"\"\n",
        "    tokenized = co.tokenize(\n",
        "                    text=long, \n",
        "                    model=\"command-a-03-2025\"\n",
        "                ).tokens\n",
        "    truncated = co.detokenize(tokens=tokenized[:max_tokens], model=\"command-a-03-2025\").text\n",
        "    short = \"\".join(truncated)\n",
        "    return short"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "id": "bxBOLcMomIHq",
        "outputId": "0273fe8b-814b-4d5b-8da5-be85bfa9ea45"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The document outlines the European Union's proposed Artificial Intelligence Act (AI Act), which aims to establish harmonized rules for AI systems, ensuring a high level of protection for health, safety, and fundamental rights while fostering innovation. The Act defines AI systems, prohibits certain practices like manipulative techniques and biometric categorization, and classifies high-risk AI systems based on their potential harm. It mandates transparency, risk management, and human oversight for high-risk systems, with specific requirements for data quality, technical documentation, and cybersecurity. The Act also addresses governance, including the establishment of an AI Office and an AI Board, and sets penalties for non-compliance. Additionally, it introduces provisions for general-purpose AI models, requiring transparency, copyright compliance, and risk mitigation for those with systemic risks. The legislation aims to balance innovation with ethical and safety considerations, ensuring AI systems align with EU values and international standards.\n"
          ]
        }
      ],
      "source": [
        "short_text = truncate(long_text, MAX_TOKENS)\n",
        "\n",
        "prompt = prompt_template.format(document=short_text)\n",
        "print(generate_response(message=prompt))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pC1l77odmIHr"
      },
      "source": [
        "<a name=\"query-based-retrieval\"></a>\n",
        "# Approach 2: Query Based Retrieval\n",
        "\n",
        "In this section we present how we can leverage a query retriereval based approach to generate an answer to the following question: `Based on the document, are there any risks related to Elon Musk?`.\n",
        "\n",
        "The solution is outlined below and can be broken down into four functional steps.\n",
        "\n",
        "1. Chunk the text into units\n",
        "    - Here we employ a simple chunking algorithm. More information about different chunking strategies can be found [here](TODO: link to chunking post).\n",
        "\n",
        "2. Use a ranking algorithm to rank chunks against the query\n",
        "    - We leverage another Cohere endpoint, `co.rerank` ([docs link](https://docs.cohere.com/reference/rerank-1)), to rank each chunk against the query.\n",
        "\n",
        "3. Keep the most-relevant chunks until context limit is reached\n",
        "    - `co.rerank` returns a relevance score, facilitating the selection of the most pertinent chunks. We can choose the most relevant chunks based on this score.\n",
        "\n",
        "4. Put condensed text back in original order\n",
        "    - Finally, we arrange the chosen chunks in their original sequence as they appear in the document.\n",
        "\n",
        "See `query_based_retrieval` function for the starting point.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rHwHtYSPjYKH"
      },
      "source": [
        "### Query based retrieval implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "XIbGWw2GmIHr",
        "outputId": "9ca56d31-d7b4-4e48-e9d2-2279f9d2caaf"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "############################################################\n",
        "#\n",
        "# Utility functions for chunking\n",
        "#\n",
        "############################################################\n",
        "def split_text_into_sentences(text) -> List[str]:\n",
        "    \"\"\"\n",
        "    Split the input text into a list of sentences.\n",
        "    \"\"\"\n",
        "    sentences = sent_tokenize(text)\n",
        "\n",
        "    return sentences\n",
        "\n",
        "def group_sentences_into_passages(sentence_list, n_sentences_per_passage=5):\n",
        "    \"\"\"\n",
        "    Group sentences into passages of n_sentences sentences.\n",
        "    \"\"\"\n",
        "    passages = []\n",
        "    passage = \"\"\n",
        "    for i, sentence in enumerate(sentence_list):\n",
        "        passage += sentence + \" \"\n",
        "        if (i + 1) % n_sentences_per_passage == 0:\n",
        "            passages.append(passage)\n",
        "            passage = \"\"\n",
        "    return passages\n",
        "\n",
        "def build_simple_chunks(text, n_sentences=5):\n",
        "    \"\"\"\n",
        "    Build chunks of text from the input text.\n",
        "    \"\"\"\n",
        "    sentences = split_text_into_sentences(text)\n",
        "    chunks = group_sentences_into_passages(sentences, n_sentences_per_passage=n_sentences)\n",
        "    return chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 160
        },
        "id": "gKS7Mo6pmIHr",
        "outputId": "ae67c468-b5d3-43a4-da36-83a2dc16956a"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Example sentence: ['Where appropriate, representative s of the advisory  forum as referred to in Article 58a may be invited to such sub - groups or to specific  meetings of those subgroups in the capacity of observers.']\n",
            "\n",
            "Example passage: ['CONCLUSION         5662/24       RB/ek   10     TREE.2.B   LIMITE   EN     1. The Presidency invites the Committee of the Permanent Representat ives to:    a.   endorse the annexed compromise text as agreed with the European Parliament during  the final trilogue, and   b.   mandate the Presidency to inform the European Parliament that, should the European  Parliament adopt its position at first reading, in  accordance with Article 294  paragraph 3 of the Treaty, in the form set out in the compromise package contained  in the Annex to this document (subject to revision by the lawyer linguists of both  institutions), the Council would, in accordance with Article 2 94, paragraph 4 of the  Treaty, approve the European Parliament’s position and the act shall be adopted in  the wording which corresponds to the European Parliament’s position. _______________________           5662/24       RB/ek   11     TREE.2.B   LIMITE   EN     ANNEX     2021/0106 (COD)   Proposal for a   REGULATION OF TH E EUROPEAN PARLIAMENT AND OF THE COUNCIL   LAYING DOWN HARMONISED RULES ON ARTIFICIAL INTELLIGENCE  (ARTIFICIAL INTELLIGENCE ACT) AND AMENDING CERTAIN UNION  LEGISLATIVE ACTS   THE EUROPEAN PARLIAMENT AND THE COUNCIL OF THE EUROPEAN UNION,   Having regard to the T reaty on the Functioning of the European Union, and in particular Articles 16  and 114 thereof,   Having regard to the proposal from the European Commission,   After transmission of the draft legislative act to the national parliaments,   Having regard to the opi nion of the European Economic and Social Committee 1 ,   Having regard to the opinion of the European Central Bank 2 ,   Having regard to the joint opinion of the European Data Protection Board and the European Data  Protection Supervisor,   Having regard to the  opinion of the Committee of the Regions 3 ,   Acting in accordance with the ordinary legislative procedure,   Whereas:   (1)   The purpose of this Regulation is to improve the functioning of the internal market by  laying down a uniform legal framework in particular  for the development, placing on the                                                      1   OJ C […], […], p. […]. 2   Reference to ECB opinion . 3   OJ C […], […], p. […]. ']\n"
          ]
        }
      ],
      "source": [
        "sentences = split_text_into_sentences(long_text)\n",
        "passages = group_sentences_into_passages(sentences, n_sentences_per_passage=5)\n",
        "print('Example sentence:', np.random.choice(np.asarray(sentences), size=1, replace=False))\n",
        "print()\n",
        "print('Example passage:', np.random.choice(np.asarray(passages), size=1, replace=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "w6pjBAA9mIHr",
        "outputId": "ab3f3ae1-3b38-4999-f8f7-1259d070c88b"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "def _add_chunks_by_priority(\n",
        "    chunks: List[str],\n",
        "    idcs_sorted_by_priority: List[int],\n",
        "    max_tokens: int,\n",
        ") -> List[Tuple[int, str]]:\n",
        "    \"\"\"\n",
        "    Given chunks of text and their indices sorted by priority (highest priority first), this function\n",
        "    fills the model context window with as many highest-priority chunks as possible.\n",
        "\n",
        "    The output is a list of (index, chunk) pairs, ordered by priority. To stitch back the chunks into\n",
        "    a cohesive text that preserves chronological order, sort the output on its index.\n",
        "    \"\"\"\n",
        "\n",
        "    selected = []\n",
        "    num_tokens = 0\n",
        "    idcs_queue = deque(idcs_sorted_by_priority)\n",
        "\n",
        "    while num_tokens < max_tokens and len(idcs_queue) > 0:\n",
        "        next_idx = idcs_queue.popleft()\n",
        "        num_tokens += len(co.tokenize(text=chunks[next_idx], model=\"command-a-03-2025\").tokens)\n",
        "        # keep index and chunk, to reorder chronologically\n",
        "        selected.append((next_idx, chunks[next_idx]))\n",
        "    if num_tokens > max_tokens:\n",
        "        selected.pop()\n",
        "\n",
        "    return selected\n",
        "\n",
        "def query_based_retrieval(\n",
        "    long: str,\n",
        "    max_tokens: int,\n",
        "    query: str,\n",
        "    n_setences_per_passage: int = 5,\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Performs query-based retrieval on a long text document.\n",
        "    \"\"\"\n",
        "    # 1. Chunk text into units\n",
        "    chunks = build_simple_chunks(long, n_setences_per_passage)\n",
        "\n",
        "    # 2. Use co.rerank to rank chunks vs. query\n",
        "    chunks_reranked = co.rerank(query=query, documents=chunks, model=\"rerank-english-v3.0\")\n",
        "    idcs_sorted_by_relevance = [\n",
        "        result.index for result in chunks_reranked.results\n",
        "    ]\n",
        "\n",
        "    # 3. Add chunks back in order of relevance\n",
        "    selected = _add_chunks_by_priority(chunks, idcs_sorted_by_relevance, max_tokens)\n",
        "\n",
        "    # 4. Put condensed text back in original order\n",
        "    separator = \" \"\n",
        "    short = separator.join([chunk for index, chunk in sorted(selected, key=lambda item: item[0], reverse=False)])\n",
        "    return short"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "SX04iD3emIHr",
        "outputId": "dbe55fc4-bbc4-482a-cace-f5e2fc9318ba"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Example prompt\n",
        "prompt_template = \"\"\"\n",
        "## Instruction\n",
        "{query}\n",
        "\n",
        "## Document\n",
        "{document}\n",
        "\n",
        "## Answer\n",
        "\"\"\".strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "53eSmzkJmIHr",
        "outputId": "093c1167-d66d-42f7-e65d-ffe02b2d71ff"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The report outlines several key points regarding biometric identification within the context of the proposed Artificial Intelligence Act:\n",
            "\n",
            "1. **Prohibition of Real-Time Biometric Identification in Public Spaces**: The report proposes a ban on real-time biometric identification by law enforcement authorities in publicly accessible spaces, with specific exceptions. These exceptions are detailed in Article 5(1)(d) and are subject to safeguards, including monitoring, oversight, and limited reporting obligations at the EU level.\n",
            "\n",
            "2. **Exceptions to the Prohibition**: The exceptions to the ban on real-time biometric identification include:\n",
            "   - Search for victims of specific crimes (e.g., abduction, trafficking, sexual exploitation).\n",
            "   - Prevention of imminent threats to life or physical safety, including terrorist attacks.\n",
            "   - Localization or identification of suspects for specific criminal offenses listed in Annex IIa, which are punishable by a custodial sentence of at least four years.\n",
            "\n",
            "3. **Safeguards and Conditions**: The use of real-time biometric identification systems in publicly accessible spaces must comply with necessary and proportionate safeguards and conditions, including temporal, geographic, and personal limitations. A fundamental rights impact assessment must be completed, and the system must be registered in a database.\n",
            "\n",
            "4. **Authorization Requirements**: Each use of real-time biometric identification systems for law enforcement purposes requires prior authorization by a judicial or independent administrative authority. In cases of urgency, authorization can be sought within 24 hours after the use has commenced. If authorization is denied\n"
          ]
        }
      ],
      "source": [
        "query = \"What does the report say about biometric identification? Answer only based on the document.\"\n",
        "short_text = query_based_retrieval(long_text, MAX_TOKENS, query)\n",
        "prompt = prompt_template.format(query=query, document=short_text)\n",
        "print(generate_response(message=prompt, max_tokens=300))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zfpvo9WwbB6e"
      },
      "source": [
        "<a name=\"text-rank\"></a>\n",
        "# Approach 3: Text rank"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YNbvzWqgmIHs"
      },
      "source": [
        "In the final section we will show how we leverage graph theory to select chunks based on their centrality. Centrality is a graph-theoretic measure of how connected a node is; the higher the centrality, the more connected the node is to surrounding nodes (with fewer connections among those neighbors).\n",
        "\n",
        "The solution presented in this document can be broken down into five functional steps:\n",
        "\n",
        "1. Break the document into chunks.\n",
        "    - This mirrors the first step in [Approach 2](#query-based-retrieval).\n",
        "\n",
        "2. Embed each chunk using an embedding model and construct a similarity matrix.\n",
        "    - We utilize `co.embed` [documentation link](https://docs.cohere.com/reference/embed).\n",
        "\n",
        "3. Compute the centrality of each chunk.\n",
        "    - We employ a package called [`NetworkX`](https://networkx.org/documentation/networkx-1.10/overview.html). It constructs a graph where the chunks are nodes, and the similarity score between them serves as the weight of the edges. Then, we calculate the centrality of each chunk as the sum of the edge weights adjacent to the node representing that chunk.\n",
        "\n",
        "4. Retain the highest-centrality chunks until the context limit is reached.\n",
        "    - This step follows a similar approach to [Approach 2](#query-based-retrieval).\n",
        "\n",
        "5. Reassemble the shortened text by reordering chunks in their original order.\n",
        "    - This step mirrors the last step in [Approach 2](#query-based-retrieval).\n",
        "\n",
        "See `text_rank` as the starting point.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zC_fkJgyj3Hv"
      },
      "source": [
        "### Text rank implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "4fWR-HKpmIHs",
        "outputId": "1ddc2bf4-90d1-44df-d4cd-6908546db506"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "def text_rank(text: str, max_tokens: int, n_setences_per_passage: int) -> str:\n",
        "    \"\"\"\n",
        "    Shortens text by extracting key units of text from it based on their centrality.\n",
        "    The output is the concatenation of those key units, in their original order.\n",
        "    \"\"\"\n",
        "\n",
        "    # 1. Chunk text into units\n",
        "    chunks = build_simple_chunks(text, n_setences_per_passage)\n",
        "\n",
        "    # 2. Embed and construct similarity matrix\n",
        "    embeddings = np.array(\n",
        "        co.embed(\n",
        "            texts=chunks,\n",
        "            model=\"embed-v4.0\",\n",
        "            input_type=\"clustering\",\n",
        "        ).embeddings\n",
        "    )\n",
        "    similarities = np.dot(embeddings, embeddings.T)\n",
        "\n",
        "    # 3. Compute centrality and sort sentences by centrality\n",
        "    # Easiest to use networkx's `degree` function with similarity as weight\n",
        "    g = nx.from_numpy_array(similarities, edge_attr=\"weight\")\n",
        "    centralities = g.degree(weight=\"weight\")\n",
        "    idcs_sorted_by_centrality = [node for node, degree in sorted(centralities, key=lambda item: item[1], reverse=True)]\n",
        "\n",
        "    # 4. Add chunks back in order of centrality\n",
        "    selected = _add_chunks_by_priority(chunks, idcs_sorted_by_centrality, max_tokens)\n",
        "\n",
        "    # 5. Put condensed text back in original order\n",
        "    short = \" \".join([chunk for index, chunk in sorted(selected, key=lambda item: item[0], reverse=False)])\n",
        "\n",
        "    return short"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "YVVMr-S9mIHs",
        "outputId": "2fe871db-ac7a-4ac1-e7a1-e8c8292d95e4"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Example summary prompt.\n",
        "prompt_template = \"\"\"\n",
        "## Instruction\n",
        "Summarize the following Document in 3-5 sentences. Only answer based on the information provided in the document.\n",
        "\n",
        "## Document\n",
        "{document}\n",
        "\n",
        "## Summary\n",
        "\"\"\".strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "id": "vyahJTGImIHs",
        "outputId": "0a12951a-b917-4e69-a5c4-951907e32a90"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The document outlines the European Union's proposed Artificial Intelligence Act, which aims to regulate AI systems based on risk levels. It defines AI systems, providers, deployers, and other key terms. The Act prohibits certain AI practices, such as those manipulating human behavior or exploiting vulnerabilities. High-risk AI systems, including those in critical sectors like healthcare and law enforcement, must comply with strict requirements, including risk management, data governance, and transparency. Providers of high-risk AI systems have obligations like quality management, documentation, and conformity assessment. The Act also addresses general-purpose AI models, requiring providers to ensure transparency, respect copyright, and manage risks. It establishes an EU database for high-risk AI systems, mandates market surveillance, and sets penalties for non-compliance. The regulation aims to ensure AI systems are trustworthy, protect fundamental rights, and support innovation while maintaining a level playing field across the EU.\n"
          ]
        }
      ],
      "source": [
        "short_text = text_rank(long_text, MAX_TOKENS, 5)\n",
        "prompt = prompt_template.format(document=short_text)\n",
        "print(generate_response(message=prompt, max_tokens=600))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "krJVT65vrRsX"
      },
      "source": [
        "## Summary\n",
        "\n",
        "In this notebook we present three useful methods to over come the limitations of context window size. In the following [blog post](TODO:add link), we talk more about how these methods can be evaluated."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iUqiXtILroUY"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "base"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
