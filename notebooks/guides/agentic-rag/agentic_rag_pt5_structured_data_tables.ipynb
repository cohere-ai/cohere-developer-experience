{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/cohere-ai/cohere-developer-experience/blob/main/notebooks/guides/agentic-rag/agentic_rag_pt5_structured_data_tables.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Querying Structured Data (Tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous tutorials, we explored how to build agentic RAG applications over unstructured and semi-structured data. In this tutorial and the next, we'll turn our focus to structured data. \n",
    "\n",
    "This tutorial focuses on querying tables, and the next tutorial will be about querying SQL databases.\n",
    "\n",
    "Consider a scenario where you have a CSV file containing evaluation results for an LLM application.\n",
    "\n",
    "A user might ask questions like \"What's the average score for a specific use case?\" or \"Which configuration has the lowest latency?\". These queries require not just retrieval, but also data analysis and interpretation.\n",
    "\n",
    "In this tutorial, we'll cover:\n",
    "- Creating a function to execute Python code\n",
    "- Setting up a tool to interact with tabular data\n",
    "- Building an agent for querying tabular data\n",
    "- Running the agent\n",
    "\n",
    "Let's get started by setting up our environment and defining the necessary tools for our agent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "To get started, first we need to install the `cohere` library and create a Cohere client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install cohere pandas -qq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import cohere\n",
    "\n",
    "co = cohere.ClientV2(\"COHERE_API_KEY\") # Get your free API key: https://dashboard.cohere.com/api-keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here's the data we'll be working with. `evaluation_results.csv` is a CSV file containing evaluation results for a set of LLM applications - name extraction, email drafting, and article summarization.\n",
    "\n",
    "The file has the following columns:\n",
    "- `usecase`: The use case.\n",
    "- `run`: The run ID.\n",
    "- `score`: The evaluation score for a particular run.\n",
    "- `temperature`: The temperature setting of the model for a particular run.\n",
    "- `tokens`: The number of tokens generated by the model for a particular run.\n",
    "- `latency`: The latency of the model's response for a particular run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important: the data can be [found here](https://github.com/cohere-ai/cohere-developer-experience/blob/main/notebooks/guides/agentic-rag/evaluation_results.csv). Make sure to have the file in the same directory as this notebook for the imports to work correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>usecase</th>\n",
       "      <th>run</th>\n",
       "      <th>score</th>\n",
       "      <th>temperature</th>\n",
       "      <th>tokens</th>\n",
       "      <th>latency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>extract_names</td>\n",
       "      <td>A</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.3</td>\n",
       "      <td>103</td>\n",
       "      <td>1.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>draft_email</td>\n",
       "      <td>A</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.3</td>\n",
       "      <td>252</td>\n",
       "      <td>2.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>summarize_article</td>\n",
       "      <td>A</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.3</td>\n",
       "      <td>350</td>\n",
       "      <td>4.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>extract_names</td>\n",
       "      <td>B</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.3</td>\n",
       "      <td>101</td>\n",
       "      <td>2.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>draft_email</td>\n",
       "      <td>B</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.3</td>\n",
       "      <td>230</td>\n",
       "      <td>3.20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             usecase run  score  temperature  tokens  latency\n",
       "0      extract_names   A    0.5          0.3     103     1.12\n",
       "1        draft_email   A    0.6          0.3     252     2.50\n",
       "2  summarize_article   A    0.8          0.3     350     4.20\n",
       "3      extract_names   B    0.2          0.3     101     2.85\n",
       "4        draft_email   B    0.4          0.3     230     3.20"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('evaluation_results.csv')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a function to execute Python code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we introduce a new tool that allows the agent to execute Python code and return the result. The agent will use this tool to generate pandas code for querying the data.\n",
    "\n",
    "To create this tool, we'll use the `PythonREPL` class from the `langchain_experimental.utilities` module. This class provides a sandboxed environment for executing Python code and returns the result.\n",
    "\n",
    "First, we define a `python_tool` that uses the `PythonREPL` class to execute Python code and return the result.\n",
    "\n",
    "Next, we define a `ToolInput` class to handle the input for the `python_tool`.\n",
    "\n",
    "Finally, we create a function `analyze_evaluation_results` that takes a string of Python code as input, executes the code using the Python tool we created, and returns the result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPORTANT:**\n",
    "\n",
    "The source code for tool definitions can be [found here](https://github.com/cohere-ai/cohere-developer-experience/blob/main/notebooks/guides/agentic-rag/tool_def.py). Make sure to have the `tool_def.py` file in the same directory as this notebook for the imports to work correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tool_def import (\n",
    "    analyze_evaluation_results,\n",
    "    analyze_evaluation_results_tool,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "functions_map = {\n",
    "    \"analyze_evaluation_results\": analyze_evaluation_results\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up a tool to interact with tabular data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define the `analyze_evaluation_results` tool. There are many ways we can set up a tool to work with CSV data, and in this example, we are using the tool description to provide the agent with the necessary context for working with the CSV file, such as:\n",
    "- the name of the CSV file to load\n",
    "- the columns of the CSV file\n",
    "- additional instructions on what libraries to use (in this case, `pandas`)\n",
    "\n",
    "The parameter of this tool is the `code` string containing the Python code that the agent writes to analyze the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "analyze_evaluation_results_tool = {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"analyze_evaluation_results\",\n",
    "            \"description\": \"Generate Python code using the pandas library to analyze evaluation results from a dataframe called `evaluation_results`. The dataframe has columns 'usecase','run','score','temperature','tokens', and 'latency'. You must start with `import pandas as pd` and read a CSV file called `evaluation_results.csv` into the `evaluation_results` dataframe.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"code\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"Executable Python code\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"code\"]\n",
    "            }\n",
    "        }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [analyze_evaluation_results_tool]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building an agent for querying tabular data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's create a `run_agent` function to run the agentic RAG workflow, the same as in Part 1.\n",
    "\n",
    "The only change we are making here is to make the system message simpler and more specific since the agent now only has one tool.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message=\"\"\"## Task and Context\n",
    "ou are an assistant who helps developers analyze LLM application evaluation results from a CSV files.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"command-a-03-2025\"\n",
    "\n",
    "def run_agent(query, messages=None):\n",
    "    if messages is None:\n",
    "        messages = []\n",
    "        \n",
    "    if \"system\" not in {m.get(\"role\") for m in messages}:\n",
    "        messages.append({\"role\": \"system\", \"content\": system_message})\n",
    "    \n",
    "    # Step 1: get user message\n",
    "    print(f\"Question:\\n{query}\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    messages.append({\"role\": \"user\", \"content\": query})\n",
    "\n",
    "    # Step 2: Generate tool calls (if any)\n",
    "    response = co.chat(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        tools=tools,\n",
    "        temperature=0.3\n",
    "    )\n",
    "\n",
    "    while response.message.tool_calls:\n",
    "        \n",
    "        print(\"TOOL PLAN:\")\n",
    "        print(response.message.tool_plan,\"\\n\")\n",
    "        print(\"TOOL CALLS:\")\n",
    "        for tc in response.message.tool_calls:\n",
    "            if tc.function.name == \"analyze_evaluation_results\":\n",
    "                print(f\"Tool name: {tc.function.name}\")\n",
    "                tool_call_prettified = print(\"\\n\".join(f\"  {line}\" for line_num, line in enumerate(json.loads(tc.function.arguments)[\"code\"].splitlines())))\n",
    "                print(tool_call_prettified)\n",
    "            else:\n",
    "                print(f\"Tool name: {tc.function.name} | Parameters: {tc.function.arguments}\")\n",
    "        print(\"=\"*50)\n",
    "\n",
    "        messages.append({\"role\": \"assistant\", \"tool_calls\": response.message.tool_calls, \"tool_plan\": response.message.tool_plan})        \n",
    "        \n",
    "        # Step 3: Get tool results\n",
    "        for tc in response.message.tool_calls:\n",
    "            tool_result = functions_map[tc.function.name](**json.loads(tc.function.arguments))\n",
    "            tool_content = [({\"type\": \"document\", \"document\": {\"data\": json.dumps(tool_result)}})]\n",
    "                \n",
    "            messages.append({\"role\": \"tool\", \"tool_call_id\": tc.id, \"content\": tool_content})\n",
    "        \n",
    "        # Step 4: Generate response and citations \n",
    "        response = co.chat(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            tools=tools,\n",
    "            temperature=0.3\n",
    "        )\n",
    "    \n",
    "    messages.append({\"role\": \"assistant\", \"content\": response.message.content[0].text})\n",
    "        \n",
    "    # Print final response\n",
    "    print(\"RESPONSE:\")\n",
    "    print(response.message.content[0].text)\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Print citations (if any)\n",
    "    verbose_source = False # Change to True to display the contents of a source\n",
    "    if response.message.citations:\n",
    "        print(\"CITATIONS:\\n\")\n",
    "        for citation in response.message.citations:\n",
    "            print(f\"Start: {citation.start}| End:{citation.end}| Text:'{citation.text}' \")\n",
    "            print(\"Sources:\")\n",
    "            for idx, source in enumerate(citation.sources):\n",
    "                print(f\"{idx+1}. {source.id}\")\n",
    "                if verbose_source:\n",
    "                    print(f\"{source.tool_output}\")\n",
    "            print(\"\\n\")\n",
    "                    \n",
    "    return messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's ask the agent a few questions, starting with this query about the average evaluation score in run A.\n",
    "\n",
    "To answer this query, the agent needs to write Python code that uses the pandas library to calculate the average evaluation score in run A. And it gets the answer right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:\n",
      "What's the average evaluation score in run A\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python REPL can execute arbitrary code. Use with caution.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOOL PLAN:\n",
      "I will write and execute Python code to calculate the average evaluation score in run A. \n",
      "\n",
      "TOOL CALLS:\n",
      "Tool name: analyze_evaluation_results\n",
      "  import pandas as pd\n",
      "  \n",
      "  df = pd.read_csv(\"evaluation_results.csv\")\n",
      "  \n",
      "  # Calculate the average evaluation score in run A\n",
      "  average_score_run_A = df[df[\"run\"] == \"A\"][\"score\"].mean()\n",
      "  \n",
      "  print(f\"Average evaluation score in run A: {average_score_run_A}\")\n",
      "None\n",
      "==================================================\n",
      "RESPONSE:\n",
      "The average evaluation score in run A is 0.63.\n",
      "==================================================\n",
      "CITATIONS:\n",
      "\n",
      "Start: 41| End:46| Text:'0.63.' \n",
      "Sources:\n",
      "1. analyze_evaluation_results_phqpwwat2hgf:0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "messages = run_agent(\"What's the average evaluation score in run A\")\n",
    "# Answer: 0.63"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we ask a slightly more complex question, this time about the latency of the highest-scoring run for one use case. This requires the agent to filter based on the use case, find the highest-scoring run, and return the latency value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:\n",
      "What's the latency of the highest-scoring run for the summarize_article use case?\n",
      "==================================================\n",
      "TOOL PLAN:\n",
      "I will write Python code to find the latency of the highest-scoring run for the summarize_article use case. \n",
      "\n",
      "TOOL CALLS:\n",
      "Tool name: analyze_evaluation_results\n",
      "  import pandas as pd\n",
      "  \n",
      "  df = pd.read_csv(\"evaluation_results.csv\")\n",
      "  \n",
      "  # Filter for the summarize_article use case\n",
      "  use_case_df = df[df[\"usecase\"] == \"summarize_article\"]\n",
      "  \n",
      "  # Find the highest-scoring run\n",
      "  highest_score_run = use_case_df.loc[use_case_df[\"score\"].idxmax()]\n",
      "  \n",
      "  # Get the latency of the highest-scoring run\n",
      "  latency = highest_score_run[\"latency\"]\n",
      "  \n",
      "  print(f\"Latency of the highest-scoring run: {latency}\")\n",
      "None\n",
      "==================================================\n",
      "RESPONSE:\n",
      "The latency of the highest-scoring run for the summarize_article use case is 4.8.\n",
      "==================================================\n",
      "CITATIONS:\n",
      "\n",
      "Start: 77| End:81| Text:'4.8.' \n",
      "Sources:\n",
      "1. analyze_evaluation_results_es3hnnnp5pey:0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "messages = run_agent(\"What's the latency of the highest-scoring run for the summarize_article use case?\")\n",
    "# Answer: 4.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we ask a question to compare the use cases in terms of token usage, and to show a markdown table to show the comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:\n",
      "Which use case uses the least amount of tokens on average? Show the comparison of all use cases in a markdown table.\n",
      "==================================================\n",
      "TOOL PLAN:\n",
      "I will use the analyze_evaluation_results tool to generate Python code to find the use case that uses the least amount of tokens on average. I will also generate code to create a markdown table to compare all use cases. \n",
      "\n",
      "TOOL CALLS:\n",
      "Tool name: analyze_evaluation_results\n",
      "  import pandas as pd\n",
      "  \n",
      "  evaluation_results = pd.read_csv(\"evaluation_results.csv\")\n",
      "  \n",
      "  # Group by 'usecase' and calculate the average tokens\n",
      "  avg_tokens_by_usecase = evaluation_results.groupby('usecase')['tokens'].mean()\n",
      "  \n",
      "  # Find the use case with the least average tokens\n",
      "  least_avg_tokens_usecase = avg_tokens_by_usecase.idxmin()\n",
      "  \n",
      "  print(f\"Use case with the least average tokens: {least_avg_tokens_usecase}\")\n",
      "  \n",
      "  # Create a markdown table comparing average tokens for all use cases\n",
      "  markdown_table = avg_tokens_by_usecase.reset_index()\n",
      "  markdown_table.columns = [\"Use Case\", \"Average Tokens\"]\n",
      "  print(markdown_table.to_markdown(index=False))\n",
      "None\n",
      "==================================================\n",
      "RESPONSE:\n",
      "The use case that uses the least amount of tokens on average is extract_names.\n",
      "\n",
      "Here is a markdown table comparing the average tokens for all use cases:\n",
      "\n",
      "| Use Case | Average Tokens |\n",
      "|:-------------------------|-------------------------------:|\n",
      "| draft_email | 245.75 |\n",
      "| extract_names | 106.25 |\n",
      "| summarize_article | 355.75 |\n",
      "==================================================\n",
      "CITATIONS:\n",
      "\n",
      "Start: 64| End:78| Text:'extract_names.' \n",
      "Sources:\n",
      "1. analyze_evaluation_results_zp68h5304e3v:0\n",
      "\n",
      "\n",
      "Start: 156| End:164| Text:'Use Case' \n",
      "Sources:\n",
      "1. analyze_evaluation_results_zp68h5304e3v:0\n",
      "\n",
      "\n",
      "Start: 167| End:181| Text:'Average Tokens' \n",
      "Sources:\n",
      "1. analyze_evaluation_results_zp68h5304e3v:0\n",
      "\n",
      "\n",
      "Start: 248| End:259| Text:'draft_email' \n",
      "Sources:\n",
      "1. analyze_evaluation_results_zp68h5304e3v:0\n",
      "\n",
      "\n",
      "Start: 262| End:268| Text:'245.75' \n",
      "Sources:\n",
      "1. analyze_evaluation_results_zp68h5304e3v:0\n",
      "\n",
      "\n",
      "Start: 273| End:286| Text:'extract_names' \n",
      "Sources:\n",
      "1. analyze_evaluation_results_zp68h5304e3v:0\n",
      "\n",
      "\n",
      "Start: 289| End:295| Text:'106.25' \n",
      "Sources:\n",
      "1. analyze_evaluation_results_zp68h5304e3v:0\n",
      "\n",
      "\n",
      "Start: 300| End:317| Text:'summarize_article' \n",
      "Sources:\n",
      "1. analyze_evaluation_results_zp68h5304e3v:0\n",
      "\n",
      "\n",
      "Start: 320| End:326| Text:'355.75' \n",
      "Sources:\n",
      "1. analyze_evaluation_results_zp68h5304e3v:0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "messages = run_agent(\"Which use case uses the least amount of tokens on average? Show the comparison of all use cases in a markdown table.\")\n",
    "# Answer: extract_names (106.25), draft_email (245.75), summarize_article (355.75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this tutorial, we learned about:\n",
    "- How to create a function to execute Python code\n",
    "- How to set up a tool to interact with tabular data\n",
    "- How to run the agent\n",
    "\n",
    "By implementing these techniques, we've expanded our agentic RAG system to handle structured data in the form of tables.\n",
    "\n",
    "While this tutorial demonstrated how to work with tabular data using pandas and Python, the agentic RAG approach can be applied to other forms of structured data as well. This means we can build agents that can translate natural language queries into various types of data analysis tasks.\n",
    "\n",
    "In Part 6, we'll learn how to do structured query generation for SQL databases."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
