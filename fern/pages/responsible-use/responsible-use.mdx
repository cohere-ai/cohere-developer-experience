---
title: Overview
slug: docs/responsible-use
hidden: false
description: >-
  Cohere's Responsible Use documentation provides guidelines, harm prevention
  measures, and model cards for ethical language model usage.
image: ../../assets/images/5d25315-cohere_docs_preview_image_1200x630_copy.jpg
keywords: 'AI safety, AI risk, responsible AI'
createdAt: 'Thu Sep 01 2022 19:22:12 GMT+0000 (Coordinated Universal Time)'
updatedAt: 'Fri Mar 15 2024 04:47:51 GMT+0000 (Coordinated Universal Time)'
---
The Responsible Use documentation aims to guide developers in using language models constructively and ethically. Toward this end, we've published [guidelines](/docs/usage-guidelines) for using our API safely, as well as our processes around [harm prevention](#harm-prevention). We provide model cards to communicate the strengths and weaknesses of our models and to encourage responsible use (motivated by [Mitchell, 2019](https://arxiv.org/pdf/1810.03993.pdf)). We also provide a [data statement](/data-statement) describing our pre-training datasets (motivated by [Bender and Friedman, 2018](https://www.aclweb.org/anthology/Q18-1041/)).

**Model Cards:**

- [Generation](/docs/generation-benchmarks)
- [Representation](/docs/representation-benchmarks)

If you have feedback or questions, please feel free to [let us know](mailto:responsibility@cohere.ai) — we are here to help.

## Harm Prevention

We aim to mitigate adverse use of our models with the following:

- **Responsible AI Research:** We’ve established a dedicated safety team which conducts [research](https://arxiv.org/abs/2108.07790) and development to build safer language models, and we’re investing in technical (e.g., usage monitoring) and non-technical (e.g., a dedicated team reviewing use cases) measures to mitigate potential harms.
- **Cohere Responsibility Council:** We’ve established an external advisory council made up of experts who work with us to ensure that the technology we’re building is deployed safely for everyone.
- **No online learning:** To safeguard model integrity and prevent underlying models from [being poisoned](https://www.theverge.com/2016/3/24/11297050/tay-microsoft-chatbot-racist) with harmful content by adversarial actors, user input goes through curation and enrichment prior to integration with training.
