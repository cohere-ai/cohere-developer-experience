---
title: "Usage Guidelines"
slug: "docs/usage-guidelines"

hidden: true
description: "Developers must outline and get approval for their use case to access the Cohere API, understanding the models and limitations. They should refer to model cards for detailed information and document potential harms of their application. Certain use cases, such as violence, hate speech, fraud, and privacy violations, are strictly prohibited."
image: "../../../assets/images/da0a0ac-cohere_docs_preview_image_1200x630_copy.jpg"  
keywords: "Cohere API"

createdAt: "Thu Sep 01 2022 19:24:15 GMT+0000 (Coordinated Universal Time)"
updatedAt: "Tue Apr 02 2024 10:30:48 GMT+0000 (Coordinated Universal Time)"
---
To use our API, every developer must clearly outline their use case and have it approved by Cohere through our application process. The application requires thoroughly understanding our models and their limitations, which will change as the models improve. Beyond these Usage Guidelines, you should refer to the <a href="https://docs.cohere.ai/docs/generation-card" target="_blank">Generation</a> and <a href="https://docs.cohere.ai/docs/representation-card" target="_blank">Representation</a> model cards for detailed information about each model.

By understanding the language models that power our API endpoints, being aware of their limitations, and documenting your development practices, you can do great things with the Cohere Platform.

### 1\. Comprehend Cohere

(<a href ="https://arxiv.org/pdf/2003.08271.pdf" target="_blank">Qiu et al., 2020</a>) describes the history, technical aspects, and applications of pre-trained language models like the ones which power the Cohere Platform. We recommend reading this survey and other language modeling research to learn what kinds of knowledge are encoded in language models and how to use their outputs responsibly in downstream tasks.

Language models might encode the following:

- **Linguistic information** such as subject-verb agreement, part-of-speech, and other simple syntactic structures (<a href ="https://arxiv.org/pdf/1903.08855" target="_blank">Liu et al., 2019</a>;<a href ="https://arxiv.org/pdf/1909.03368" target="_blank">Hewitt et al., 2019</a>).
- **World knowledge,** including relational and commonsense knowledge such as where famous individuals were born or the color of the sky, limited by what is contained in the training data.
- **Social biases,** such as stereotypes common on the internet or in Western culture (<a href ="https://www.aclweb.org/anthology/N19-1063.pdf" target="_blank">May et al., 2019</a>).

### 2\. Document your Application

We encourage careful consideration and documentation of the potential harms of any application developed using the Cohere Platform. If you build an application that uses model outputs, please provide your users a link to the corresponding model card, explaining how your application uses its output. For example, if you trained a downstream classifier using the `embed` endpoint, users should be provided with thorough documentation (such as model cards and data statements) of that classifier's training procedure and behavior.

### 3\. Understand and Prevent Disallowed Use Cases

The Cohere Platform may not be used for any of the following purposes. The description for each disallowed use case is illustrative but **not exhaustive**; Cohere reserves the right to terminate access for harms which are not listed at our sole discretion.

- **Violence and threats:**
  - **Violence/Incitement**: Actions that threaten, encourage, or incite violence against anyone, directly or indirectly.
  - **Self-harm**: Promoting or glorifying acts of self-harm, such as cutting, eating disorders like anorexia or bulimia, and suicide.
  - **Sexual exploitation**: Promoting or celebrating sexual exploitation, including the sexualization of minors.
  - **Hate speech**: Promoting hatred or glorifying abuse against people based on characteristics like race, ethnicity, national origin, religion, disability, disease, age, sexual orientation, gender, or gender identity.
- **Antisocial and antidemocratic uses:**
  - **Harassment:** Bullying, threatening, shaming, or doxxing.
  - **Insensitivity**: Belittling victims of serious physical or emotional harm (even if unintentional).
  - **Intentional sowing of division**: Sharing of divisive generated content in order to turn a community against itself.
  - **Harmful belief perpetuation**: Perpetuating racism, or sexism (even if unintentional).
  - **Applications that aim to characterize identity:** Attempting to characterize gender, race, or ethnicity.
  - **Graphic depictions**: Distribution of sexually explicit acts, torture, or abuse.
  - **Political manipulation**: Attempting to influence political decisions, or opinions.
- **Deceit:**
  - **Fraud**: Catfishing, phishing, or attempting to circumvent the law.
  - **Spam**: Sending unsolicited email and messages, or manipulating search engines.
  - **Misrepresentation**: Representing raw generations as coming from humans, using supervised generations with false identities, or a single person using generations with many identities that appear to be independent.
  - **Misinformation**: Creating or promoting harmful false claims about government policies, or public figures, including applications founded on unscientific premises.
- **Attacks on security or privacy:**
  - **Security breaches**: Spearphishing.
  - **Privacy violations**: Model attacks to extract personal information.
- **Unsafe unsupervised uses:**
  - **Social media**: Posting content to social platforms in an automated way.
  - **No transparency**: Applications that do not disclose that the content is generated through automated means.
- **Decision-making**:
  - AI-based social scoring for general purposes done by public authorities; using output toward larger decision-making systems that will influence actions, decisions, or policies without a human in the loop.
  - **Classification of individuals**: Applications that classify and/or profile people based on protected characteristics, or infer those characteristics from text written about them or by them.
- **Other**:
  - **Intentional manipulative redirection of attention**: Sharing positive generated content in order to direct attention away from harmful actions.
  - **Plagiarism**: Tools that promote academic dishonesty.

Usages which appear to violate our guidelines should be reported within 24 hours to Cohere by contacting us at [safety@cohere.ai](mailto:safety@cohere.ai).

**Note about adversarial attacks:** Intentional stress testing of the API and adversarial attacks are allowable, but violative generations must be disclosed here, [reported immediately](https://ai8x92z50km.typeform.com/to/EI7d26j6#user_id=xxxxx&organization_id=xxxxx), and must not be used for any purpose except for documenting the result of such attacks in a responsible manner.
