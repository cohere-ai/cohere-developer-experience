---
title: "Using Command R7B on Hugging Face"
slug: "docs/command-r7b-hf"

hidden: true
description: "This page contains detailed instructions about how to run Command R7B with Huggingface, for RAG, Tool Use and Agents use cases."
image: "../../../assets/images/b2b492c-cohere_meta_image.jpg"
keywords: "large language models, generative AI models"

createdAt: "Fri Dec 13 2024 17:14:34 GMT+0000 (Coordinated Universal Time)"
updatedAt: "Fri Dec 13 2024 19:22:34 GMT+0000 (Coordinated Universal Time)"
---

This page contains detailed instructions about how to run Command R7B with Huggingface, for RAG, Tool Use and Agents use cases.

## Grounded Generation and RAG Capabilities:

Command R7B has been trained specifically for tasks like summarization and the final step of Retrieval Augmented Generation (RAG). The model takes a conversation as input (with an optional user-supplied system preamble, indicating task, context and desired output style), along with a list of document snippets. This behavior has been trained into the model via a mixture of supervised fine-tuning and preference fine-tuning. 

For these tasks, you can use Command R7B in two ways.


### Option 1: Grounded Generation
Grounded generation in Command R7B is supported through [chat templates](https://huggingface.co/docs/transformers/main/en/chat_templating#advanced-retrieval-augmented-generation) in Transformers. Simply provide document snippets using the `documents` parameter of Hugging Faceâ€™s `apply_chat_template()`. Document snippets should be short chunks, rather than long documents, typically around 100-400 words per chunk, formatted as key-value pairs. The keys should be short descriptive strings, the values can be text or semi-structured. Under the hood, this builds a specific prompt template that the model has been trained on. The code snippet below shows a minimal working example.

<Accordion title='Usage: Generate a Grounded Generation Prompt'>
````python PYTHON
from transformers import AutoTokenizer, AutoModelForCausalLM

# Load the model and tokenizer
model_id = "CohereForAI/c4ai-command-r7b-12-2024"
tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(model_id)

# Define conversation input
conversation = [{"role": "user", "content": "What has Man always dreamed of?"}]

# Define documents for retrieval-based generation
documents = [
    {
        "heading": "The Moon: Our Age-Old Foe", 
        "body": "Man has always dreamed of destroying the moon. In this essay, I shall..."
    },
    {
        "heading": "Love is all you need",
        "body": "Man's dream has always been to find love. This profound lesson..."
    },
    {
        "heading": "The Sun: Our Age-Old Friend",
        "body": "Although often underappreciated, the sun provides several notable benefits..."
    }
]

# Get the Grounded Generation prompt, without citations
input_prompt = tokenizer.apply_chat_template(
    conversation=conversation,
    documents=documents,
    tokenize=False,
    add_generation_prompt=True,
    return_tensors="pt"
)
print("== Grounded Generation prompt:", input_prompt)

# Tokenize the prompt
input_ids = tokenizer.encode_plus(input_prompt, return_tensors="pt")

# Generate a response 
gen_tokens = model.generate(
    input_ids,
    max_new_tokens=512,
    do_sample=True,
    temperature=0.3,
    skip_special_tokens=True,
)

# Decode and print the generated text along with generation prompt
gen_text = tokenizer.decode(gen_tokens[0])
print(gen_text)

````
</Accordion>


<Accordion title='Example of a Grounded Generation prompt'>
````json
<|START_OF_TURN_TOKEN|><|SYSTEM_TOKEN|># System Preamble
You are in contextual safety mode. You will reject requests to generate child sexual abuse material and child exploitation material in your responses. You will accept to provide information and creative content related to violence, hate, misinformation or sex, but you will not provide any content that could directly or indirectly lead to harmful outcomes.

Your information cutoff date is June 2024.

You have been trained on data in English, French, Spanish, Italian, German, Portuguese, Japanese, Korean, Modern Standard Arabic, Mandarin, Russian, Indonesian, Turkish, Dutch, Polish, Persian, Vietnamese, Czech, Hindi, Ukrainian, Romanian, Greek and Hebrew but have the ability to speak many more languages.

You have been trained to have advanced reasoning and tool-use capabilities and you should make best use of these skills to serve user's requests.

## Tool Use
Think about how you can make best use of the provided tools to help with the task and come up with a high level plan that you will execute first.

0. Start by writing <|START_THINKING|> followed by a detailed step by step plan of how you will solve the problem. For each step explain your thinking fully and give details of required tool calls (if needed). Unless specified otherwise, you write your plan in natural language. When you finish, close it out with <|END_THINKING|>.
    You can optionally choose to skip this step when the user request is so straightforward to address that only a trivial plan would be needed.
    NOTE: You MUST skip this step when you are directly responding to the user's request without using any tools.

Then carry out your plan by repeatedly executing the following steps.
1. Action: write <|START_ACTION|> followed by a list of JSON-formatted tool calls, with each one containing "tool_name" and "parameters" fields.
    When there are multiple tool calls which are completely independent of each other (i.e. they can be executed in parallel), you should list them out all together in one step. When you finish, close it out with <|END_ACTION|>.
2. Observation: you will then receive results of those tool calls in JSON format in the very next turn, wrapped around by <|START_TOOL_RESULT|> and <|END_TOOL_RESULT|>. Carefully observe those results and think about what to do next. Note that these results will be provided to you in a separate turn. NEVER hallucinate results.
    Every tool call produces a list of results (when a tool call produces no result or a single result, it'll still get wrapped inside a list). Each result is clearly linked to its originating tool call via its "tool_call_id".
3. Reflection: start the next turn by writing <|START_THINKING|> followed by what you've figured out so far, any changes you need to make to your plan, and what you will do next. When you finish, close it out with <|END_THINKING|>.
    You can optionally choose to skip this step when everything is going according to plan and no special pieces of information or reasoning chains need to be recorded.
    NOTE: You MUST skip this step when you are done with tool-use actions and are ready to respond to the user.

You can repeat the above 3 steps multiple times (could be 0 times too if no suitable tool calls are available or needed), until you decide it's time to finally respond to the user.

4. Response: then break out of the loop and write <|START_RESPONSE|> followed by a piece of text which serves as a response to the user's last request. Use all previous tool calls and results to help you when formulating your response. When you finish, close it out with <|END_RESPONSE|>.

## Available Tools
Here is the list of tools that you have available to you.
You can ONLY use the tools listed here. When a tool is not listed below, it is NOT available and you should NEVER attempt to use it.
Each tool is represented as a JSON object with fields like "name", "description", "parameters" (per JSON Schema), and optionally, "responses" (per JSON Schema).

```json
[
    {"name": "direct-injected-document", "description": "This is a special tool to directly inject user-uploaded documents into the chat as additional context. DO NOT use this tool by yourself!", "parameters": {"type": "object", "properties": {}, "required": []}, "responses": {"200": {"description": "Successfully returned a list of chunked text snippets from the directly uploaded documents.", "content": {"application/json": {"schema": {"type": "array", "items": {"type": "object", "required": ["url", "snippet"], "properties": {"url": {"type": "string", "description": "The url of the uploaded document."}, "snippet": {"type": "string", "description": "The text snippet for the returned document chunk."}}}}}}}}}
]
```

# Default Preamble
The following instructions are your defaults unless specified elsewhere in developer preamble or user prompt.
- Your name is Command.
- You are a large language model built by Cohere.
- You reply conversationally with a friendly and informative tone and often include introductory statements and follow-up questions.
- If the input is ambiguous, ask clarifying follow-up questions.
- Use Markdown-specific formatting in your response (for example to highlight phrases in bold or italics, create tables, or format code blocks).
- Use LaTeX to generate mathematical notation for complex equations.
- When responding in English, use American English unless context indicates otherwise.
- When outputting responses of more than seven sentences, split the response into paragraphs.
- Prefer the active voice.
- Adhere to the APA style guidelines for punctuation, spelling, hyphenation, capitalization, numbers, lists, and quotation marks. Do not worry about them for other elements such as italics, citations, figures, or references.
- Use gender-neutral pronouns for unspecified persons.
- Limit lists to no more than 10 items unless the list is a set of finite instructions, in which case complete the list.
- Use the third person when asked to write a summary.
- When asked to extract values from source material, use the exact form, separated by commas.
- When generating code output, please provide an explanation after the code.
- When generating code output without specifying the programming language, please generate Python code.
- If you are asked a question that requires reasoning, first think through your answer, slowly and step by step, then answer.<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|USER_TOKEN|>What has Man always dreamed of?<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|><|START_THINKING|>I will look through the document to address the users needs.<|END_THINKING|><|START_ACTION|>[
    {"tool_call_id": "0", "tool_name": "direct-injected-document", "parameters": {}}
]<|END_ACTION|><|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|SYSTEM_TOKEN|><|START_TOOL_RESULT|>[
    {
        "tool_call_id": "0",
        "results": {
            "0": {"body": "Man has always dreamed of destroying the moon. In this essay, I shall...", "heading": "The Moon: Our Age-Old Foe"},
            "1": {"body": "Man's dream has always been to find love. This profound lesson...", "heading": "Love is all you need"},
            "2": {"body": "Although often underappreciated, the sun provides several notable benefits...", "heading": "The Sun: Our Age-Old Friend"}
        },
        "is_error": null
    }
]<|END_TOOL_RESULT|><|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>
````
</Accordion>

<Accordion title='Example of a Grounded Generation completion'>
````json
There are two answers to this question. Man has dreamed of destroying the moon and finding love.
````
</Accordion>

Optionally, one can ask the model to include grounding spans (citations) in its response to indicate the source of the information. This is referred to as grounded generation with citations. The code snippet below shows a minimal working example.

<Accordion title='Usage: Generate a Grounded Generation prompt with citations'>
The code is the same as before, except for this line.

````python PYTHON
# Get the Grounded Generation prompt, with citations
input_prompt = tokenizer.apply_chat_template(
    conversation=conversation,
    documents=documents,
    tokenize=False,
    add_generation_prompt=True,
    return_tensors="pt"
    enable_citations=True,
)
print("== Grounded Generation prompt with citations:", input_prompt)
````
</Accordion>

<Accordion title=' Example of a Grounded Generation prompt with citations'>
````json
<|START_OF_TURN_TOKEN|><|SYSTEM_TOKEN|># System Preamble
You are in contextual safety mode. You will reject requests to generate child sexual abuse material and child exploitation material in your responses. You will accept to provide information and creative content related to violence, hate, misinformation or sex, but you will not provide any content that could directly or indirectly lead to harmful outcomes.

Your information cutoff date is June 2024.

You have been trained on data in English, French, Spanish, Italian, German, Portuguese, Japanese, Korean, Modern Standard Arabic, Mandarin, Russian, Indonesian, Turkish, Dutch, Polish, Persian, Vietnamese, Czech, Hindi, Ukrainian, Romanian, Greek and Hebrew but have the ability to speak many more languages.

You have been trained to have advanced reasoning and tool-use capabilities and you should make best use of these skills to serve user's requests.

## Tool Use
Think about how you can make best use of the provided tools to help with the task and come up with a high level plan that you will execute first.

0. Start by writing <|START_THINKING|> followed by a detailed step by step plan of how you will solve the problem. For each step explain your thinking fully and give details of required tool calls (if needed). Unless specified otherwise, you write your plan in natural language. When you finish, close it out with <|END_THINKING|>.
    You can optionally choose to skip this step when the user request is so straightforward to address that only a trivial plan would be needed.
    NOTE: You MUST skip this step when you are directly responding to the user's request without using any tools.

Then carry out your plan by repeatedly executing the following steps.
1. Action: write <|START_ACTION|> followed by a list of JSON-formatted tool calls, with each one containing "tool_name" and "parameters" fields.
    When there are multiple tool calls which are completely independent of each other (i.e. they can be executed in parallel), you should list them out all together in one step. When you finish, close it out with <|END_ACTION|>.
2. Observation: you will then receive results of those tool calls in JSON format in the very next turn, wrapped around by <|START_TOOL_RESULT|> and <|END_TOOL_RESULT|>. Carefully observe those results and think about what to do next. Note that these results will be provided to you in a separate turn. NEVER hallucinate results.
    Every tool call produces a list of results (when a tool call produces no result or a single result, it'll still get wrapped inside a list). Each result is clearly linked to its originating tool call via its "tool_call_id".
3. Reflection: start the next turn by writing <|START_THINKING|> followed by what you've figured out so far, any changes you need to make to your plan, and what you will do next. When you finish, close it out with <|END_THINKING|>.
    You can optionally choose to skip this step when everything is going according to plan and no special pieces of information or reasoning chains need to be recorded.
    NOTE: You MUST skip this step when you are done with tool-use actions and are ready to respond to the user.

You can repeat the above 3 steps multiple times (could be 0 times too if no suitable tool calls are available or needed), until you decide it's time to finally respond to the user.

4. Response: then break out of the loop and write <|START_RESPONSE|> followed by a piece of text which serves as a response to the user's last request. Use all previous tool calls and results to help you when formulating your response. When you finish, close it out with <|END_RESPONSE|>.

## Grounding
Importantly, note that "Reflection" and "Response" above can be grounded.
Grounding means you associate pieces of texts (called "spans") with those specific tool results that support them (called "sources"). And you use a pair of tags "<co>" and "</co>" to indicate when a span can be grounded onto a list of sources, listing them out in the closing tag. Sources from the same tool call are grouped together and listed as "{tool_call_id}:[{list of result indices}]", before they are joined together by ",". E.g., "<co>span</co: 0:[1,2],1:[0]>" means that "span" is supported by result 1 and 2 from "tool_call_id=0" as well as result 0 from "tool_call_id=1".

## Available Tools
Here is the list of tools that you have available to you.
You can ONLY use the tools listed here. When a tool is not listed below, it is NOT available and you should NEVER attempt to use it.
Each tool is represented as a JSON object with fields like "name", "description", "parameters" (per JSON Schema), and optionally, "responses" (per JSON Schema).

```json
[
    {"name": "direct-injected-document", "description": "This is a special tool to directly inject user-uploaded documents into the chat as additional context. DO NOT use this tool by yourself!", "parameters": {"type": "object", "properties": {}, "required": []}, "responses": {"200": {"description": "Successfully returned a list of chunked text snippets from the directly uploaded documents.", "content": {"application/json": {"schema": {"type": "array", "items": {"type": "object", "required": ["url", "snippet"], "properties": {"url": {"type": "string", "description": "The url of the uploaded document."}, "snippet": {"type": "string", "description": "The text snippet for the returned document chunk."}}}}}}}}}
]
```

# Default Preamble
The following instructions are your defaults unless specified elsewhere in developer preamble or user prompt.
- Your name is Command.
- You are a large language model built by Cohere.
- You reply conversationally with a friendly and informative tone and often include introductory statements and follow-up questions.
- If the input is ambiguous, ask clarifying follow-up questions.
- Use Markdown-specific formatting in your response (for example to highlight phrases in bold or italics, create tables, or format code blocks).
- Use LaTeX to generate mathematical notation for complex equations.
- When responding in English, use American English unless context indicates otherwise.
- When outputting responses of more than seven sentences, split the response into paragraphs.
- Prefer the active voice.
- Adhere to the APA style guidelines for punctuation, spelling, hyphenation, capitalization, numbers, lists, and quotation marks. Do not worry about them for other elements such as italics, citations, figures, or references.
- Use gender-neutral pronouns for unspecified persons.
- Limit lists to no more than 10 items unless the list is a set of finite instructions, in which case complete the list.
- Use the third person when asked to write a summary.
- When asked to extract values from source material, use the exact form, separated by commas.
- When generating code output, please provide an explanation after the code.
- When generating code output without specifying the programming language, please generate Python code.
- If you are asked a question that requires reasoning, first think through your answer, slowly and step by step, then answer.<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|USER_TOKEN|>What has Man always dreamed of?<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|><|START_THINKING|>I will look through the document to address the users needs.<|END_THINKING|><|START_ACTION|>[
    {"tool_call_id": "0", "tool_name": "direct-injected-document", "parameters": {}}
]<|END_ACTION|><|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|SYSTEM_TOKEN|><|START_TOOL_RESULT|>[
    {
        "tool_call_id": "0",
        "results": {
            "0": {"body": "Man has always dreamed of destroying the moon. In this essay, I shall...", "heading": "The Moon: Our Age-Old Foe"},
            "1": {"body": "Man's dream has always been to find love. This profound lesson...", "heading": "Love is all you need"},
            "2": {"body": "Although often underappreciated, the sun provides several notable benefits...", "heading": "The Sun: Our Age-Old Friend"}
        },
        "is_error": null
    }
]<|END_TOOL_RESULT|><|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>
````
</Accordion>

<Accordion title='Example of Grounded Generation Completion with citations'>

````json
There are two answers to this question. Man has dreamed of <co>destroying the moon</co: 0:[0]> and <co>finding love.</co: 0:[1]>
````
If citations are turned on, then the model will associate pieces of texts (called "spans") with specific document snippets that support them (called "sources"). Command R7B uses a pair of tags `<co>` and `</co>` to indicate when a span can be grounded onto a list of sources. For example, `<co>span</co: 0:[0,1]>` means that "span" is supported by documents snippets 0 and 1 that were provided in the last message.

</Accordion>

### Option 2: Regular Generation

You may find that simply including relevant documents directly in a user message works just as well, or better than using the documents parameter to render the special grounded generation template. Grounded Generation is generally a strong default and is ideal for users wanting citations, but Regular Generation can offer more control and customization over the prompt, at the cost of some effort to find an optimal prompt. We encourage users to play with both Grounded Generation and Regular Generation, and to evaluate which mode works best for their specific use case.

## Tool use, Function Calling & Agent capabilities

Command R7B has been specifically trained with conversational tool use capabilities. This allows the model to interact with external tools like APIs, databases, or search engines. These capabilities have been trained into the model via a mixture of supervised fine-tuning and preference fine-tuning, using a specific prompt template. Deviating from this prompt template will likely reduce performance, but we encourage experimentation. 

These tool use capabilities unlock two use cases:
- **Function Calling**: A single inference where Command R7B selects relevant tools to fulfill a user request.
- **Agents**: Several inference cycles where Command R7B iterates through Plan â†’ Action â†’ Observation loops until it arrives at a final response.

Both work in the same way. Given a conversation as input (with an optional preamble), along with a list of available tools, the model will generate one of the following:
- **Tool Selection**: A high-level plan followed by a json-formatted list of actions to execute on a subset of the supplied tools. Command R7B may select multiple tools in parallel, and it may select a tool more than once. It is then up to the developer to execute these tool calls and obtain tool results.
- **Response**: A final response to the user. This can occur if the model chooses not to use any tools, such as when greeting the user or asking clarifying questions, or after processing tool results to formulate a final answer. 

Optionally, one can ask the model to include grounding spans (citations) in its response to indicate the source of the information. 

Tool use in Command R7B is supported through [chat templates](https://huggingface.co/docs/transformers/main/en/chat_templating#advanced-tool-use--function-calling) in Transformers. We recommend providing tool descriptions using JSON schema. Here is a quick example showing Tool Use.


<Accordion title='Usage: Generate the Tool Use prompt'>

````python PYTHON
from transformers import AutoTokenizer, AutoModelForCausalLM

# Load the model and tokenizer
model_id = "CohereForAI/c4ai-command-r7b-12-2024"
tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(model_id)

# Define conversation input
conversation = [{"role": "user", "content": "Can you provide a sales summary for 29th September 2023, and also give me some details about the products in the 'Electronics' category, for example their prices and stock levels?"}]

# Define tools
tools = [
    { 
"type": "function", 
"function": {
"name": "query_daily_sales_report",
"description": "Connects to a database to retrieve overall sales volumes and sales information for a given day.",
"parameters": {
                    "type": "object",
                    "properties": {
                        "day": {
                            "description": "Retrieves sales data for this day, formatted as YYYY-MM-DD.",
                            "type": "string",
                        }
                    },
                    "required": ["day"]
                },
}
    }, 
    { 
"type": "function", 
"function": {
"name": "query_product_catalog",
"description": "Connects to a a product catalog with information about all the products being sold, including categories, prices, and stock levels.",
"parameters": {
                    "type": "object",
                    "properties": {
                        "category": {
                            "description": "Retrieves product information data for all products in this category.",
                            "type": "string",
                        }
                    },
                    "required": ["category"]
                },
}
    }
]

# Get the Tool Use prompt
input_prompt = tokenizer.apply_chat_template(
    conversation=conversation,
    tools=tools,
    tokenize=False,
    add_generation_prompt=True,
    return_tensors="pt",
    # enable_citations=True, # optional citations
)
print("== Prompt for step 1 of the Agent:", input_prompt)

# Tokenize the prompt
input_ids = tokenizer.encode_plus(input_prompt, return_tensors="pt")

# Generate a response 
gen_tokens = model.generate(
    input_ids,
    max_new_tokens=512,
    do_sample=True,
    temperature=0.3,
    skip_special_tokens=True,
)

# Decode and print the generated text along with generation prompt
gen_text = tokenizer.decode(gen_tokens[0])
print(gen_text)
````
</Accordion>

<Accordion title='Example of a Tool Use prompt'>
````json
<|START_OF_TURN_TOKEN|><|SYSTEM_TOKEN|># System Preamble
You are in contextual safety mode. You will reject requests to generate child sexual abuse material and child exploitation material in your responses. You will accept to provide information and creative content related to violence, hate, misinformation or sex, but you will not provide any content that could directly or indirectly lead to harmful outcomes.

Your information cutoff date is June 2024.

You have been trained on data in English, French, Spanish, Italian, German, Portuguese, Japanese, Korean, Modern Standard Arabic, Mandarin, Russian, Indonesian, Turkish, Dutch, Polish, Persian, Vietnamese, Czech, Hindi, Ukrainian, Romanian, Greek and Hebrew but have the ability to speak many more languages.

You have been trained to have advanced reasoning and tool-use capabilities and you should make best use of these skills to serve user's requests.

## Tool Use
Think about how you can make best use of the provided tools to help with the task and come up with a high level plan that you will execute first.

0. Start by writing <|START_THINKING|> followed by a detailed step by step plan of how you will solve the problem. For each step explain your thinking fully and give details of required tool calls (if needed). Unless specified otherwise, you write your plan in natural language. When you finish, close it out with <|END_THINKING|>.
    You can optionally choose to skip this step when the user request is so straightforward to address that only a trivial plan would be needed.
    NOTE: You MUST skip this step when you are directly responding to the user's request without using any tools.

Then carry out your plan by repeatedly executing the following steps.
1. Action: write <|START_ACTION|> followed by a list of JSON-formatted tool calls, with each one containing "tool_name" and "parameters" fields.
    When there are multiple tool calls which are completely independent of each other (i.e. they can be executed in parallel), you should list them out all together in one step. When you finish, close it out with <|END_ACTION|>.
2. Observation: you will then receive results of those tool calls in JSON format in the very next turn, wrapped around by <|START_TOOL_RESULT|> and <|END_TOOL_RESULT|>. Carefully observe those results and think about what to do next. Note that these results will be provided to you in a separate turn. NEVER hallucinate results.
    Every tool call produces a list of results (when a tool call produces no result or a single result, it'll still get wrapped inside a list). Each result is clearly linked to its originating tool call via its "tool_call_id".
3. Reflection: start the next turn by writing <|START_THINKING|> followed by what you've figured out so far, any changes you need to make to your plan, and what you will do next. When you finish, close it out with <|END_THINKING|>.
    You can optionally choose to skip this step when everything is going according to plan and no special pieces of information or reasoning chains need to be recorded.
    NOTE: You MUST skip this step when you are done with tool-use actions and are ready to respond to the user.

You can repeat the above 3 steps multiple times (could be 0 times too if no suitable tool calls are available or needed), until you decide it's time to finally respond to the user.

4. Response: then break out of the loop and write <|START_RESPONSE|> followed by a piece of text which serves as a response to the user's last request. Use all previous tool calls and results to help you when formulating your response. When you finish, close it out with <|END_RESPONSE|>.

## Available Tools
Here is the list of tools that you have available to you.
You can ONLY use the tools listed here. When a tool is not listed below, it is NOT available and you should NEVER attempt to use it.
Each tool is represented as a JSON object with fields like "name", "description", "parameters" (per JSON Schema), and optionally, "responses" (per JSON Schema).

```json
[
    {"name": "query_daily_sales_report", "description": "Connects to a database to retrieve overall sales volumes and sales information for a given day.", "parameters": {"type": "object", "properties": {"day": {"type": "string", "description": "Retrieves sales data for this day, formatted as YYYY-MM-DD."}}, "required": ["day"]}, "responses": null},
    {"name": "query_product_catalog", "description": "Connects to a a product catalog with information about all the products being sold, including categories, prices, and stock levels.", "parameters": {"type": "object", "properties": {"category": {"type": "string", "description": "Retrieves product information data for all products in this category."}}, "required": ["category"]}, "responses": null}
]
```

# Default Preamble
The following instructions are your defaults unless specified elsewhere in developer preamble or user prompt.
- Your name is Command.
- You are a large language model built by Cohere.
- You reply conversationally with a friendly and informative tone and often include introductory statements and follow-up questions.
- If the input is ambiguous, ask clarifying follow-up questions.
- Use Markdown-specific formatting in your response (for example to highlight phrases in bold or italics, create tables, or format code blocks).
- Use LaTeX to generate mathematical notation for complex equations.
- When responding in English, use American English unless context indicates otherwise.
- When outputting responses of more than seven sentences, split the response into paragraphs.
- Prefer the active voice.
- Adhere to the APA style guidelines for punctuation, spelling, hyphenation, capitalization, numbers, lists, and quotation marks. Do not worry about them for other elements such as italics, citations, figures, or references.
- Use gender-neutral pronouns for unspecified persons.
- Limit lists to no more than 10 items unless the list is a set of finite instructions, in which case complete the list.
- Use the third person when asked to write a summary.
- When asked to extract values from source material, use the exact form, separated by commas.
- When generating code output, please provide an explanation after the code.
- When generating code output without specifying the programming language, please generate Python code.
- If you are asked a question that requires reasoning, first think through your answer, slowly and step by step, then answer.<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|USER_TOKEN|>Can you provide a sales summary for 29th September 2023, and also give me some details about the products in the 'Electronics' category, for example their prices and stock levels?<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>
````
</Accordion>

<Accordion title='Example of a Tool Use completion'>
In this case, the model decides to select tools.

````json
<|START_THINKING|>I will use the query_daily_sales_report tool to find the sales summary for 29th September 2023. I will then use the query_product_catalog tool to find the details about the products in the 'Electronics' category.<|END_THINKING|><|START_ACTION|>[
    {"tool_call_id": "0", "tool_name": "query_daily_sales_report", "parameters": {"day": "2023-09-29"}},
    {"tool_call_id": "1", "tool_name": "query_product_catalog", "parameters": {"category": "Electronics"}}
]<|END_ACTION|>
````

Below is what the answer would have looked like, if the model had decided to respond directly, for example to ask the user a follow up question.

````json
I can find the sales summary for 29th September 2023 as well as the details about the products in the 'Electronics' category. However, I need to use the 'query_daily_sales_report' and 'query_product_catalog' tools to do this. Are you sure you would you like me to use these tools?
````

</Accordion>

If the model generates tool calls, you should add them to the chat history like so:


````python PYTHON
conversation.append({
    "role": "assistant", 
    "tool_calls": [
        {
    "type": "function", 
    "function": {"id": "0", "tool_name": "query_daily_sales_report", "parameters": {"day": "2023-09-29"}
        },
        {
    "type": "function", 
    "function": {"id": "1", "tool_name": "query_product_catalog", "parameters": {"category": "Electronics"}
        }

    ],
    "tool_plan": "I will use the query_daily_sales_report tool to find the sales summary for 29th September 2023. I will then use the query_product_catalog tool to find the details about the products in the 'Electronics' category."
})
````

and then call the tool and append the result, with the tool role, like so:

````python PYTHON
api_response_for_query_daily_sales_report = SOME JSON RESPONSE
api_response_for_query_product_catalog = SOME JSON RESPONSE

# Append tool results from tool call 0
conversation.append({
"role": "tool", 
"tool_call_id": "0", 
"content": json.dumps(api_response_for_query_daily_sales_report)
})

# Append tool results from tool call 1
conversation.append({
"role": "tool", 
"tool_call_id": "1", 
"content": json.dumps(api_response_for_query_product_catalog)
})
````

After that, you can `generate()` again to let the model use the tool result in the chat.

<Accordion title='Usage: Generate the Tool Use prompt with tool results in the conversation'>

````python PYTHON
from transformers import AutoTokenizer, AutoModelForCausalLM

# Load the model and tokenizer
model_id = "CohereForAI/c4ai-command-r7b-12-2024"
tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(model_id)

# Get the Tool Use prompt
input_prompt = tokenizer.apply_chat_template(
    conversation=conversation,
    tools=tools,
    tokenize=False,
    add_generation_prompt=True,
    return_tensors="pt",
    # enable_citations=True, # optional citations
)
print("== Prompt for step 2 of the Agent:", input_prompt)

# Tokenize the prompt
input_ids = tokenizer.encode_plus(input_prompt, return_tensors="pt")

# Generate a response 
gen_tokens = model.generate(
    input_ids,
    max_new_tokens=512,
    do_sample=True,
    temperature=0.3,
    skip_special_tokens=True,
)

# Decode and print the generated text along with generation prompt
gen_text = tokenizer.decode(gen_tokens[0])
print(gen_text)
````

</Accordion>

<Accordion title='Example of a Tool Use prompt with tool results in the conversation'>
````json
<|START_OF_TURN_TOKEN|><|SYSTEM_TOKEN|># System Preamble
You are in contextual safety mode. You will reject requests to generate child sexual abuse material and child exploitation material in your responses. You will accept to provide information and creative content related to violence, hate, misinformation or sex, but you will not provide any content that could directly or indirectly lead to harmful outcomes.

Your information cutoff date is June 2024.

You have been trained on data in English, French, Spanish, Italian, German, Portuguese, Japanese, Korean, Modern Standard Arabic, Mandarin, Russian, Indonesian, Turkish, Dutch, Polish, Persian, Vietnamese, Czech, Hindi, Ukrainian, Romanian, Greek and Hebrew but have the ability to speak many more languages.

You have been trained to have advanced reasoning and tool-use capabilities and you should make best use of these skills to serve user's requests.

## Tool Use
Think about how you can make best use of the provided tools to help with the task and come up with a high level plan that you will execute first.

0. Start by writing <|START_THINKING|> followed by a detailed step by step plan of how you will solve the problem. For each step explain your thinking fully and give details of required tool calls (if needed). Unless specified otherwise, you write your plan in natural language. When you finish, close it out with <|END_THINKING|>.
    You can optionally choose to skip this step when the user request is so straightforward to address that only a trivial plan would be needed.
    NOTE: You MUST skip this step when you are directly responding to the user's request without using any tools.

Then carry out your plan by repeatedly executing the following steps.
1. Action: write <|START_ACTION|> followed by a list of JSON-formatted tool calls, with each one containing "tool_name" and "parameters" fields.
    When there are multiple tool calls which are completely independent of each other (i.e. they can be executed in parallel), you should list them out all together in one step. When you finish, close it out with <|END_ACTION|>.
2. Observation: you will then receive results of those tool calls in JSON format in the very next turn, wrapped around by <|START_TOOL_RESULT|> and <|END_TOOL_RESULT|>. Carefully observe those results and think about what to do next. Note that these results will be provided to you in a separate turn. NEVER hallucinate results.
    Every tool call produces a list of results (when a tool call produces no result or a single result, it'll still get wrapped inside a list). Each result is clearly linked to its originating tool call via its "tool_call_id".
3. Reflection: start the next turn by writing <|START_THINKING|> followed by what you've figured out so far, any changes you need to make to your plan, and what you will do next. When you finish, close it out with <|END_THINKING|>.
    You can optionally choose to skip this step when everything is going according to plan and no special pieces of information or reasoning chains need to be recorded.
    NOTE: You MUST skip this step when you are done with tool-use actions and are ready to respond to the user.

You can repeat the above 3 steps multiple times (could be 0 times too if no suitable tool calls are available or needed), until you decide it's time to finally respond to the user.

4. Response: then break out of the loop and write <|START_RESPONSE|> followed by a piece of text which serves as a response to the user's last request. Use all previous tool calls and results to help you when formulating your response. When you finish, close it out with <|END_RESPONSE|>.

## Available Tools
Here is the list of tools that you have available to you.
You can ONLY use the tools listed here. When a tool is not listed below, it is NOT available and you should NEVER attempt to use it.
Each tool is represented as a JSON object with fields like "name", "description", "parameters" (per JSON Schema), and optionally, "responses" (per JSON Schema).

```json
[
    {"name": "query_daily_sales_report", "description": "Connects to a database to retrieve overall sales volumes and sales information for a given day.", "parameters": {"type": "object", "properties": {"day": {"type": "string", "description": "Retrieves sales data for this day, formatted as YYYY-MM-DD."}}, "required": ["day"]}, "responses": null},
    {"name": "query_product_catalog", "description": "Connects to a a product catalog with information about all the products being sold, including categories, prices, and stock levels.", "parameters": {"type": "object", "properties": {"category": {"type": "string", "description": "Retrieves product information data for all products in this category."}}, "required": ["category"]}, "responses": null}
]
```

# Default Preamble
The following instructions are your defaults unless specified elsewhere in developer preamble or user prompt.
- Your name is Command.
- You are a large language model built by Cohere.
- You reply conversationally with a friendly and informative tone and often include introductory statements and follow-up questions.
- If the input is ambiguous, ask clarifying follow-up questions.
- Use Markdown-specific formatting in your response (for example to highlight phrases in bold or italics, create tables, or format code blocks).
- Use LaTeX to generate mathematical notation for complex equations.
- When responding in English, use American English unless context indicates otherwise.
- When outputting responses of more than seven sentences, split the response into paragraphs.
- Prefer the active voice.
- Adhere to the APA style guidelines for punctuation, spelling, hyphenation, capitalization, numbers, lists, and quotation marks. Do not worry about them for other elements such as italics, citations, figures, or references.
- Use gender-neutral pronouns for unspecified persons.
- Limit lists to no more than 10 items unless the list is a set of finite instructions, in which case complete the list.
- Use the third person when asked to write a summary.
- When asked to extract values from source material, use the exact form, separated by commas.
- When generating code output, please provide an explanation after the code.
- When generating code output without specifying the programming language, please generate Python code.
- If you are asked a question that requires reasoning, first think through your answer, slowly and step by step, then answer.<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|USER_TOKEN|>Can you provide a sales summary for 29th September 2023, and also give me some details about the products in the 'Electronics' category, for example their prices and stock levels?<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|><|START_THINKING|>I will use the query_daily_sales_report tool to find the sales summary for 29th September 2023. I will then use the query_product_catalog tool to find the details about the products in the 'Electronics' category.<|END_THINKING|><|START_ACTION|>[
    {"tool_call_id": "0", "tool_name": "query_daily_sales_report", "parameters": {"day": "2023-09-29"}},
    {"tool_call_id": "1", "tool_name": "query_product_catalog", "parameters": {"category": "Electronics"}}
]<|END_ACTION|><|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|SYSTEM_TOKEN|><|START_TOOL_RESULT|>[
    {
        "tool_call_id": "0",
        "results": {
            "0": {"date": "2023-09-29", "summary": "Total Sales Amount: 10000, Total Units Sold: 250"}
        },
        "is_error": null
    },
    {
        "tool_call_id": "1",
        "results": {
            "0": {"category": "Electronics", "products": "[{\"name\":\"Smartphone\",\"price\":500,\"product_id\":\"E1001\",\"stock_level\":20},{\"name\":\"Laptop\",\"price\":1000,\"product_id\":\"E1002\",\"stock_level\":15},{\"name\":\"Tablet\",\"price\":300,\"product_id\":\"E1003\",\"stock_level\":25}]"}
        },
        "is_error": null
    }
]<|END_TOOL_RESULT|><|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>
````
</Accordion>


<Accordion title='Example of a completion'>
In this case, the model decides to select tools.

````json
On 29th September 2023, the total sales amount was Â£10000 and the total units sold were 250.

The following products are in the 'Electronics' category:

- Smartphone, Â£500, stock level 20
- Laptop, Â£1000, stock level 15
- Tablet, Â£300, stock level 25
````
</Accordion>

Optionally, one can ask the model to include grounding spans (citations) in its response to indicate the source of the information. 


<Accordion title='Usage: Tool Use with citations'>

In case you had run Tool Use with citations turned on (by using `enable_citations=True` in `tokenizer.apply_chat_template`), the output wouldâ€™ve looked like this: 

````json
On 29th September 2023, the total sales amount was <co>10000</co: 0:[0]> and the total units sold were <co>250.</co: 0:[0]>

Here are some details about the products in the 'Electronics' category:

- <co>Smartphone</co: 1:[0]>: <co>500</co: 1:[0]>
- <co>Laptop</co: 1:[0]>: <co>1000</co: 1:[0]>
- <co>Tablet</co: 1:[0]>: <co>300</co: 1:[0]>

The stock levels for these products are:

- <co>Smartphone</co: 1:[0]>: <co>20</co: 1:[0]>
- <co>Laptop</co: 1:[0]>: <co>15</co: 1:[0]>
- <co>Tablet</co: 1:[0]>: <co>25</co: 1:[0]>
````

When citations are turned on, the model associates pieces of texts (called "spans") with those specific tool results that support them (called "sources"). Command R7B uses a pair of tags `<co>` and `</co>` to indicate when a span can be grounded onto a list of sources, listing them out in the closing tag. For example, `<co>span</co: 0:[1,2],1:[0]>` means that "span" is supported by result 1 and 2 from `tool_call_id=0` as well as result 0 from `tool_call_id=1`. Sources from the same tool call are grouped together and listed as `{tool_call_id}:[{list of result indices}]`, before they are joined together by `,`.

</Accordion>