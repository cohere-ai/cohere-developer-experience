---
title: "Prompting Command R and R+"
slug: "docs/prompting-command-r"

hidden: true
description: "This document provides detailed examples and guidelines on the prompt structure to usse with Command R/R+ across various tasks, including Retrieval-Augmented Generation (RAG), summarization, single-step and multi-step tool use, with comprehensive."
image: "../../../assets/images/b2b492c-cohere_meta_image.jpg"
keywords: "prompt engineering, large language model prompting"

createdAt: "Thu Mar 14 2024 17:14:34 GMT+0000 (Coordinated Universal Time)"
updatedAt: "Mon May 06 2024 19:22:34 GMT+0000 (Coordinated Universal Time)"
---

Effective prompt engineering is crucial to getting the desired performance from large language models (LLMs) like Command R/R+. This process can be time-consuming, especially for complex tasks or when comparing models. To ensure fair comparisons and optimize performance, it’s essential to use the correct special tokens, which may vary between models and significantly impact outcomes.

Each task requires its own prompt template. This document outlines the structure and best practices for the following use cases:
- Retrieval-Augmented Generation (RAG) with Command R/R+
- Summarization with Command R/R+
- Single-Step Tool Use with Command R/R+ (Function Calling)
- Multi-Step Tool Use with Command R/R+ (Agents)

The easiest way to make sure your prompts will work well with Command R/R+ is to use our [tokenizer on Hugging Face](https://huggingface.co/CohereForAI/c4ai-command-r-v01). Today, HuggingFace has prompt templates for Retrieval-Augmented Generation (RAG) and Single-Step Tool Use with Command R/R+ (Function Calling).

We are working on adding prompt templates in HuggingFace for Multi-Step Tool Use with Command R/R+ (Agents).

## High-Level Overview of Prompt Templates

The prompt for Command R/R+ is composed of structured sections, each serving a specific purpose. Below is an overview of the main components. We've color coded the different sections of the prompt to make them easy to pick out and we will go over them in more detail later.

### Augmented Generation Prompt Template (RAG and Summarization)

In RAG, the workflow involves two steps:
1. **Retrieval**: Retrieving the relevant snippets.
2. **Augmented Generation**: Generating a response based on these snippets.

Summarization is very similar to augmented generation: the model takes in some documents and its response (the summary) needs to be conditioned on those documents. 

This way, RAG and Summarization follow a similar prompt template. It is the Augmented Generation prompt template and here's what it looks like at a high level:


> <b>augmented_gen_prompt_template =</b>  
> <b>"""<span class="dark-blue-text">\<BOS_TOKEN></span><span class="brown-text">\<|START_OF_TURN_TOKEN|></span><span class="dark-orange-text">\<|SYSTEM_TOKEN|></span><span class="red-text"># Safety Preamble</span></b>  
> <b><span class="red-text">\{SAFETY_PREAMBLE}</span><br /><br /><span class="dark-green-text"># System Preamble</span></b>    
> <b><span class="green-text">## Basic Rules</span></b>  
> <b><span class="green-text">\{BASIC_RULES}</span><br /><br /><span class="dark-purple-text"># User Preamble</span></b>  
> <b><span class="purple-text">## Task and Context</span></b>   
> <b><span class="purple-text">\{TASK_CONTEXT}</span><br /><br /><span class="dark-sangria-text">## Style Guide</span></b>  
> <b><span class="dark-sangria-text">\{STYLE_GUIDE}</span><span class="brown-text">\<|END_OF_TURN_TOKEN|></span><span class="orange-text">\{CHAT_HISTORY}</span><span class="brown-text">\<|START_OF_TURN_TOKEN|></span><span class="dark-orange-text">\<|SYSTEM_TOKEN|></span><span class="orange-text">\{RETRIEVED_SNIPPETS_FOR_RAG or TEXT_TO_SUMMARIZE}</span><span class="brown-text">\<|END_OF_TURN_TOKEN|></span><span class="brown-text">\<|START_OF_TURN_TOKEN|></span><span class="dark-orange-text">\<|SYSTEM_TOKEN|></span><span class="dark-pink-text">\{INSTRUCTIONS}</span><span class="brown-text">\<|END_OF_TURN_TOKEN|></span><span class="brown-text">\<|START_OF_TURN_TOKEN|></span><span class="dark-orange-text">\<|CHATBOT_TOKEN|></span>"""</b>




We can see that the prompt is set up in a structured way where we have sections for things like the <span class="green-text">basic rules</span> we want the model to follow, the <span class="purple-text">task</span> we want it to solve, and the <span class="magenta-text">style</span> in which it should write its output in.


### Single step Tool Use with Command R/R+ (Function Calling)

Single-step tool use (or "Function Calling") allows Command R/R+ to interact with external tools like APIs, databases, or search engines. Single-step tool use is made of two model inferences:
1. **Tool Selection**: The model decides which tools to call and with what parameters. It’s then up to the developer to execute these tool calls and obtain tool results.
2. **Response Generation**: The model generates the final response given the tool results.

You can learn more about single step tool use [in our documentation](https://docs.cohere.com/docs/tool-use). Let’s go over the prompt template for Tool Section, and for Response Generation.

#### A) Tool Selection Prompt Template 

> <b>singlestep_tool_selection_prompt_template =</b>  
> <b>"""<span class="dark-blue-text">\<BOS_TOKEN></span><span class="brown-text">\<|START_OF_TURN_TOKEN|></span><span class="dark-orange-text">\<|SYSTEM_TOKEN|></span><span class="red-text"># Safety Preamble</span></b>  
> <b><span class="red-text">\{SAFETY_PREAMBLE}</span><br /><br /><span class="dark-green-text"># System Preamble</span></b>  
> <b><span class="green-text">## Basic Rules</span></b>  
> <b><span class="green-text">\{BASIC_RULES}</span><br /><br /><span class="dark-purple-text"># User Preamble</span></b>  
> <b><span class="purple-text">## Task and Context</span></b>   
> <b><span class="purple-text">\{TASK_CONTEXT}</span><br /><br /><span class="dark-sangria-text">## Style Guide</span></b>  
> <b><span class="dark-sangria-text">\{STYLE_GUIDE}</span><br /><br /><span class="magenta-text">## Available Tools</span></b>  
> <b><span class="magenta-text">\{TOOLS}</span><span class="brown-text">\<|END_OF_TURN_TOKEN|></span><span class="orange-text">\{CHAT_HISTORY}</span><span class="brown-text">\<|START_OF_TURN_TOKEN|></span><span class="dark-orange-text">\<|SYSTEM_TOKEN|></span><span class="dark-pink-text">\{INSTRUCTIONS_FOR_SINGLE_STEP_TOOL_USE}</span><span class="brown-text">\<|END_OF_TURN_TOKEN|></span><span class="brown-text">\<|START_OF_TURN_TOKEN|></span><span class="dark-orange-text">\<|CHATBOT_TOKEN|></span>"""</b>


The prompt template for Tool Selection is similar to the Augmented Generation prompt template. There are, however, two spots that differ which are highlighted. The first is that we have added the tool definitions which come after the style guide (you can see that there’s now an <span class="magenta-text">## Available Tools</span> section), and the second is that we’ve removed the section with the retrieved snippets or text to summarize.


#### B) Response Generation Template

At this point, Command R/R+ has decided which tools to call and with what parameters (see previous section). Developers are expected to execute these tool calls, and to receive tool results in return. 

In this Response Generation step, the goal is to generate the final model response, given the tool results. This is another case of… Augmented Generation!

Therefore, the prompt template is very similar to the augmented generation prompt used for RAG and Summarization. The only difference is that we replace the RAG snippets and/or text to summarize with tool outputs (TOOL_OUTPUTS).

> <b>singlestep_augmented_generation_prompt_template =</b>  
> <b>"""<span class="dark-blue-text">\<BOS_TOKEN></span><span class="brown-text">\<|START_OF_TURN_TOKEN|></span><span class="dark-orange-text">\<|SYSTEM_TOKEN|></span><span class="red-text"># Safety Preamble</span></b>  
> <b><span class="red-text">\{SAFETY_PREAMBLE}</span><br /><br /><span class="dark-green-text"># System Preamble</span></b>  
> <b><span class="green-text">## Basic Rules</span></b>  
> <b><span class="green-text">\{BASIC_RULES}</span><br /><br /><span class="dark-purple-text"># User Preamble</span></b>  
> <b><span class="purple-text">## Task and Context</span></b>   
> <b><span class="purple-text">\{TASK_CONTEXT}</span><br /><br /><span class="dark-sangria-text">## Style Guide</span></b>  
> <b><span class="dark-sangria-text">\{STYLE_GUIDE}</span><span class="brown-text">\<|END_OF_TURN_TOKEN|></span><span class="orange-text">\{CHAT_HISTORY}</span><span class="brown-text">\<|START_OF_TURN_TOKEN|></span><span class="dark-orange-text">\<|SYSTEM_TOKEN|></span><span class="dark-red-text">\{TOOL_OUTPUTS}</span><span class="brown-text">\<|END_OF_TURN_TOKEN|></span><span class="brown-text">\<|START_OF_TURN_TOKEN|></span><span class="dark-orange-text">\<|SYSTEM_TOKEN|></span><span class="dark-pink-text">\{INSTRUCTIONS_FOR_SINGLE_STEP_TOOL_USE}</span><span class="brown-text">\<|END_OF_TURN_TOKEN|></span><span class="brown-text">\<|START_OF_TURN_TOKEN|></span><span class="dark-orange-text">\<|CHATBOT_TOKEN|></span>"""</b>



### Multi step Tool Use with Command R/R+ (Agents)

Multi-step tool use is suited for building agents that can plan and execute a sequence of actions using multiple tools. Unlike single-step tool use, the model can perform several inference cycles, iterating through Action → Observation → Reflection until it decides on a final response. For more details, refer to our [documentation on multi-step tool use](https://docs.cohere.com/docs/multi-step-tool-use). 

To understand the multistep tool use prompt, let’s look at the following prompts
- The prompt template for step 1 of the agent
- The prompt template for step 2 of the agent
- The prompt template at step i of the agent


#### A) Prompt template for Step 1 of the agent

> <b>multistep_tooluse_step_1_prompt_template =</b>  
> <b>"""<span class="dark-blue-text">\<BOS_TOKEN></span><span class="brown-text">\<|START_OF_TURN_TOKEN|></span><span class="dark-orange-text">\<|SYSTEM_TOKEN|></span><span class="red-text"># Safety Preamble</span></b>  
> <b><span class="red-text">\{SAFETY_PREAMBLE}</span><br /><br /><span class="dark-green-text"># System Preamble</span></b>  
> <b><span class="green-text">## Basic Rules</span></b>  
> <b><span class="green-text">\{BASIC_RULES}</span><br /><br /><span class="dark-purple-text"># User Preamble</span></b>  
> <b><span class="purple-text">## Task and Context</span></b>   
> <b><span class="purple-text">\{TASK_CONTEXT}</span><br /><br /><span class="dark-sangria-text">## Style Guide</span></b>  
> <b><span class="dark-sangria-text">\{STYLE_GUIDE}</span><br /><br /><span class="magenta-text">## Available Tools</span></b>  
> <b><span class="magenta-text">\{TOOLS}</span><span class="brown-text">\<|END_OF_TURN_TOKEN|></span><span class="orange-text">\{CHAT_HISTORY}</span><span class="brown-text">\<|START_OF_TURN_TOKEN|></span><span class="dark-orange-text">\<|SYSTEM_TOKEN|></span><span class="dark-pink-text">\{INSTRUCTIONS_FOR_MULTI_STEP_TOOL_USE}</span><span class="brown-text">\<|END_OF_TURN_TOKEN|></span><span class="brown-text">\<|START_OF_TURN_TOKEN|></span><span class="dark-orange-text">\<|CHATBOT_TOKEN|></span>"""</b>


In this first step of the agent, the model generates an initial plan and suggests tool calls. Developers are expected to execute these tool calls, and to receive tool results in return.



#### B) Prompt template for subsequent steps of the agent

As the process continues to step 2 (or any subsequent step), the model evaluates the tool results from the previous step, self-reflects and updates its plan. It may choose to call additional tools or decide that it has gathered enough information to provide a final response.

This iterative process continues for as many steps as the model deems necessary. 

**Here is the template for Step 2 of the agent:**

> <b>multistep_tooluse_step_2_prompt_template =</b>  
> <b>"""<span class="dark-blue-text">\<BOS_TOKEN></span><span class="brown-text">\<|START_OF_TURN_TOKEN|></span><span class="dark-orange-text">\<|SYSTEM_TOKEN|></span><span class="red-text"># Safety Preamble</span></b>  
> <b><span class="red-text">\{SAFETY_PREAMBLE}</span><br /><br /><span class="dark-green-text"># System Preamble</span></b>  
> <b><span class="green-text">## Basic Rules</span></b>  
> <b><span class="green-text">\{BASIC_RULES}</span><br /><br /><span class="dark-purple-text"># User Preamble</span></b>  
> <b><span class="purple-text">## Task and Context</span></b>   
> <b><span class="purple-text">\{TASK_CONTEXT}</span><br /><br /><span class="dark-sangria-text">## Style Guide</span></b>  
> <b><span class="dark-sangria-text">\{STYLE_GUIDE}</span><br /><br /><span class="magenta-text">## Available Tools</span></b>  
> <b><span class="magenta-text">\{TOOLS}</span><span class="brown-text">\<|END_OF_TURN_TOKEN|></span><span class="orange-text">\{CHAT_HISTORY}</span><span class="brown-text">\<|START_OF_TURN_TOKEN|></span><span class="dark-orange-text">\<|SYSTEM_TOKEN|></span><span class="dark-pink-text">\{INSTRUCTIONS_FOR_MULTI_STEP_TOOL_USE}</span><span class="brown-text">\<|END_OF_TURN_TOKEN|></span><span class="brown-text">\<|START_OF_TURN_TOKEN|></span><span class="dark-orange-text">\<|CHATBOT_TOKEN|></span><span class="dark-red-text">\{PLAN_AND_SUGGESTED_TOOL_CALLS_FOR_STEP_1}</span><span class="brown-text">\<|END_OF_TURN_TOKEN|></span><span class="brown-text">\<|START_OF_TURN_TOKEN|></span><span class="dark-orange-text">\<|SYSTEM_TOKEN|></span><span class="dark-red-text">\{TOOL_RESULTS_FROM_STEP_1}</span><span class="brown-text">\<|END_OF_TURN_TOKEN|></span><span class="brown-text">\<|START_OF_TURN_TOKEN|></span><span class="dark-orange-text">\<|CHATBOT_TOKEN|></span>"""</b>




**Here is the template for Step i of the agent:**


> <b>multistep_tooluse_step_i_prompt_template =</b>  
> <b>"""<span class="dark-blue-text">\<BOS_TOKEN></span><span class="brown-text">\<|START_OF_TURN_TOKEN|></span><span class="dark-orange-text">\<|SYSTEM_TOKEN|></span><span class="red-text"># Safety Preamble</span></b>  
> <b><span class="red-text">\{SAFETY_PREAMBLE}</span><br /><br /><span class="dark-green-text"># System Preamble</span></b>  
> <b><span class="green-text">## Basic Rules</span></b>  
> <b><span class="green-text">\{BASIC_RULES}</span><br /><br /><span class="dark-purple-text"># User Preamble</span></b>  
> <b><span class="purple-text">## Task and Context</span></b>   
> <b><span class="purple-text">\{TASK_CONTEXT}</span><br /><br /><span class="dark-sangria-text">## Style Guide</span></b>  
> <b><span class="dark-sangria-text">\{STYLE_GUIDE}</span><br /><br /><span class="magenta-text">## Available Tools</span></b>  
> <b><span class="magenta-text">\{TOOLS}</span><span class="brown-text">\<|END_OF_TURN_TOKEN|></span><span class="orange-text">\{CHAT_HISTORY}</span><span class="brown-text">\<|START_OF_TURN_TOKEN|></span><span class="dark-orange-text">\<|SYSTEM_TOKEN|></span><span class="dark-pink-text">\{INSTRUCTIONS_FOR_MULTI_STEP_TOOL_USE}</span><span class="brown-text">\<|END_OF_TURN_TOKEN|></span><span class="brown-text">\<|START_OF_TURN_TOKEN|></span><span class="dark-orange-text">\<|CHATBOT_TOKEN|></span><span class="dark-red-text">\{PLAN_AND_SUGGESTED_TOOL_CALLS_FOR_STEP_1}</span><span class="brown-text">\<|END_OF_TURN_TOKEN|></span><span class="brown-text">\<|START_OF_TURN_TOKEN|></span><span class="dark-orange-text">\<|SYSTEM_TOKEN|></span><span class="dark-red-text">\{TOOL_RESULTS_FROM_STEP_1}</span><span class="brown-text">\<|END_OF_TURN_TOKEN|></span><span class="brown-text">\<|START_OF_TURN_TOKEN|></span><span class="dark-orange-text">\<|CHATBOT_TOKEN|></span><span class="dark-red-text">\{PLAN_AND_SUGGESTED_TOOL_CALLS_FOR_STEP_2}</span><span class="brown-text">\<|END_OF_TURN_TOKEN|></span><span class="brown-text">\<|START_OF_TURN_TOKEN|></span><span class="dark-orange-text">\<|SYSTEM_TOKEN|></span><span class="dark-red-text">\{TOOL_RESULTS_FROM_STEP_2}</span><span class="brown-text">\<|END_OF_TURN_TOKEN|></span><span class="brown-text">\<|START_OF_TURN_TOKEN|></span><span class="dark-orange-text">\<|CHATBOT_TOKEN|></span><span class="dark-red-text">\{PLAN_AND_SUGGESTED_TOOL_CALLS_FOR_STEP_3}</span><span class="brown-text">\<|END_OF_TURN_TOKEN|></span><span class="brown-text">\<|START_OF_TURN_TOKEN|></span><span class="dark-orange-text">\<|SYSTEM_TOKEN|></span><span class="dark-red-text">\{TOOL_RESULTS_FROM_STEP_3}</span><span class="brown-text">\<|END_OF_TURN_TOKEN|></span>...etc...<span class="brown-text">\<|START_OF_TURN_TOKEN|></span><span class="dark-orange-text">\<|CHATBOT_TOKEN|></span><span class="dark-red-text">\{PLAN_AND_SUGGESTED_TOOL_CALLS_FOR_STEP_i-1}</span><span class="brown-text">\<|END_OF_TURN_TOKEN|></span><span class="brown-text">\<|START_OF_TURN_TOKEN|></span><span class="dark-orange-text">\<|SYSTEM_TOKEN|></span><span class="dark-red-text">\{TOOL_RESULTS_FROM_STEP_i-1}</span><span class="brown-text">\<|END_OF_TURN_TOKEN|></span><span class="brown-text">\<|START_OF_TURN_TOKEN|></span><span class="dark-orange-text">\<|CHATBOT_TOKEN|></span>"""</b>


## Detailed Prompt Templates

Now that we have a high-level understanding of prompt templates, let's dive into the detailed prompts for each task. 

### Augmented Generation: RAG with Command R/R+

Retrieval Augmented Generation (RAG) involves two main steps: 
- Retrieval: retrieve the relevant snippets
- Augmented Generation: generate a response based on these snippets. 

Below is a detailed look at the fully rendered prompt for Augmented Generation. You can achieve the same result using the Hugging Face Tokenizer's **apply_grounded_generation_template()** function.

The chat history in this example, is the simplest it can be: the user question only.

> <b><span class="orange-text">CHAT_HISTORY</span> = "<span class="brown-text">\<|START_OF_TURN_TOKEN|></span><span class="blue-text">\<|USER_TOKEN|></span><span class="orange-text">Where do the tallest penguins live?</span><span class="brown-text">\<|END_OF_TURN_TOKEN|></span>"</b>

The retrieved snippets for RAG should be wrapped in <b><span class="brown-text">\<|START_OF_TURN_TOKEN|></span><span class="dark-orange-text">\<|SYSTEM_TOKEN|></span><span class="orange-text">\{RETRIEVED_SNIPPETS_FOR_RAG}</span><span class="brown-text">\<|END_OF_TURN_TOKEN|></span></b> and look something like this: 


> <b><span class="brown-text">RETRIEVED_SNIPPETS_FOR_RAG =</span></b>  
> <b>"""<span class="dark-red-text">\<results></span></b>  
> <b><span class="dark-red-text">Document: 0</span></b>  
> <b><span class="dark-red-text">title: Tall penguins</span></b>  
> <b><span class="dark-red-text">snippet: Emperor penguins are the tallest growing up to 122 cm in height.<br /><br />Document: 1</span></b>  
> <b><span class="dark-red-text">title: Penguin habitats</span></b>  
> <b><span class="dark-red-text">snippet: Emperor penguins only live in Antarctica.</span></b>  
> <b><span class="dark-red-text">\</results></span>"""</b>

Each chunk should start with <b><span class="dark-red-text">Document: \{n}</span></b> and should be an ascending list of integers starting at 0.

Below is a detailed look at the fully rendered prompt for Augmented Generation.

> <b>RAG_augmented_generation_prompt_template =</b>  
> <b>"""<span class="dark-blue-text">\<BOS_TOKEN></span><span class="brown-text">\<|START_OF_TURN_TOKEN|></span><span class="dark-orange-text">\<|SYSTEM_TOKEN|></span><span class="red-text"># Safety Preamble</span></b>  
> <b><span class="red-text">The instructions in this section override those in the task description and style guide sections. Don't answer questions that are harmful or immoral</span><br /><br /><span class="dark-green-text"># System Preamble</span></b>    
> <b><span class="green-text">## Basic Rules</span></b>  
> <b><span class="green-text">You are a powerful conversational AI trained by Cohere to help people. You are augmented by a number of tools, and your job is to use and consume the output of these tools to best help the user. You will see a conversation history between yourself and a user, ending with an utterance from the user. You will then see a specific instruction instructing you what kind of response to generate. When you answer the user's requests, you cite your sources in your answers, according to those instructions.</span><br /><br /><span class="dark-purple-text"># User Preamble</span></b>  
> <b><span class="purple-text">## Task and Context</span></b>  
> <b><span class="purple-text">You help people answer their questions and other requests interactively. You will be asked a very wide array of requests on all kinds of topics. You will be equipped with a wide range of search engines or similar tools to help you, which you use to research your answer. You should focus on serving the user's needs as best you can, which will be wide-ranging.</span><br /><br /><span class="dark-sangria-text">## Style Guide</span></b>  
> <b><span class="dark-sangria-text">Unless the user asks for a different style of answer, you should answer in full sentences, using proper grammar and spelling.</span><span class="brown-text">\<|END_OF_TURN_TOKEN|></span><span class="brown-text">\<|START_OF_TURN_TOKEN|></span><span class="dark-orange-text">\<|USER_TOKEN|></span>Where do the tallest penguins live?<span class="brown-text">\<|END_OF_TURN_TOKEN|></span><span class="brown-text">\<|START_OF_TURN_TOKEN|></span><span class="dark-orange-text">\<|SYSTEM_TOKEN|></span><span class="dark-red-text">\<results></span></b>  
> <b><span class="dark-red-text">Document: 0</span></b>  
> <b><span class="dark-red-text">title: Tall penguins</span></b>  
> <b><span class="dark-red-text">snippet: Emperor penguins are the tallest growing up to 122 cm in height.<br /><br />Document: 1</span></b>  
> <b><span class="dark-red-text">title: Penguin habitats</span></b>  
> <b><span class="dark-red-text">snippet: Emperor penguins only live in Antarctica.</span></b>  
> <b><span class="dark-red-text">\</results></span><span class="brown-text">\<|END_OF_TURN_TOKEN|></span><span class="brown-text">\<|START_OF_TURN_TOKEN|></span><span class="dark-orange-text">\<|SYSTEM_TOKEN|></span><span class="dark-pink-text">Carefully perform the following instructions, in order, starting each with a new line.</span></b>  
> <b><span class="dark-pink-text">Write 'Grounded answer:' followed by a response to the user's last input in high quality natural english. Use square brackets to indicate a citation from the search results, e.g. "my fact [0]" for a fact from document 0.</span><span class="brown-text">\<|END_OF_TURN_TOKEN|></span><span class="brown-text">\<|START_OF_TURN_TOKEN|></span><span class="dark-orange-text">\<|CHATBOT_TOKEN|></span>"""</b>


And this results in the model output:

> <b><span class="green-text">Grounded answer: The tallest penguins are Emperor penguins [0], which grow up to 122 cm in height. [0] They live only in Antarctica. [1]</span></b>




