---
title: How to Get Predictable Outputs with Cohere Models
slug: docs/predictable-outputs
hidden: false
description: >-
  Strategies for decoding text, and the parameters that impact the randomness
  and predictability of a language model's output.
image: ../../assets/images/60e44be-cohere_meta_image.jpg
keywords: generative AI output
createdAt: 'Thu Feb 29 2024 18:08:15 GMT+0000 (Coordinated Universal Time)'
updatedAt: 'Thu Jun 06 2024 04:52:20 GMT+0000 (Coordinated Universal Time)'
---
The predictability of the model's output can be controlled using the `seed` and `temperature` parameters of the Chat API.

## Seed

<Info title="Note"> 
 The `seed` parameter does not guarantee long-term reproducibility. Under-the-hood updates to the model may invalidate the seed.
</Info>

The easiest way to force the model into reproducible behavior is by providing a value for the `seed` parameter. Specifying the same integer `seed` in consecutive requests will result in the same set of tokens being generated by the model. This can be useful for debugging and testing.

```python PYTHON
import cohere

co = cohere.Client(api_key="YOUR API KEY")

res = co.chat(
    model="command-a-03-2025", message="say a random word", seed=45
)
print(res.text)  # Sure! How about "onomatopoeia"?

# making another request with the same seed results in the same generated text
res = co.chat(
    model="command-a-03-2025", message="say a random word", seed=45
)
print(res.text)  # Sure! How about "onomatopoeia"?
```

## Temperature

Sampling from generation models incorporates randomness, so the same prompt may yield different outputs from generation to generation. Temperature is a parameter ranging from `0-1` used to tune the degree of randomness, and it defaults to a value of `.3`.

### How to pick temperature when sampling

A lower temperature means less randomness; a temperature of `0` will always yield the same output. Lower temperatures (around `.1` to `.3`) are more appropriate when performing tasks that have a "correct" answer, like question answering or summarization. If the model starts repeating itself this is a sign that the temperature may be too low.

High temperature means more randomness and less grounding. This can help the model give more creative outputs, but if you're using [retrieval augmented generation](/docs/retrieval-augmented-generation-rag), it can also mean that it doesn't correctly use the context you provide. If the model starts going off topic, giving nonsensical outputs, or failing to ground properly, this is a sign that the temperature is too high.

<img src='../../assets/images/775d949-Temperature_Visual_1.png' alt='setting' />

Temperature can be tuned for different problems, but most people will find that a temperature of `.3` or `.5` is a good starting point.

As sequences get longer, the model naturally becomes more confident in its predictions, so you can raise the temperature much higher for long prompts without going off topic. In contrast, using high temperatures on short prompts can lead to outputs being very unstable.
