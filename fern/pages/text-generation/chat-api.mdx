---
title: "Using the Cohere Chat API for Text Generation"
slug: docs/chat-api
hidden: false
description: >-
  How to use the Chat API endpoint with Cohere LLMs to generate text responses in a conversational interface.
image: ../../assets/images/4a5325a-cohere_meta_image.jpg
keywords: 'Cohere, text generation, LLMs, generative AI'
createdAt: 'Thu Feb 29 2024 18:05:29 GMT+0000 (Coordinated Universal Time)'
updatedAt: 'Tue Jun 18 2024 07:20:15 GMT+0000 (Coordinated Universal Time)'
---
The Chat API endpoint is used to generate text with Cohere LLMs. This endpoint facilitates a conversational interface, allowing users to send messages to the model and receive text responses.

<CodeBlocks>
```python PYTHON
import cohere

co = cohere.Client(api_key="<YOUR API KEY>")

response = co.chat(
    model="command-a-03-2025",
    message="Write a title for a blog post about API design. Only output the title text.",
)

print(response.text)
# "The Art of API Design: Crafting Elegant and Powerful Interfaces"
```
```java JAVA
public class ChatPost {
  public static void main(String[] args) {
    Cohere cohere = Cohere.builder().token("<YOUR API KEY>").build();

    NonStreamedChatResponse response = cohere.chat(
      ChatRequest.builder()
      	.model("command-a-03-2025")
      	.message("Write a title for a blog post about API design. Only output the title text.")
    )

    System.out.println(response); // "The Art of API Design: Crafting Elegant and Powerful Interfaces"
  }
}
```
```typescript TYPESCRIPT
const { CohereClient } = require('cohere-ai');

const cohere = new CohereClient({
  token: '<YOUR API KEY>',
});

(async () => {
  const response = await cohere.chat({
    message: 'Write a title for a blog post about API design. Only output the title text.',
  });

  console.log(response.text)
})();
```
</CodeBlocks>

## Response Structure

Below is a sample response from the Chat API

```json JSON
{
    "text": "The Art of API Design: Crafting Elegant and Powerful Interfaces",
    "generation_id": "dd78b9fe-988b-4c18-9419-8fbdf9968948",
    "chat_history": [
        {
            "role": "USER",
            "message": "Write a title for a blog post about API design. Only output the title text."
        },
        {
            "role": "CHATBOT",
            "message": "The Art of API Design: Crafting Elegant and Powerful Interfaces"
        }
    ],
    "finish_reason": "COMPLETE",
    "meta": {
        "api_version": {
            "version": "1"
        },
        "billed_units": {
            "input_tokens": 17,
            "output_tokens": 12
        },
        "tokens": {
            "input_tokens": 83,
            "output_tokens": 12
        }
    }
}
```

Every response contains the following fields:

- `text` the generated message from the model.
- `generation_id` the ID corresponding to this response. Can be used together with the Feedback API endpoint to promote great responses and flag bad ones.
- `chat_history` the conversation presented in a chat log format
- `finish_reason` can be one of the following:
  - `COMPLETE` the model successfully finished generating the message
  - `MAX_TOKENS` the model's context limit was reached before the generation could be completed
- `meta` contains information with token counts, billing etc.

## Multi-turn conversations

The user message in the Chat request can be sent together with a `chat_history` to provide the model with conversational context:

```python PYTHON
import cohere

co = cohere.Client(api_key="<YOUR API KEY>")

message = "Can you tell me about LLMs?"

response = co.chat(
    model="command-a-03-2025",
    chat_history=[
        {"role": "USER", "text": "Hey, my name is Michael!"},
        {
            "role": "CHATBOT",
            "text": "Hey Michael! How can I help you today?",
        },
    ],
    message=message,
)

print(response.text)  # "Sure thing Michael, LLMs are ..."
```

Instead of manually building the chat_history, we can grab it from the response of the previous turn.

```python PYTHON
chat_history = []
max_turns = 10

for _ in range(max_turns):
    # get user input
    message = input("Send the model a message: ")

    # generate a response with the current chat history
    response = co.chat(
        model="command-a-03-2025",
        message=message,
        chat_history=chat_history,
    )

    # print the model's response on this turn
    print(response.text)

    # set the chat history for next turn
    chat_history = response.chat_history
```

### Using `conversation_id` to Save Chat History

Providing the model with the conversation history is one way to have a multi-turn conversation with the model. Cohere has developed another option for users who do not wish to save the conversation history, and it works through a user-defined `conversation_id`. 

```python PYTHON
import cohere

co = cohere.Client("<YOUR API KEY>")

response = co.chat(
    model="command-a-03-2025",
    message="The secret word is 'fish', remember that.",
    conversation_id="user_defined_id_1",
)

answer = response.text
```

Then, if you wanted to continue the conversation, you could do so like this (keeping the `id` consistent):

```python PYTHON
response2 = co.chat(
    model="command-a-03-2025",
    message="What is the secret word?",
    conversation_id="user_defined_id_1",
)

print(response2.text)  # "The secret word is 'fish'"
```

Note that the `conversation_id` should not be used in conjunction with the `chat_history`. They are mutually exclusive.

