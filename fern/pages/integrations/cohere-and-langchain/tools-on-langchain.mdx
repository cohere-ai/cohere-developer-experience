---
title: Cohere Tools on LangChain (Integration Guide)
slug: docs/tools-on-langchain
hidden: false
description: >-
  Explore code examples for multi-step and single-step tool usage in chatbots,
  harnessing internet search and vector storage.
image: ../../../assets/images/50ab198-cohere_meta_image.jpg
keywords: 'tool use, generative AI, langchain'
createdAt: 'Wed Apr 03 2024 12:21:56 GMT+0000 (Coordinated Universal Time)'
updatedAt: 'Tue Jun 11 2024 15:41:50 GMT+0000 (Coordinated Universal Time)'
---
Cohere supports various integrations with LangChain, a large language model (LLM) framework which allows you to quickly create applications based on Cohere's models. This doc will guide you through how to leverage [Cohere tools](/docs/tool-use) with LangChain. 

### Prerequisites

Running Cohere tools with LangChain doesn't require many prerequisites, consult the [top-level document](/docs/cohere-and-langchain) for more information.

## Multi-Step Tool Use

Multi-step is enabled by default. Here's an example of using it to put together a simple agent:

```python PYTHON
from langchain.agents import AgentExecutor
from langchain_cohere.react_multi_hop.agent import create_cohere_react_agent
from langchain_core.prompts import ChatPromptTemplate
from langchain_cohere import ChatCohere

from langchain_community.tools.tavily_search import TavilySearchResults
from pydantic import BaseModel, Field

os.environ["TAVILY_API_KEY"] ="TAVILY_API_KEY"

internet_search = TavilySearchResults()
internet_search.name = "internet_search"
internet_search.description = "Returns a list of relevant document snippets for a textual query retrieved from the internet."

class TavilySearchInput(BaseModel):
    query: str = Field(description="Query to search the internet with")
internet_search.args_schema = TavilySearchInput

# Define the Cohere LLM
llm = ChatCohere(cohere_api_key="COHERE_API_KEY",              
                 model="command-r-plus-08-2024",
                 temperature=0)
# Preamble
preamble = """
You are an expert who answers the user's question with the most relevant datasource. You are equipped with an internet search tool and a special vectorstore of information about how to write good essays.
"""

# Prompt template
prompt = ChatPromptTemplate.from_template("{input}")

# Create the ReAct agent
agent = create_cohere_react_agent(
    llm=llm,
    tools=[internet_search],
    prompt=prompt,
)

agent_executor = AgentExecutor(agent=agent, tools=[internet_search], verbose=True)

response = agent_executor.invoke({
    "input": "Who is the mayor of the capital of Ontario",
    "preamble": preamble,
})

print(response['output'])
```

## Single-Step Tool Use

In order to utilize single-step mode, you have to set `force_single_step=True`. Here's an example of using it to answer a few questions:

```python PYTHON
from langchain_cohere import ChatCohere
from langchain_core.messages import HumanMessage
from pydantic import BaseModel, Field

# Data model
class web_search(BaseModel):
    """
    The internet. Use web_search for questions that are related to anything else than agents, prompt engineering, and adversarial attacks.
    """
    query: str = Field(description="The query to use when searching the internet.")

class vectorstore(BaseModel):
    """
    A vectorstore containing documents related to agents, prompt engineering, and adversarial attacks. Use the vectorstore for questions on these topics.
    """
    query: str = Field(description="The query to use when searching the vectorstore.")

# Preamble
preamble = """You are an expert at routing a user question to a vectorstore or web search.
The vectorstore contains documents related to agents, prompt engineering, and adversarial attacks.
Use the vectorstore for questions on these topics. Otherwise, use web-search."""

# LLM with tool use and preamble
# Define the Cohere LLM
llm = ChatCohere(cohere_api_key="COHERE_API_KEY",              
                 model="command-r-plus-08-2024")

llm_with_tools = llm.bind_tools(tools=[web_search, vectorstore], preamble=preamble)

messages = [HumanMessage("Who will the Bears draft first in the NFL draft?")]
response = llm_with_tools.invoke(messages, force_single_step=True)
print(response.response_metadata['tool_calls'])

messages = [HumanMessage("What are the types of agent memory?")]
response = llm_with_tools.invoke(messages, force_single_step=True)
print(response.response_metadata['tool_calls'])

messages = [HumanMessage("Hi, How are you?")]
response = llm_with_tools.invoke(messages, force_single_step=True)
print('tool_calls' in response.response_metadata)
```

## SQL Agent
LangChain's SQL Agent abstraction provides a flexible way of interacting with SQL Databases. This can be accessed via the `create_sql_agent` constructor.

```python PYTHON
from langchain_cohere import ChatCohere, create_sql_agent
from langchain_community.utilities import SQLDatabase
import urllib.request
import pandas as pd
import sqlite3

# Download the Chinook SQLite database
url = "https://github.com/lerocha/chinook-database/raw/master/ChinookDatabase/DataSources/Chinook_Sqlite.sqlite"
urllib.request.urlretrieve(url, "Chinook.db")
print("Chinook database downloaded successfully.")

db = SQLDatabase.from_uri("sqlite:///Chinook.db")
print(db.dialect)
print(db.get_usable_table_names())
db.run("SELECT * FROM Artist LIMIT 10;")

# Define the Cohere LLM
llm = ChatCohere(cohere_api_key="COHERE_API_KEY",              
                 model="command-r-plus-08-2024",
                 temperature=0)

agent_executor = create_sql_agent(llm, db=db, verbose=True)

resp = agent_executor.invoke("Show me the first 5 rows of the Album table.")
print(resp)
```

## CSV Agent
LangChain's CSV Agent abstraction enables building agents that can interact with CSV files. This can be accessed via the `create_csv_agent` constructor.

```python PYTHON
from langchain_cohere import ChatCohere, create_csv_agent

# Define the Cohere LLM
llm = ChatCohere(cohere_api_key="COHERE_API_KEY",              
                 model="command-r-plus-08-2024",
                 temperature=0)

agent_executor = create_csv_agent(
    llm,
    "titanic.csv" # https://github.com/langchain-ai/langchain/blob/master/templates/csv-agent/titanic.csv
)

resp = agent_executor.invoke({"input":"How many people were on the titanic?"})
print(resp.get("output")) 
```

## Streaming for Tool Calling

When tools are called in a streaming context, message chunks will be populated with tool call chunk objects in a list via the `.tool_call_chunks` attribute.

```python PYTHON
from langchain_core.tools import tool
from langchain_cohere import ChatCohere

@tool
def add(a: int, b: int) -> int:
    """Adds a and b."""
    return a + b


@tool
def multiply(a: int, b: int) -> int:
    """Multiplies a and b."""
    return a * b

tools = [add, multiply]

# Define the Cohere LLM
llm = ChatCohere(cohere_api_key="COHERE_API_KEY",              
                 model="command-r-plus-08-2024",
                 temperature=0)

llm_with_tools = llm.bind_tools(tools)

query = "What is 3 * 12? Also, what is 11 + 49?"

for chunk in llm_with_tools.stream(query):
    if chunk.tool_call_chunks:
        print(chunk.tool_call_chunks)
```

## LangGraph Agents
LangGraph is a stateful, orchestration framework that brings added control to agent workflows.

To use LangGraph with Cohere, you need to install the LangGraph package. To install it, run `pip install langgraph`.

### Basic Chatbot
This simple chatbot example will illustrate the core concepts of building with LangGraph.

```python PYTHON
from typing import Annotated
from typing_extensions import TypedDict
from langgraph.graph import StateGraph, START, END
from langgraph.graph.message import add_messages
from langchain_cohere import ChatCohere

# Create a state graph
class State(TypedDict):
    messages: Annotated[list, add_messages]

graph_builder = StateGraph(State)

# Define the Cohere LLM
llm = ChatCohere(cohere_api_key="COHERE_API_KEY",              
                 model="command-r-plus-08-2024")

# Add nodes
def chatbot(state: State):
    return {"messages": [llm.invoke(state["messages"])]}

graph_builder.add_node("chatbot", chatbot)
graph_builder.add_edge(START, "chatbot")
graph_builder.add_edge("chatbot", END)

# Compile the graph
graph = graph_builder.compile()

# Run the chatbot
while True:
    user_input = input("User: ")
    print("User: "+ user_input)
    if user_input.lower() in ["quit", "exit", "q"]:
        print("Goodbye!")
        break
    for event in graph.stream({"messages": ("user", user_input)}):
        for value in event.values():
            print("Assistant:", value["messages"][-1].content)
```

### Enhancing the Chatbot with Tools
To handle queries our chatbot can't answer "from memory", we'll integrate a web search tool. Our bot can use this tool to find relevant information and provide better responses.

```python PYTHON
from langchain_community.tools.tavily_search import TavilySearchResults
from langchain_cohere import ChatCohere
from langgraph.graph import StateGraph, START
from langgraph.graph.message import add_messages
from langchain_core.messages import ToolMessage
from langchain_core.messages import BaseMessage
from typing import Annotated, Literal
from typing_extensions import TypedDict
import json


# Create a tool
tool = TavilySearchResults(max_results=2)
tools = [tool]


# Create a state graph
class State(TypedDict):
    messages: Annotated[list, add_messages]


graph_builder = StateGraph(State)

# Define the LLM
llm = ChatCohere(cohere_api_key="COHERE_API_KEY",              
                 model="command-r-plus-08-2024")

# Bind the tools to the LLM
llm_with_tools = llm.bind_tools(tools)


# Add nodes
def chatbot(state: State):
    return {"messages": [llm_with_tools.invoke(state["messages"])]}


graph_builder.add_node("chatbot", chatbot)

class BasicToolNode:
    """A node that runs the tools requested in the last AIMessage."""

    def __init__(self, tools: list) -> None:
        self.tools_by_name = {tool.name: tool for tool in tools}

    def __call__(self, inputs: dict):
        if messages := inputs.get("messages", []):
            message = messages[-1]
        else:
            raise ValueError("No message found in input")
        outputs = []
        for tool_call in message.tool_calls:
            tool_result = self.tools_by_name[tool_call["name"]].invoke(
                tool_call["args"]
            )
            outputs.append(
                ToolMessage(
                    content=json.dumps(tool_result),
                    name=tool_call["name"],
                    tool_call_id=tool_call["id"],
                )
            )
        return {"messages": outputs}


tool_node = BasicToolNode(tools=[tool])
graph_builder.add_node("tools", tool_node)

def route_tools(
    state: State,
) -> Literal["tools", "__end__"]:
    """
    Use in the conditional_edge to route to the ToolNode if the last message
    has tool calls. Otherwise, route to the end.
    """
    if isinstance(state, list):
        ai_message = state[-1]
    elif messages := state.get("messages", []):
        ai_message = messages[-1]
    else:
        raise ValueError(f"No messages found in input state to tool_edge: {state}")
    if hasattr(ai_message, "tool_calls") and len(ai_message.tool_calls) > 0:
        return "tools"
    return "__end__"


graph_builder.add_conditional_edges(
    "chatbot",
    route_tools,
    {"tools": "tools", "__end__": "__end__"},
)
graph_builder.add_edge("tools", "chatbot")
graph_builder.add_edge(START, "chatbot")

# Compile the graph
graph = graph_builder.compile()

# Run the chatbot
while True:
    user_input = input("User: ")
    if user_input.lower() in ["quit", "exit", "q"]:
        print("Goodbye!")
        break
    for event in graph.stream({"messages": [("user", user_input)]}):
        for value in event.values():
            if isinstance(value["messages"][-1], BaseMessage):
                print("Assistant:", value["messages"][-1].content)
```
