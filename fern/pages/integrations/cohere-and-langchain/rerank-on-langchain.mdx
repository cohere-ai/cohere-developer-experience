---
title: "Rerank on LangChain"
slug: "docs/rerank-on-langchain"

hidden: false

description: "This page describes how to integrate Cohere's ReRank models with LangChain."
image: "../../../assets/images/f1cc130-cohere_meta_image.jpg"  
keywords: "Cohere, language models, LangChain, Rerank models"

createdAt: "Fri Mar 01 2024 16:19:13 GMT+0000 (Coordinated Universal Time)"
updatedAt: "Tue May 28 2024 16:53:20 GMT+0000 (Coordinated Universal Time)"
---
Cohere supports various integrations with LangChain, a large language model (LLM) framework which allows you to quickly create applications based on Cohere's models. This doc will guide you through how to leverage Rerank with LangChain. 

### Prerequisites

Running Cohere Rerank with LangChain doesn't require many prerequisites, consult the [top-level document](/docs/cohere-and-langchain) for more information.

### Cohere ReRank with LangChain

To use Cohere's [rerank functionality ](/docs/reranking) with LangChain, start with instantiating a [CohereRerank](https://github.com/langchain-ai/langchain/blob/master/libs/langchain/langchain/retrievers/document_compressors/cohere_rerank.py) object as follows: `cohere_rerank = CohereRerank(cohere_api_key="{API_KEY}")`.

You can then use it with LangChain retrievers, embeddings, and RAG. The example below uses the vector DB chroma, for which you will need to install `pip install chromadb`. Other vector DB's [from this list](https://python.langchain.com/docs/integrations/vectorstores) can also be used.

```python PYTHON
from langchain.retrievers import ContextualCompressionRetriever
from langchain_cohere import CohereEmbeddings
from langchain_cohere import ChatCohere
from langchain_cohere import CohereRerank, CohereRagRetriever
from langchain.text_splitter import CharacterTextSplitter
from langchain_community.document_loaders import TextLoader
from langchain_community.vectorstores import Chroma
from langchain_community.document_loaders import WebBaseLoader

user_query =  "what is Cohere Toolkit?"

# Define the Cohere LLM
llm = ChatCohere(cohere_api_key="COHERE_API_KEY",              
                 model="command-r-plus-08-2024")

# Define the Cohere embedding model
embeddings = CohereEmbeddings(cohere_api_key="COHERE_API_KEY",
                              model="embed-english-light-v3.0")

# Load text files and split into chunks, you can also use data gathered elsewhere in your application
raw_documents = WebBaseLoader("https://docs.cohere.com/docs/cohere-toolkit").load()
text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)
documents = text_splitter.split_documents(raw_documents)

# Create a vector store from the documents
db = Chroma.from_documents(documents, embeddings)

# Create Cohere's reranker with the vector DB using Cohere's embeddings as the base retriever
reranker = CohereRerank(cohere_api_key="COHERE_API_KEY", 
                        model="rerank-english-v3.0")

compression_retriever = ContextualCompressionRetriever(
    base_compressor=reranker,
    base_retriever=db.as_retriever()
)
compressed_docs = compression_retriever.get_relevant_documents(user_query)
# Print the relevant documents from using the embeddings and reranker
print(compressed_docs)

# Create the cohere rag retriever using the chat model
rag = CohereRagRetriever(llm=llm, connectors=[])
docs = rag.get_relevant_documents(
    user_query,
    documents=compressed_docs,
)
# Print the documents
print("Documents:")
for doc in docs[:-1]:
    print(doc.metadata)
    print("\n\n" + doc.page_content)
    print("\n\n" + "-" * 30 + "\n\n")
# Print the final generation
answer = docs[-1].page_content
print("Answer:")
print(answer)
# Print the final citations
citations = docs[-1].metadata['citations']
print("Citations:")
print(citations)
```
