---
title: "Retrieval Augmented Generation (RAG)"
slug: "v2/docs/retrieval-augmented-generation-rag"

hidden: false
description: "Retrieval Augmented Generation (RAG) is a method for generating text using external data sources to improve accuracy. The Chat API in combination with the Command model can help generate grounded text with inline citations based on provided documents."
image: "../../../assets/images/1edd35f-cohere_meta_image.jpg"  
keywords: "retrieval augmented generation, RAG, grounded replies, text generation"

createdAt: "Fri Aug 18 2023 19:13:29 GMT+0000 (Coordinated Universal Time)"
updatedAt: "Wed Jun 19 2024 13:01:22 GMT+0000 (Coordinated Universal Time)"
---
Retrieval Augmented Generation (RAG) is a method for generating text using additional information fetched from an external data source, which can greatly increase the accuracy of the response. When used in conjunction with a [Command](https://docs.cohere.com/docs/command-beta), [Command R](https://docs.cohere.com/docs/command-r), or [Command R+](https://docs.cohere.com/docs/command-r-plus), the [Chat API](https://docs.cohere.com/reference/chat) makes it easy to generate text that is grounded on supplementary information.

To call the Chat API with RAG, create a `user` message containing two content fields:
- `content` for the user's query.
- `documents` for storing all the documents.

The `documents` field consists of a list of dictionaries where each dictionary represents a document. A document can have different fields depending on its nature, such as `title`, `text`, `url`, etc. Optionally, you can also pass your own IDs for each document using the `id` field. Otherwise, the API will automatically generate the IDs based on the documents position in the list.

Then pass this `user` message to the `messages` parameter in the Chat endpoint call.

The code snippet below, for example, will produce a grounded answer to `"Where do the tallest penguins live?"`, along with inline citations based on the provided documents.

**Request**

```python
import cohere
co = cohere.ClientV2(api_key="<YOUR API KEY>")

# Retrieve the documents
documents=[
    {"title": "Tall penguins", "snippet": "Emperor penguins are the tallest."},
    {"title": "Penguin habitats", "snippet": "Emperor penguins only live in Antarctica."},
    {"title": "What are animals?", "snippet": "Animals are different from plants."}
    ]

# Add the user message containing the query and documents
message = "Where do the tallest penguins live?"
messages = [{'role': 'user', 'content': message, "documents": documents}]

response = co.chat(
    model="command-r-plus",
    messages=messages)

print(response.message.content)

print(response.message.citations)
```
Here is the full content of the `messages` object from the example above, which includes the `documents` as part of the `user` message.

```json
{
  "messages": [{
  "role": "user",
  "content": [
    {
      "role": "user",
      "content": "Where do the tallest penguins live?",
      "documents": [
        {
          "title": "Tall penguins",
          "snippet": "Emperor penguins are the tallest."
        },
        {
          "title": "Penguin habitats",
          "snippet": "Emperor penguins only live in Antarctica."
        },
        {
          "title": "What are animals?",
          "snippet": "Animals are different from plants."
        }
      ]
    }
  ]
}]
}
```

The resulting generation is`"The tallest penguins are emperor penguins, which live in Antarctica."`. The model was able to combine partial information from multiple sources and ignore irrelevant documents to arrive at the full answer.

Nice :penguin:❄️!

**Response**

```
# response.message.content
[TextContent(text='The tallest penguins are emperor penguins, which live in Antarctica.', type='text')]


# response.message.citations
[Citation(start=4, 
          end=41, text='tallest penguins are emperor penguins', sources=[Source_Document(id='doc:0:0', document={'id': 'doc:0:0', 'snippet': 'Emperor penguins are the tallest.', 'title': 'Tall penguins'}, type='document')]), 
          
 Citation(start=57, 
          end=68, 
          text='Antarctica.', 
          sources=[Source_Document(id='doc:0:1', document={'id': 'doc:0:1', 'snippet': 'Emperor penguins only live in Antarctica.', 'title': 'Penguin habitats'}, type='document')])]
```

The response also includes **inline citations**  that reference the first two documents, since they hold the answers.

![](../../../assets/images/0062bc8-image.png)


You can find more code and context in [this colab notebook](https://github.com/cohere-ai/notebooks/blob/main/notebooks/Vanilla_RAG_v2.ipynb).

### Three steps of RAG

The RAG workflow generally consists of **3 steps**:

- **Generating search queries** for finding relevant documents. _What does the model recommend looking up before answering this question? _
- **Fetching relevant documents** from an external data source using the generated search queries. _Performing a search to find some relevant information._
- **Generating a response **with inline citations using the fetched documents. _Using the acquired knowledge to produce an educated answer_.

#### Example: Using RAG to identify the definitive 90s boy band

In this section, we will use the three step RAG workflow to finally settle the score between the notorious boy bands Backstreet Boys and NSYNC. We ask the model to provide an informed answer to the question `"Who is more popular: Nsync or Backstreet Boys?"`

#### Step 1: Generating search queries

First, the model needs to generate an optimal set of search queries to use for retrieval. 

There are different possible approaches to this. In this example, we'll take a [tool use](v2/docs/tool-use) approach.

Here, we build a tool that takes a user query and returns a list of relevant document snippets for that query. The tool can generate one or multiple search queries depending on the user query.

```python PYTHON
def generate_search_queries(message):
    
    tools = [{
        "type": "function",
        "function": {
            "name": "document_search",
            "description" : "Searches for textual documents based on a user query.",
            "parameters": {
            "type": "object",
            "properties": {
                "query": {"type": "string", "description": "The search query"}
            },
            "required": ["query"]    
            }
        }
    }]
        
    response = co.chat(
        model="command-r-plus",
        messages=[{"role": "user", "content": message}],
        tools=tools
    )
    
    search_queries = []
    
    if response.message.tool_calls:
        res = response.message
        documents = []
        for tc in res.tool_calls:
            query = json.loads(tc.function.arguments)["query"]
            search_queries.append(query)
        
    return search_queries
```

Now, given the query, the following search queries are generated.

``` PYTHON
generate_search_queries("Who is more popular: Nsync or Backstreet Boys?")
```
```
# Sample response
['popularity of NSync', 'popularity of Backstreet Boys']
```
Indeed, to generate a factually accurate answer to the question "Who is more popular: Nsync or Backstreet Boys?", looking up `popularity of NSync` and `popularity of Backstreet Boys` first would be helpful.

#### Step 2: Fetching relevant documents

The next step is to [fetch documents](https://docs.cohere.com/docs/documents-and-citations) from the relevant data source using the generated search queries. For example, to answer the question about the two pop sensations _NSYNC_ and _Backstreet Boys_, one might want to use an API from a web search engine, and fetch the contents of the websites listed at the top of the search results.

We won't go into details of fetching data in this guide, since it's very specific to the search API you're querying. However we should mention that breaking up long documents into smaller ones first (1-2 paragraphs) will help you not go over the context limit. When trying to stay within the context length limit, you might need to omit some of the documents from the request. To make sure that only the least relevant documents are omitted, we recommend using the [Rerank endpoint](https://docs.cohere.com/reference/rerank) endpoint which will sort the documents by relevancy to the query. The lowest ranked documents are the ones you should consider dropping first.

#### Step 3: Generating a response

In the final step, we will be calling the Chat API again, but this time passing along the `documents` you acquired in Step 2. A `document` object is a dictionary containing the content and the metadata of the text. We recommend using a few descriptive keys such as `"title"`, `"snippet"`, or `"last updated"` and only including semantically relevant data. The keys and the values will be formatted into the prompt and passed to the model.

**Request**

```py
import cohere
co = cohere.ClientV2(api_key="<YOUR API KEY>")

documents=[
    {
      "title": "CSPC: Backstreet Boys Popularity Analysis - ChartMasters",
      "snippet": "↓ Skip to Main Content\n\nMusic industry – One step closer to being accurate\n\nCSPC: Backstreet Boys Popularity Analysis\n\nHernán Lopez Posted on February 9, 2017 Posted in CSPC 72 Comments Tagged with Backstreet Boys, Boy band\n\nAt one point, Backstreet Boys defined success: massive albums sales across the globe, great singles sales, plenty of chart topping releases, hugely hyped tours and tremendous media coverage.\n\nIt is true that they benefited from extraordinarily good market conditions in all markets. After all, the all-time record year for the music business, as far as revenues in billion dollars are concerned, was actually 1999. That is, back when this five men group was at its peak."
    },
    {
      "title": "CSPC: NSYNC Popularity Analysis - ChartMasters",
      "snippet": "↓ Skip to Main Content\n\nMusic industry – One step closer to being accurate\n\nCSPC: NSYNC Popularity Analysis\n\nMJD Posted on February 9, 2018 Posted in CSPC 27 Comments Tagged with Boy band, N'Sync\n\nAt the turn of the millennium three teen acts were huge in the US, the Backstreet Boys, Britney Spears and NSYNC. The latter is the only one we haven’t study so far. It took 15 years and Adele to break their record of 2,4 million units sold of No Strings Attached in its first week alone.\n\nIt wasn’t a fluke, as the second fastest selling album of the Soundscan era prior 2015, was also theirs since Celebrity debuted with 1,88 million units sold."
    },
    {
      "title": "CSPC: Backstreet Boys Popularity Analysis - ChartMasters",
      "snippet": " 1997, 1998, 2000 and 2001 also rank amongst some of the very best years.\n\nYet the way many music consumers – especially teenagers and young women’s – embraced their output deserves its own chapter. If Jonas Brothers and more recently One Direction reached a great level of popularity during the past decade, the type of success achieved by Backstreet Boys is in a completely different level as they really dominated the business for a few years all over the world, including in some countries that were traditionally hard to penetrate for Western artists.\n\nWe will try to analyze the extent of that hegemony with this new article with final results which will more than surprise many readers."
    },
    {
      "title": "CSPC: NSYNC Popularity Analysis - ChartMasters",
      "snippet": " Was the teen group led by Justin Timberlake really that big? Was it only in the US where they found success? Or were they a global phenomenon?\n\nAs usual, I’ll be using the Commensurate Sales to Popularity Concept in order to relevantly gauge their results. This concept will not only bring you sales information for all NSYNC‘s albums, physical and download singles, as well as audio and video streaming, but it will also determine their true popularity. If you are not yet familiar with the CSPC method, the next page explains it with a short video. I fully recommend watching the video before getting into the sales figures."
    }
]

# Add the user message containing the query and documents
message = "Who is more popular: Nsync or Backstreet Boys?"
messages = [{'role': 'user', 'content': message, "documents": documents}]

response = co.chat(
    model="command-r-plus",
    messages=messages)

print(response.message.content)

print(response.message.citations)
```

**Response**

```
# response.message.content
[TextContent(text='Both NSync and Backstreet Boys were extremely popular at the turn of the millennium. Backstreet Boys achieved massive album sales across the globe, great singles sales, plenty of chart-topping releases, hyped tours, and tremendous media coverage. NSync also had huge sales, with their album No Strings Attached selling 2.4 million units in its first week. However, Backstreet Boys achieved success in some countries that were traditionally hard to penetrate for Western artists, which suggests that they may have been more popular overall.', type='text')]

# response.message.citations (truncated for brevity)
[Citation(start=36, 
          end=84, 
          text='extremely popular at the turn of the millennium.', 
          sources=[Source_Document(id='1', document={'id': '1', 'snippet': "↓ Skip to Main Content\n\nMusic industry – One step closer ...", 'title': 'CSPC: NSYNC Popularity Analysis - ChartMasters'}, type='document')]), 

Citation(start=110, 
          end=146, 
          text='massive album sales across the globe', 
          sources=[Source_Document(id='0', document={'id': '0', 'snippet': '↓ Skip to Main Content\n\nMusic industry – One step closer ...', 'title': 'CSPC: Backstreet Boys Popularity Analysis - ChartMasters'}, type='document')]), 
Citation(start=148, 
          end=167, 
          text='great singles sales', 
          sources=[Source_Document(id='0', document={'id': '0', 'snippet': '↓ Skip to Main Content\n\nMusic industry – One step closer to being accurate\n\nCSPC: Backstreet ...', 'title': 'CSPC: Backstreet Boys Popularity Analysis - ChartMasters'}, type='document')]), 
Citation(start=169, 
          end=201, 
          ...
...]

```

Not only will we discover that the Backstreet Boys were the more popular band, but the model can also _Tell Me Why_, by providing details [supported by citations](https://docs.cohere.com/docs/documents-and-citations).


### Caveats

It’s worth underscoring that RAG does not guarantee accuracy. It involves giving a model context which informs its replies, but if the provided documents are themselves out-of-date, inaccurate, or biased, whatever the model generates might be as well. What’s more, RAG doesn’t guarantee that a model won’t hallucinate. It greatly reduces the risk, but doesn’t necessarily eliminate it altogether. This is why we put an emphasis on including inline citations, which allow users to verify the information.
