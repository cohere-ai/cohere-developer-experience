---
title: Toxicity Detection
slug: docs/toxicity-detection
subtitle: >-
  This endpoint classifies text into one of several classes. It uses a few
  examples to create a classifier from a generative model. In the background, it
  constructs a few-shot classification prompt and uses it classify the `input`
  texts you pass to it.
hidden: false
description: >-
  Learn how to classify user comments for toxicity using the Cohere SDK in this
  interactive tutorial.
image: ../../../assets/images/68e785a-meta_docs_image_cohere.jpg
createdAt: 'Thu Sep 08 2022 23:27:58 GMT+0000 (Coordinated Universal Time)'
updatedAt: 'Fri Mar 15 2024 04:33:37 GMT+0000 (Coordinated Universal Time)'
---
This document is an interactive tutorial on classifying online user comments for toxicity using the Cohere SDK. It provides instructions on setting up the client, adding examples, adding inputs, and getting classifications.
