---
title: "Toxicity Detection"
slug: "docs/toxicity-detection"
subtitle: "This endpoint classifies text into one of several classes. It uses a few examples to create a classifier from a generative model. In the background, it constructs a few-shot classification prompt and uses it classify the `input` texts you pass to it."
hidden: false 
description: "This document is an interactive tutorial on classifying online user comments for toxicity using the Cohere SDK. It provides instructions on setting up the client, adding examples, adding inputs, and getting classifications."
image: "../../../assets/images/68e785a-meta_docs_image_cohere.jpg"
createdAt: "Thu Sep 08 2022 23:27:58 GMT+0000 (Coordinated Universal Time)"
updatedAt: "Fri Mar 15 2024 04:33:37 GMT+0000 (Coordinated Universal Time)"
---
<Note title="This is an interactive tutorial!">  
 To run this tutorial, click on **Examples** and select one of the options.
</Note>

The internet is dominated by user-generated content. While it provides an avenue for online platforms to grow, it is a bane for content moderators managing them. It is impossible for humans to manually moderate all the user content that is created. This is why an automated solution is needed, such as in flagging for toxic content.

Here we look at an example of classifying online user comments for toxicity by classifying them in `Toxic` or `Not Toxic`.

## Set up

Install the SDK.
