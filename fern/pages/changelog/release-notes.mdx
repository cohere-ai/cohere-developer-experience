---
title: "Release Notes (Archive)"
slug: "changelog/release-notes"
type: ""
createdAt: "Tue Jul 12 2022 21:07:29 GMT+0000 (Coordinated Universal Time)"
hidden: true
---
## Updates

### August 5th, 2022

**Introducing Moderate (Beta)**  
Use Moderate (Beta) to classify harmful text across the following categories: `profane`, `hate speech`, `violence`, `self-harm`, `sexual`, `sexual (non-consenual)`, `harassment`, `spam`, `information hazard (e.g., pii)`. Moderate returns an array containing each category and its associated confidence score. Over the coming weeks, expect performance to improve significantly as we optimize the underlying model.

### July 20th, 2022

**Model parameter now optional** Our APIs no longer require a model to be specified. Each endpoint comes with great defaults. For more control, a model can still be specified by adding a model param in the request.

### April 25th, 2022

**New Extremely Large**  
Our new and improved `xlarge` has better generation quality and a 4x faster prediction speed. This model now supports a maximum token length of 2048 tokens and frequency and presence penalties.

**Updated Small, Medium, and Large Generation Models**  
Updated `small`, `medium`, and `large` models are more stable and resilient against abnormal inputs due to a FP16 quantization fix. We also fixed a bug in generation presence & frequency penalty, which will result in more effective penalties.

### March 29, 2022

**New & Improved Generation and Representation Models**

We've retrained our `small`, `medium`, and `large` generation and representation models. Updated representation models now support contexts up to 4096 tokens (previously 1024 tokens). We recommend keeping text lengths below 512 tokens for optimal performance; for any text longer than 512 tokens, the text is spliced and the resulting embeddings of each component are then averaged and returned.

### March 8, 2022

**Classification Endpoint**

Classification is now available via our classification endpoint. This endpoint is currently powered by our generation models (`small` and `medium`) and supports few-shot classification. We will be deprecating support for Choose Best by May 18th. To learn more about classification at Cohere check out the docs [here](/classify-reference).

**New & Improved Generation Models**

We’ve shipped updated `small`, `medium`, and `large` generation models. You’ll find significant improvements in performance that come from our newly assembled high quality dataset. 

**Finetuning is Generally Available**

You no longer need to wait for Full Access approval to build your own custom finetuned generation or representation model. Upload your dataset and start seeing even better performance for your specific task.

**Policy Updates**

The Cohere team continues to be focused on improving our products and features to enable our customers to build powerful NLP solutions. To help reflect some of the changes in our product development and research process, we have updated our [Terms of Use](https://cohere.ai/terms-of-use), [Privacy Policy](https://cohere.ai/privacy), and click-through [SaaS Agreement](https://cohere.ai/saas-agreement). Please carefully read and review these updates. By continuing to use Cohere’s services, you acknowledge that you have read, understood, and consent to all of the changes. If you have any questions or concerns about these updates, please contact us at support@cohere.ai.

### March 1, 2022

**Extremely Large (Beta) Release**

Our biggest and most performant generation model is now available. `Extremely Large (Beta)` outperforms our previous `large` model on a variety of downstream tasks including sentiment analysis, named entity recognition (NER) and common sense reasoning, as measured by our internal benchmarks. You can access `Extremely Large (Beta)` as `xlarge-20220301`. While in Beta, note that this model will have a maximum token length of 1024 tokens and maximum `num_generations` of 1.

### February 18, 2022

**Larger Representation Models**

Representation Models are now available in the sizes of `medium-20220217` and `large-20220217` as well as an updated version of `small-20220217`. Our previous `small` model will be available as `small-20211115`. In addition, the maximum tokens length per text has increased from 512 to 1024. We recommend keeping text lengths below 128 tokens for optimal performance; for any text longer than 128 tokens, the text is spliced and the resulting embeddings of each component are then averaged and returned.

### January 18, 2022

**Representation Finetuning**

Representation finetuning is now available in the dashboard. Upon uploading a `.csv` file in the format of `[Examples,Labels]` with a minimum of 200 examples, users will be able to finetune a baseline represenation model. For optimal finetunes, we recommend a minimum of 500 examples. Head on to [Represenation Finetunes](/finetuning-representation-models) for more information.

### January 16, 2022

**Embeds Max Batch Size**

The Embed endpoint's maximum number of strings per call has been lowered from 100 to 5.

### January 13, 2022

**Token Likelihood Endpoint**

The Likelihood endpoint has been removed. To retrieve token likelihoods, use the `return_likelihoods` parameter of the [Generate](/generate-reference) endpoint. Consequently, Likelihood is no longer a separate page in the Playground.

### December 3, 2021

**Generate Feedback**

We’ve added a feedback button to the Generate playground. If you come across a generation that is below your expected quality, classify the result to help improve the Cohere Platform.

### November 8, 2021

**Versioning**

The API version can be specified by setting the `Cohere-Version` header to the desired date, beginning with `2021-11-08`.  
The date must be a valid version.

**Multiple Generations**

Generate requests can use the `num_generations` parameter to specify how many generations should be returned.  
This feature requires the `Cohere-Version` header to be set with a version of `2021-11-08` or higher.

**Go SDK**

The Cohere Go SDK is now live. Head on to [API Reference](/api-reference) for installation instructions.

### November 5, 2021

**Marine Life Model Update**

We’ve adjusted our model offerings and bid farewell to the ocean animals. Instead, we’ll now provide access to three models: Small (previous seal), Medium (previous shark), and Large (previous orca). As a farewell to our ocean animals, we’ll donate $20,000 to an ocean wildlife foundation. Although code pointed toward our previous models won’t break, please update to our new offerings by December 1st.

### October 6, 2021

**Orca Model Update**

Calls to `orca` will now be faster, produce better generations, and have the context size of 2048 tokens.

This update is automatic. Any code pointed towards the `orca` model will start seeing these improvements immediately.

### September 20, 2021

**New & Improved Generation and Embedding Models**

`otter` (representation) is part of our new line of embedding models. Our tests show this line to run significantly faster and with improved performance on some tasks, and has an increased vector size of 1024 tokens. We suggest migrating all upcoming embedding usage to this model.

`shark` (generation) has significant advancements in speed and performance and now has double the capacity of tokens (2048). Any new calls to `shark` will be served by this new model.

### August 9, 2021

**Seal Model Update**

The updated seal model runs faster with a noticeable increase in quality, shown by our internal tests. This model is generally more performant across all tasks, especially those involving common sense, reasoning, and reading comprehension.

This update is automatic. Any code pointed towards the `seal` model will start seeing these improvements immediately.

### July 21, 2021

**Choose Best Likelihoods**

The Choose Best endpoint now returns `scores`, `tokens`, and `token_log_likelihoods`.

### July 13, 2021

**Finetuning Validation**

Sample validation is now available in the dashboard. Upon uploading a `.txt` file, there is an option to "Review your Finetine" to validate a set of sample training data.

developer hub and documentation for Cohere Test!
