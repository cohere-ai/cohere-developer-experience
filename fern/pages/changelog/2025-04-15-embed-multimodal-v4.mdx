---
title: "Announcing Embed Multimodal v4"
slug: "changelog/embed-multimodal-v4"
createdAt: "Tue April 15 2025 00:00:00 (MST)"
hidden: false
description: >-
  Release of Embed Multimodal v4, a performant search model, with Matryoshka embeddings and a 128k context length.
---

We’re thrilled to announce the release of Embed 4, the most recent entrant into the Embed family of enterprise-focused [large language models](https://docs.cohere.com/docs/the-cohere-platform#large-language-models-llms) (LLMs).

Embed v4 is Cohere’s most performant search model to date, and supports the following new features: 

1. Matryoshka Embeddings in the following dimensions: '[256, 512, 1024, 1536]'
2. Unified Embeddings produced from mixed modality input (i.e. a single payload of image(s) and text(s)) 
3. Context length of 128k 

Embed v4 achieves state of the art in the following areas: 

1. Text-to-text retrieval 
2. Text-to-image retrieval 
3. Text-to-mixed modality retrieval (from e.g. PDFs) 

Embed v4 is available today on the [Cohere Platform](), [AWS Sagemaker](), and [Azure AI Foundry](). For more information, check out our dedicated blog post [here]().