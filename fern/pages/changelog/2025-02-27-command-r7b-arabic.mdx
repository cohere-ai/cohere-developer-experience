---
title: "Cohere Releases Arabic-Optimized Command Model!"
slug: "changelog/command-r7b-arabic"
createdAt: "Wed Feb 26 2025 00:00:00 (MST)"
hidden: false
description: >-
  Release announcement for the Command R7B Arabic model
---
Cohere is thrilled to announce the release of Command R7B Arabic (`c4ai-command-r7b-12-2024`). This is an open weights research release of an advanced, 8-billion parameter custom model optimized for the Arabic language (MSA dialect), in addition to English. As with Cohere's other command models, this one comes with context length of 128,000 tokens; it excels at a number of critical enterprise tasks -- instruction following, length control, [retrieval-augmented generation (RAG)](https://docs.cohere.com/docs/retrieval-augmented-generation-rag), minimizing code-switching -- and it demonstrates excellent general purpose knowledge and understanding of the Arabic language and culture.

## Try Command R7B Arabic
If you want to try Command R7B Arabic, it's very easy: you can use it through the [Cohere playground](https://dashboard.cohere.com/playground/chat) or in our dedicated [Hugging Face Space](https://huggingface.co/spaces/CohereForAI/c4ai-command-r-plus).

Alternatively, you can use the model in your own code. To do that, first install the `transformers` library from its source repository:

```bash
pip install 'git+https://github.com/huggingface/transformers.git'
```

Then, use this Python snippet to run a simple text-generation task with the model:

```python
from transformers import AutoTokenizer, AutoModelForCausalLM

model_id = "CohereForAI/c4ai-command-r7b-12-2024"
tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(model_id)

# Format message with the c4ai-command-r7b-12-2024 chat template
messages = [{"role": "user", "content": "مرحبا، كيف حالك؟"}]
input_ids = tokenizer.apply_chat_template(
    messages,
    tokenize=True,
    add_generation_prompt=True,
    return_tensors="pt",
)

gen_tokens = model.generate(
    input_ids,
    max_new_tokens=100,
    do_sample=True,
    temperature=0.3,
)

gen_text = tokenizer.decode(gen_tokens[0])
print(gen_text)
```

## Chat Capabilities

Command R7B Arabic can be operated in two modes, "conversational" and "instruct" mode:

- *Conversational mode* conditions the model on interactive behaviour, meaning it is expected to reply in a conversational fashion, provide introductory statements and follow-up questions, and use Markdown as well as LaTeX where appropriate. This mode is optimized for interactive experiences, such as chatbots, where the model engages in dialogue.
- *Instruct mode* conditions the model to provide concise yet comprehensive responses, and to not use Markdown or LaTeX by default. This mode is designed for non-interactive, task-focused use cases such as extracting information, summarizing text, translation, and categorization.

<aside>
Note: Command R7B Arabic is delivered without a system preamble by default, though we encourage you to experiment with the conversational and instruct mode preambles. More information can be [found in our docs](https://docs.cohere.com/docs/command-r7b-hf).
</aside>

## Multilingual RAG Capabilities
Command R7B Arabic has been trained specifically for Arabic and English tasks, such as the _generation_ step of Retrieval Augmented Generation (RAG).

Command R7B Arabic's RAG functionality is supported through chat templates in Transformers. Using our RAG chat template, the model takes a conversation (with an optional user-supplied system preamble) and a list of document snippets as input. The resulting output contains a response with in-line citations. Here's what that looks like:

```python
# Define conversation input
conversation = [
    {
        "role": "user",
        "content": "اقترح طبقًا يمزج نكهات من عدة دول عربية",
    }
]

# Define documents for retrieval-based generation
documents = [
    {
        "heading": "المطبخ العربي: أطباقنا التقليدية",
        "body": "يشتهر المطبخ العربي بأطباقه الغنية والنكهات الفريدة. في هذا المقال، سنستكشف ...",
    },
    {
        "heading": "وصفة اليوم: مقلوبة",
        "body": "المقلوبة هي طبق فلسطيني تقليدي، يُحضر من الأرز واللحم أو الدجاج والخضروات. في وصفتنا اليوم ...",
    },
]

# Get the RAG prompt
input_prompt = tokenizer.apply_chat_template(
    conversation=conversation,
    documents=documents,
    tokenize=False,
    add_generation_prompt=True,
    return_tensors="pt",
)
# Tokenize the prompt
input_ids = tokenizer.encode_plus(input_prompt, return_tensors="pt")
```

You can then generate text from this input as normal.

## Notes on Usage

We recommend document snippets be short chunks (around 100-400 words per chunk) instead of long documents. They should also be formatted as key-value pairs, where the keys are short descriptive strings and the values are either text or semi-structured. 

You may find that simply including relevant documents directly in a user message works as well as or better than using the `documents` parameter to render the special RAG template (though the template is a strong default for those wanting [citations](https://docs.cohere.com/docs/retrieval-augmented-generation-rag#citation-modes)). We encourage users to experiment with both approaches, and to evaluate which mode works best for their specific use case.








