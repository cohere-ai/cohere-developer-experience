{
  "custompage": {
    "metadata": {
      "image": [],
      "title": "",
      "description": "",
      "keywords": ""
    },
    "algolia": {
      "recordCount": 9,
      "publishPending": false,
      "updatedAt": "2024-07-11T01:20:30.055Z"
    },
    "title": "Analysis of Form 10-K/10-Q Using Cohere and RAG",
    "slug": "analysis-of-financial-forms",
    "body": "[block:html]\n{\n  \"html\": \"<div class=\\\"cookbook-nav-container\\\">\\n  <a href=\\\"/page/cookbooks\\\" class=\\\"back-button pt-10 group inline-block cursor-pointer font-medium \\\" rel=\\\"noreferrer\\\"\\n    target=\\\"_self\\\">\\n    <div class=\\\"pr-1 inline-block group-hover:no-underline\\\">\\n      <svg width=\\\"11.8\\\" height=\\\"11\\\" viewBox=\\\"0 0 14 13\\\" fill=\\\"none\\\" xmlns=\\\"http://www.w3.org/2000/svg\\\">\\n        <path\\n          d=\\\"M1.1554 7.20808C1.35066 7.40335 1.66724 7.40335 1.8625 7.20808L7.18477 1.88582C7.38003 1.69055 7.38003 1.37397 7.18477 1.17871L6.83121 0.825157C6.63595 0.629895 6.31937 0.629896 6.12411 0.825157L0.801842 6.14742C0.60658 6.34269 0.60658 6.65927 0.801842 6.85453L1.1554 7.20808Z\\\"\\n          fill=\\\"currentColor\\\" />\\n        <path\\n          d=\\\"M1.1554 5.79226C1.35066 5.597 1.66724 5.597 1.8625 5.79226L7.18477 11.1145C7.38003 11.3098 7.38003 11.6264 7.18477 11.8216L6.83121 12.1752C6.63595 12.3705 6.31937 12.3705 6.12411 12.1752L0.801842 6.85292C0.60658 6.65766 0.60658 6.34108 0.801842 6.14582L1.1554 5.79226Z\\\"\\n          fill=\\\"currentColor\\\" />\\n        <path\\n          d=\\\"M2.52491 6.23674C2.52492 5.9606 2.74878 5.73675 3.02491 5.73675H6.28412C6.4513 5.73675 6.60742 5.8203 6.70015 5.95941L7.03347 6.45941C7.25499 6.79169 7.01679 7.23675 6.61745 7.23675H3.0249C2.74876 7.23675 2.5249 7.01289 2.5249 6.73674L2.52491 6.23674Z\\\"\\n          fill=\\\"currentColor\\\" />\\n        <path\\n          d=\\\"M13.5517 6.73676C13.5517 7.0129 13.3278 7.23675 13.0517 7.23675H8.79246C8.62528 7.23675 8.46916 7.1532 8.37643 7.0141L8.04311 6.5141C7.8216 6.18182 8.05979 5.73675 8.45914 5.73675H13.0517C13.3278 5.73675 13.5517 5.96062 13.5517 6.23676L13.5517 6.73676Z\\\"\\n          fill=\\\"currentColor\\\" />\\n      </svg>\\n    </div>\\n    Back to Cookbooks\\n  </a>\\n\\n  <a href=https://github.com/cohere-ai/cohere-developer-experience/blob/main/notebooks/guides/Analysis_of_Form_10_K_Using_Cohere_and_RAG.ipynb class=\\\"github-button pt-10 group inline-block cursor-pointer font-medium \\\" rel=\\\"noreferrer\\\"\\n    target=\\\"_blank\\\">\\n    Open in GitHub\\n    <div class=\\\"pl-1 inline-block group-hover:no-underline\\\">\\n      <svg width=\\\"14\\\" height=\\\"10\\\" viewBox=\\\"0 0 14 10\\\" fill=\\\"none\\\" xmlns=\\\"http://www.w3.org/2000/svg\\\">\\n        <path\\n          d=\\\"M8.63218 0.366821C8.35604 0.366821 8.13218 0.590679 8.13218 0.866821V8.39364C8.13218 8.66978 8.35604 8.89364 8.63218 8.89364H9.13218C9.40832 8.89364 9.63218 8.66978 9.63218 8.39364V0.866821C9.63218 0.590678 9.40832 0.366821 9.13218 0.366821H8.63218Z\\\"\\n          fill=\\\"currentColor\\\" />\\n        <path\\n          d=\\\"M9.63332 1.36796C9.63332 1.6441 9.40946 1.86796 9.13332 1.86796H1.6065C1.33035 1.86796 1.1065 1.6441 1.1065 1.36796V0.867956C1.1065 0.591813 1.33035 0.367956 1.6065 0.367956H9.13332C9.40946 0.367956 9.63332 0.591813 9.63332 0.867956V1.36796Z\\\"\\n          fill=\\\"currentColor\\\" />\\n        <path\\n          d=\\\"M8.35063 2.02206C8.54588 2.21732 8.54588 2.5339 8.35062 2.72916L6.04601 5.03377C5.9278 5.15198 5.75833 5.20329 5.59439 5.1705L5.00515 5.05264C4.61356 4.97432 4.46728 4.49118 4.74966 4.2088L7.28997 1.66849C7.48523 1.47323 7.80182 1.47323 7.99708 1.6685L8.35063 2.02206Z\\\"\\n          fill=\\\"currentColor\\\" />\\n        <path\\n          d=\\\"M0.199967 9.46558C0.0047119 9.27032 0.0047151 8.95374 0.199974 8.75848L3.21169 5.74677C3.3299 5.62855 3.49938 5.57724 3.66331 5.61003L4.25256 5.72789C4.64414 5.80621 4.79042 6.28935 4.50804 6.57173L1.26063 9.81915C1.06536 10.0144 0.748774 10.0144 0.553513 9.81914L0.199967 9.46558Z\\\"\\n          fill=\\\"currentColor\\\" />\\n      </svg>\\n    </div>\\n  </a>\\n</div>\\n\\n<div>\\n  <h1>Analysis of Form 10-K/10-Q Using Cohere and RAG</h1>\\n</div>\\n\\n<!-- Authors -->\\n<div class=\\\"authors-container\\\">\\n  \\n  <div class=\\\"author-container\\\">\\n    <img class=\\\"author-image\\\" src=\\\"https://files.readme.io/bf2c763-Alex.jpg\\\" alt=\\\"Alex Barbet\\\" />\\n    <p class=\\\"author-name\\\">Alex Barbet</p>\\n  </div>\\n  \\n</div>\\n\\n<style>\\n  .header {\\n    padding: 9px 0 17px 0;\\n    display: flex;\\n    flex-direction: column;\\n  }\\n\\n  a[href],\\n  .field-description a:not([href=\\\"\\\"]),\\n  .markdown-body a[href],\\n  .markdown-body a:not([href=\\\"\\\"]) {\\n    text-decoration: none;\\n  }\\n\\n  #content {\\n    padding: 0 32px;\\n  }\\n\\n  #content-head {\\n    display: none;\\n  }\\n\\n  .guide-page-title {\\n    font-size: 29px !important;\\n  }\\n\\n  .back-button .github-button {\\n    border-radius: 0 !important;\\n    border-width: 0 !important;\\n    background-color: inherit !important;\\n  }\\n\\n  .cookbook-nav-container {\\n    display: flex;\\n    flex-direction: row;\\n    justify-content: space-between;\\n    align-items: center;\\n  }\\n\\n  .authors-container {\\n    display: flex;\\n    flex-direction: row;\\n    flex-flow: row wrap;\\n    gap: 2px 14px;\\n    margin-top: 8px;\\n  }\\n\\n  .author-container {\\n    line-height: 1.5em;\\n    display: flex;\\n    flex-direction: row;\\n    vertical-align: middle;\\n    flex-shrink: 0;\\n  }\\n\\n  .author-image {\\n    height: 1.5em;\\n    margin: 0px 6px 6px 0px !important;\\n    border-radius: 50%;\\n  }\\n\\n  .author-name {\\n    white-space: nowrap;\\n    vertical-align: middle;\\n  }\\n\\n  @media only screen and (min-width: 620px) {\\n    .guide-page-title {\\n      width: 70%;\\n    }\\n\\n    .header {\\n      display: flex;\\n      flex-direction: row;\\n      align-items: center;\\n      justify-content: space-between;\\n      padding: 9px 0 17px 0;\\n    }\\n\\n    .git--button {\\n      width: 145px !important;\\n      display: flex;\\n      flex-direction: row;\\n      justify-content: center;\\n      align-items: center;\\n      padding: 8px 16px;\\n\\n      width: 154px;\\n      height: 35px;\\n\\n      background: #D4D9D4;\\n      border: 1px solid #9DAAA4;\\n      border-radius: 6px;\\n    }\\n  }\\n\\n\\n  @media only screen and (min-width: 1024px) {\\n    .guide-page-title {\\n      font-size: 46px !important;\\n    }\\n\\n    .header {\\n      padding: 9px 0 32px 0;\\n    }\\n  }\\n</style>\"\n}\n[/block]\n\n\n## **Getting Started**\n\nYou may use this script to jumpstart financial analysis of 10-Ks or 10-Qs with Cohere's Command model.\n\nThis cookbook relies on helpful tooling from LlamaIndex, as well as our Cohere SDK. If you're familiar with LlamaIndex, it should be easy to slot this process into your own productivity flows.\n\n\n```python\n%%capture\n!sudo apt install tesseract-ocr poppler-utils\n!pip install \"cohere<5\" langchain llama-index llama-index-embeddings-cohere llama-index-postprocessor-cohere-rerank pytesseract pdf2image\n```\n\n\n```python\n# Due to compatibility issues, we need to do imports like this\nfrom llama_index.core.schema import TextNode\n\n%%capture\n!pip install unstructured\n```\n\n\n```python\nimport cohere\nfrom getpass import getpass\n\n# Set up Cohere client\nCOHERE_API_KEY = getpass(\"Enter your Cohere API key: \")\n\n# Instantiate a client to communicate with Cohere's API using our Python SDK\nco = cohere.Client(COHERE_API_KEY)\n\n```\n\n\n\n<style>\n  pre {\n      white-space: pre-wrap;\n  }\n</style>\n\n\n\n    Enter your Cohere API key: ··········\n\n\n## **Step 1: Loading a 10-K**\n\nYou may run the following cells to load a 10-K that has already been preprocessed with OCR.\n\n> 💡 If you'd like to run the OCR pipeline yourself, you can find more info in the section titled **PDF to Text using OCR and `pdf2image`**.\n\n\n```python\n# Using langchain here since they have access to the Unstructured Data Loader powered by unstructured.io\nfrom langchain_community.document_loaders import UnstructuredURLLoader\n\n# Load up Airbnb's 10-K from this past fiscal year (filed in 2024)\n# Feel free to fill in some other EDGAR path\nurl = \"https://www.sec.gov/Archives/edgar/data/1559720/000155972024000006/abnb-20231231.htm\"\nloader = UnstructuredURLLoader(urls=[url], headers={\"User-Agent\": \"cohere cohere@cohere.com\"})\ndocuments = loader.load()\n\nedgar_10k = documents[0].page_content\n\n# Load the document(s) as simple text nodes, to be passed to the tokenization processor\nnodes = [TextNode(text=document.page_content, id_=f\"doc_{i}\") for i, document in enumerate(documents)]\n```\n\n\n\n<style>\n  pre {\n      white-space: pre-wrap;\n  }\n</style>\n\n\n\n    [nltk_data] Downloading package averaged_perceptron_tagger to\n    [nltk_data]     /root/nltk_data...\n    [nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n\n\nWe'll need to convert the text into chunks of a certain size in order for the Cohere embedding model to properly ingest them down the line.\n\nWe choose to use LlamaIndex's `SentenceSplitter` in this case in order to get these chunks. We must pass a tokenization callable, which we can do using the `transformers` library.\n\nYou may also apply further transformations from the LlamaIndex repo if you so choose. Take a look at the [docs](https://docs.llamaindex.ai/en/stable/understanding/loading/loading.html) for inspiration on what is possible with transformations.\n\n\n```python\nfrom llama_index.core.ingestion import IngestionPipeline\nfrom llama_index.core.node_parser import SentenceSplitter\n\nfrom transformers import AutoTokenizer\n\nmodel_id = \"CohereForAI/c4ai-command-r-v01\"\ntokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n\n# TODO: replace with a HF implementation so this is much faster. We'll\n# presumably release it when we OS the model\ntokenizer_fn = lambda x: tokenizer(x).input_ids if len(x) > 0 else []\n\npipeline = IngestionPipeline(\n    transformations=[\n        SentenceSplitter(chunk_size=512, chunk_overlap=0, tokenizer=tokenizer_fn)\n    ]\n)\n\n# Run the pipeline to transform the text\nnodes = pipeline.run(nodes=nodes)\n```\n\n\n\n<style>\n  pre {\n      white-space: pre-wrap;\n  }\n</style>\n\n\n\n    /usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n    The secret `HF_TOKEN` does not exist in your Colab secrets.\n    To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n    You will be able to reuse this secret in all of your notebooks.\n    Please note that authentication is recommended but still optional to access public models or datasets.\n      warnings.warn(\n\n\n\n    tokenizer_config.json:   0%|          | 0.00/7.92k [00:00<?, ?B/s]\n\n\n\n    tokenization_cohere_fast.py:   0%|          | 0.00/43.7k [00:00<?, ?B/s]\n\n\n\n    configuration_cohere.py:   0%|          | 0.00/7.37k [00:00<?, ?B/s]\n\n\n    A new version of the following files was downloaded from https://huggingface.co/CohereForAI/c4ai-command-r-v01:\n    - configuration_cohere.py\n    . Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n    A new version of the following files was downloaded from https://huggingface.co/CohereForAI/c4ai-command-r-v01:\n    - tokenization_cohere_fast.py\n    - configuration_cohere.py\n    . Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n\n\n\n    tokenizer.json:   0%|          | 0.00/12.8M [00:00<?, ?B/s]\n\n\n\n    special_tokens_map.json:   0%|          | 0.00/429 [00:00<?, ?B/s]\n\n\n    Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n\n\n## **Step 2: Load document into a LlamaIndex vector store**\n\nLoading the document into a LlamaIndex vector store will allow us to use the Cohere embedding model and rerank model to retrieve the relevant parts of the form to pass into Command.\n\n\n```python\nfrom llama_index.core import Settings, VectorStoreIndex\n\nfrom llama_index.postprocessor.cohere_rerank import CohereRerank\n\nfrom llama_index.embeddings.cohere import CohereEmbedding\n\n# Instantiate the embedding model\nembed_model = CohereEmbedding(cohere_api_key=COHERE_API_KEY)\n\n# Global settings\nSettings.chunk_size = 512\nSettings.embed_model = embed_model\n\n# Create the vector store\nindex = VectorStoreIndex(nodes)\n\nretriever = index.as_retriever(similarity_top_k=30) # Change to whatever top_k you want\n\n# Instantiate the reranker\nrerank = CohereRerank(api_key=COHERE_API_KEY, top_n=15)\n\n# Function `retrieve` is ready, using both Cohere embeddings for similarity search as well as\nretrieve = lambda query: rerank.postprocess_nodes(retriever.retrieve(query), query_str=query)\n```\n\n\n\n<style>\n  pre {\n      white-space: pre-wrap;\n  }\n</style>\n\n\n\n## **Step 3: Query generation and retrieval**\n\nIn order to do RAG, we need a query or a set of queries to actually _do_ the retrieval step. As is standard in RAG settings, we'll use Command to generate those queries for us. Then, we'll use those queries along with the LlamaIndex retriever we built earlier to retrieve the most relevant pieces of the 10-K.\n\nTo learn more about document mode and query generation, check out [our documentation](https://docs.cohere.com/docs/retrieval-augmented-generation-rag).\n\n\n```python\nPROMPT = \"List the overall revenue numbers for 2021, 2022, and 2023 in the 10-K as bullet points, then explain the revenue growth trends.\"\n\n# Get queries to run against our index from the command-nightly model\nr = co.chat(PROMPT, model=\"command-r\", search_queries_only=True)\nif r.search_queries:\n    queries = [q[\"text\"] for q in r.search_queries]\nelse:\n    print(\"No queries returned by the model\")\n```\n\n\n\n<style>\n  pre {\n      white-space: pre-wrap;\n  }\n</style>\n\n\n\nNow, with the queries in hand, we search against our vector index.\n\n\n```python\n# Convenience function for formatting documents\ndef format_for_cohere_client(nodes_):\n    return [\n        {\n            \"text\": node.node.text,\n            \"llamaindex_id\": node.node.id_,\n        }\n        for node\n        in nodes_\n    ]\n\n\ndocuments = []\n# Retrieve a set of chunks from the vector index and append them to the list of\n# documents that should be included in the final RAG step\nfor query in queries:\n    ret_nodes = retrieve(query)\n    documents.extend(format_for_cohere_client(ret_nodes))\n\n# One final dedpulication step in case multiple queries return the same chunk\ndocuments = [dict(t, id=f\"doc_{i}\") for i, t in enumerate({tuple(d.items()) for d in documents})]\n```\n\n\n\n<style>\n  pre {\n      white-space: pre-wrap;\n  }\n</style>\n\n\n\n## **Step 4: Make a RAG request to Command using document mode**\n\nNow that we have our nicely formatted chunks from the 10-K, we can pass them directly into Command using the Cohere SDK. By passing the chunks into the `documents` kwarg, we enable document mode, which will perform grounded inference on the documents you pass in.\n\nYou can see this for yourself by inspecting the `response.citations` field to check where the model is citing from.\n\nYou can learn more about the `chat` endpoint by checking out the API reference [here](https://docs.cohere.com/reference/chat).\n\n\n```python\n# Make a request to the model\nresponse = co.chat(\n    message=PROMPT,\n    model=\"command-r\",\n    temperature=0.3,\n    documents=documents,\n    prompt_truncation=\"AUTO\"\n)\n\nprint(response.text)\n```\n\n\n\n<style>\n  pre {\n      white-space: pre-wrap;\n  }\n</style>\n\n\n\n    Here are the overall revenue numbers for the years 2021, 2022, and 2023 as bullet points:\n    - 2021: $5,992 million\n    - 2022: $8,399 million\n    - 2023: $9,917 million\n    \n    Revenue increased by 18% in 2023 compared to 2022, primarily due to a 14% increase in Nights and Experiences Booked, which reached 54.5 million. This, combined with higher average daily rates, resulted in a 16% increase in Gross Booking Value, which reached $10.0 billion. \n    \n    The revenue growth trend demonstrates sustained strong travel demand. On a constant-currency basis, revenue increased by 17% in 2023 compared to the previous year.\n    \n    Other factors influencing the company's financial performance are described outside of the revenue growth trends.\n\n\n\n```python\n# Helper function for displaying response WITH citations\ndef insert_citations(text: str, citations: list[dict]):\n    \"\"\"\n    A helper function to pretty print citations.\n    \"\"\"\n    offset = 0\n    # Process citations in the order they were provided\n    for citation in citations:\n        # Adjust start/end with offset\n        start, end = citation['start'] + offset, citation['end'] + offset\n        cited_docs = [doc[4:] for doc in citation[\"document_ids\"]]\n        # Shorten citations if they're too long for convenience\n        if len(cited_docs) > 3:\n            placeholder = \"[\" + \", \".join(cited_docs[:3]) + \"...]\"\n        else:\n            placeholder = \"[\" + \", \".join(cited_docs) + \"]\"\n        # ^ doc[4:] removes the 'doc_' prefix, and leaves the quoted document\n        modification = f'{text[start:end]} {placeholder}'\n        # Replace the cited text with its bolded version + placeholder\n        text = text[:start] + modification + text[end:]\n        # Update the offset for subsequent replacements\n        offset += len(modification) - (end - start)\n\n    return text\n\nprint(insert_citations(response.text, response.citations))\n```\n\n\n\n<style>\n  pre {\n      white-space: pre-wrap;\n  }\n</style>\n\n\n\n    Here are the overall revenue numbers for the years 2021, 2022, and 2023 as bullet points:\n    - 2021: $5,992 million [13]\n    - 2022: $8,399 million [13]\n    - 2023: $9,917 million [13]\n    \n    Revenue increased by 18% in 2023 [11] compared to 2022, primarily due to a 14% increase in Nights and Experiences Booked [11], which reached 54.5 million. [11] This, combined with higher average daily rates [11], resulted in a 16% increase in Gross Booking Value [11], which reached $10.0 billion. [11] \n    \n    The revenue growth trend demonstrates sustained strong travel demand. [11] On a constant-currency basis [11], revenue increased by 17% in 2023 [11] compared to the previous year.\n    \n    Other factors [8, 14] influencing the company's financial performance are described outside of the revenue growth trends. [8, 14]\n\n\n# **Appendix**\n\n## PDF to Text using OCR and `pdf2image`\n\nThis method will be required for any PDFs you have that need to be converted to text.\n\n**WARNING**: this process can take a long time without the proper optimizations. We have provided a snippet for your use below, but use at your own risk.\n\nTo go from PDF to text with PyTesseract, there is an intermediary step of converting the PDF to an image first, then passing that image into the OCR package, as OCR is usually only available for images.\n\nTo do this, we use `pdf2image`, which uses `poppler` behind the scenes to convert the PDF into a PNG. From there, we can pass the image (which is a PIL Image object) directly into the OCR tool.\n\n\n```python\nimport pytesseract\nfrom pdf2image import convert_from_path\n\n# pdf2image extracts as a list of PIL.Image objects\n# TODO: host this PDF somewhere\npages = convert_from_path(\"/content/uber_10k.pdf\")\n\n# We access the only page in this sample PDF by indexing at 0\npages = [pytesseract.image_to_string(page) for page in pages]\n```\n\n## Token count / price comparison and latency\n\n\n```python\ndef get_response(prompt, rag):\n    if rag:\n        # Get queries to run against our index from the command-nightly model\n        r = co.chat(prompt, model=\"command-r\", search_queries_only=True)\n        if r.search_queries:\n            queries = [q[\"text\"] for q in r.search_queries]\n        else:\n            print(\"No queries returned by the model\")\n\n        documents = []\n        # Retrieve a set of chunks from the vector index and append them to the list of\n        # documents that should be included in the final RAG step\n        for query in queries:\n            ret_nodes = retrieve(query)\n            documents.extend(format_for_cohere_client(ret_nodes))\n\n        # One final dedpulication step in case multiple queries return the same chunk\n        documents = [dict(t) for t in {tuple(d.items()) for d in documents}]\n\n        # Make a request to the model\n        response = co.chat(\n            message=prompt,\n            model=\"command-r\",\n            temperature=0.3,\n            documents=documents,\n            prompt_truncation=\"AUTO\"\n        )\n    else:\n        response = co.chat(\n            message=prompt,\n            model=\"command-r\",\n            temperature=0.3,\n        )\n\n    return response\n```\n\n\n\n<style>\n  pre {\n      white-space: pre-wrap;\n  }\n</style>\n\n\n\n\n```python\nprompt_template = \"\"\"# financial form 10-K\n{tenk}\n\n# question\n{question}\"\"\"\n\nfull_context_prompt = prompt_template.format(tenk=edgar_10k, question=PROMPT)\n```\n\n\n\n<style>\n  pre {\n      white-space: pre-wrap;\n  }\n</style>\n\n\n\n\n```python\nr1 = get_response(PROMPT, rag=True)\nr2 = get_response(full_context_prompt, rag=False)\n```\n\n\n\n<style>\n  pre {\n      white-space: pre-wrap;\n  }\n</style>\n\n\n\n\n```python\ndef get_price(r):\n    return (r.token_count[\"prompt_tokens\"] * 0.5 / 10e6) + (r.token_count[\"response_tokens\"] * 1.5 / 10e6)\n```\n\n\n\n<style>\n  pre {\n      white-space: pre-wrap;\n  }\n</style>\n\n\n\n\n```python\nrag_price = get_price(r1)\nfull_context_price = get_price(r2)\n\nprint(f\"RAG is {(full_context_price - rag_price) / full_context_price:.0%} cheaper than full context\")\n```\n\n\n\n<style>\n  pre {\n      white-space: pre-wrap;\n  }\n</style>\n\n\n\n    RAG is 93% cheaper than full context\n\n\n\n```python\n%timeit get_response(PROMPT, rag=True)\n```\n\n\n\n<style>\n  pre {\n      white-space: pre-wrap;\n  }\n</style>\n\n\n\n    14.9 s ± 1.4 s per loop (mean ± std. dev. of 7 runs, 1 loop each)\n\n\n\n```python\n%timeit get_response(full_context_prompt, rag=False)\n```\n\n\n\n<style>\n  pre {\n      white-space: pre-wrap;\n  }\n</style>\n\n\n\n    22.7 s ± 7.43 s per loop (mean ± std. dev. of 7 runs, 1 loop each)",
    "html": "",
    "htmlmode": false,
    "fullscreen": false,
    "hidden": true,
    "revision": 3,
    "_id": "665e06ba051b6f006bc1a05f",
    "__v": 0,
    "createdAt": "2024-06-03T18:08:58.434Z",
    "lastUpdatedHash": "bd791cda5178ead4a3c157c3d40f7a0646361920",
    "project": "62cde2919aafea009aefb289",
    "updatedAt": "2024-07-11T01:20:30.055Z",
    "user": "5af39863989da435b05d284e"
  },
  "meta": {
    "user": {
      "allowedProjects": ["cohere-ai", "cohere-enterprise"],
      "apiKey": "",
      "email": "andrewjiang@hey.com",
      "name": "Andrew Jiang",
      "version": 1,
      "Name": "Andrew Jiang",
      "Email": "andrewjiang@hey.com",
      "APIKey": "",
      "AllowedProjects": ["cohere-ai", "cohere-enterprise"]
    },
    "baseUrl": "/",
    "hidden": true,
    "title": "Analysis of Form 10-K/10-Q Using Cohere and RAG",
    "metaTitle": "Analysis of Form 10-K/10-Q Using Cohere and RAG",
    "keywords": "",
    "description": "",
    "image": [],
    "slug": "analysis-of-financial-forms",
    "type": "custompage",
    "full": false
  }
}
