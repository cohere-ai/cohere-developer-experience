{
  "custompage": {
    "metadata": {
      "image": [],
      "title": "",
      "description": "",
      "keywords": ""
    },
    "algolia": {
      "recordCount": 15,
      "publishPending": false,
      "updatedAt": "2024-07-11T01:20:29.577Z"
    },
    "title": "Agentic RAG for PDFs with mixed data",
    "slug": "agentic-rag-mixed-data",
    "body": "[block:html]\n{\n  \"html\": \"<div class=\\\"cookbook-nav-container\\\">\\n  <a href=\\\"/page/cookbooks\\\" class=\\\"back-button pt-10 group inline-block cursor-pointer font-medium \\\" rel=\\\"noreferrer\\\"\\n    target=\\\"_self\\\">\\n    <div class=\\\"pr-1 inline-block group-hover:no-underline\\\">\\n      <svg width=\\\"11.8\\\" height=\\\"11\\\" viewBox=\\\"0 0 14 13\\\" fill=\\\"none\\\" xmlns=\\\"http://www.w3.org/2000/svg\\\">\\n        <path\\n          d=\\\"M1.1554 7.20808C1.35066 7.40335 1.66724 7.40335 1.8625 7.20808L7.18477 1.88582C7.38003 1.69055 7.38003 1.37397 7.18477 1.17871L6.83121 0.825157C6.63595 0.629895 6.31937 0.629896 6.12411 0.825157L0.801842 6.14742C0.60658 6.34269 0.60658 6.65927 0.801842 6.85453L1.1554 7.20808Z\\\"\\n          fill=\\\"currentColor\\\" />\\n        <path\\n          d=\\\"M1.1554 5.79226C1.35066 5.597 1.66724 5.597 1.8625 5.79226L7.18477 11.1145C7.38003 11.3098 7.38003 11.6264 7.18477 11.8216L6.83121 12.1752C6.63595 12.3705 6.31937 12.3705 6.12411 12.1752L0.801842 6.85292C0.60658 6.65766 0.60658 6.34108 0.801842 6.14582L1.1554 5.79226Z\\\"\\n          fill=\\\"currentColor\\\" />\\n        <path\\n          d=\\\"M2.52491 6.23674C2.52492 5.9606 2.74878 5.73675 3.02491 5.73675H6.28412C6.4513 5.73675 6.60742 5.8203 6.70015 5.95941L7.03347 6.45941C7.25499 6.79169 7.01679 7.23675 6.61745 7.23675H3.0249C2.74876 7.23675 2.5249 7.01289 2.5249 6.73674L2.52491 6.23674Z\\\"\\n          fill=\\\"currentColor\\\" />\\n        <path\\n          d=\\\"M13.5517 6.73676C13.5517 7.0129 13.3278 7.23675 13.0517 7.23675H8.79246C8.62528 7.23675 8.46916 7.1532 8.37643 7.0141L8.04311 6.5141C7.8216 6.18182 8.05979 5.73675 8.45914 5.73675H13.0517C13.3278 5.73675 13.5517 5.96062 13.5517 6.23676L13.5517 6.73676Z\\\"\\n          fill=\\\"currentColor\\\" />\\n      </svg>\\n    </div>\\n    Back to Cookbooks\\n  </a>\\n\\n  <a href=https://github.com/cohere-ai/cohere-developer-experience/blob/main/notebooks/agents/agentic-RAG/agentic_rag_langchain.ipynb class=\\\"github-button pt-10 group inline-block cursor-pointer font-medium \\\" rel=\\\"noreferrer\\\"\\n    target=\\\"_blank\\\">\\n    Open in GitHub\\n    <div class=\\\"pl-1 inline-block group-hover:no-underline\\\">\\n      <svg width=\\\"14\\\" height=\\\"10\\\" viewBox=\\\"0 0 14 10\\\" fill=\\\"none\\\" xmlns=\\\"http://www.w3.org/2000/svg\\\">\\n        <path\\n          d=\\\"M8.63218 0.366821C8.35604 0.366821 8.13218 0.590679 8.13218 0.866821V8.39364C8.13218 8.66978 8.35604 8.89364 8.63218 8.89364H9.13218C9.40832 8.89364 9.63218 8.66978 9.63218 8.39364V0.866821C9.63218 0.590678 9.40832 0.366821 9.13218 0.366821H8.63218Z\\\"\\n          fill=\\\"currentColor\\\" />\\n        <path\\n          d=\\\"M9.63332 1.36796C9.63332 1.6441 9.40946 1.86796 9.13332 1.86796H1.6065C1.33035 1.86796 1.1065 1.6441 1.1065 1.36796V0.867956C1.1065 0.591813 1.33035 0.367956 1.6065 0.367956H9.13332C9.40946 0.367956 9.63332 0.591813 9.63332 0.867956V1.36796Z\\\"\\n          fill=\\\"currentColor\\\" />\\n        <path\\n          d=\\\"M8.35063 2.02206C8.54588 2.21732 8.54588 2.5339 8.35062 2.72916L6.04601 5.03377C5.9278 5.15198 5.75833 5.20329 5.59439 5.1705L5.00515 5.05264C4.61356 4.97432 4.46728 4.49118 4.74966 4.2088L7.28997 1.66849C7.48523 1.47323 7.80182 1.47323 7.99708 1.6685L8.35063 2.02206Z\\\"\\n          fill=\\\"currentColor\\\" />\\n        <path\\n          d=\\\"M0.199967 9.46558C0.0047119 9.27032 0.0047151 8.95374 0.199974 8.75848L3.21169 5.74677C3.3299 5.62855 3.49938 5.57724 3.66331 5.61003L4.25256 5.72789C4.64414 5.80621 4.79042 6.28935 4.50804 6.57173L1.26063 9.81915C1.06536 10.0144 0.748774 10.0144 0.553513 9.81914L0.199967 9.46558Z\\\"\\n          fill=\\\"currentColor\\\" />\\n      </svg>\\n    </div>\\n  </a>\\n</div>\\n\\n<div>\\n  <h1>Agentic RAG for PDFs with mixed data</h1>\\n</div>\\n\\n<!-- Authors -->\\n<div class=\\\"authors-container\\\">\\n  \\n  <div class=\\\"author-container\\\">\\n    <img class=\\\"author-image\\\" src=\\\"https://files.readme.io/d17fc44-Shaan.jpg\\\" alt=\\\"Shaan Desai\\\" />\\n    <p class=\\\"author-name\\\">Shaan Desai</p>\\n  </div>\\n  \\n</div>\\n\\n<style>\\n  .header {\\n    padding: 9px 0 17px 0;\\n    display: flex;\\n    flex-direction: column;\\n  }\\n\\n  a[href],\\n  .field-description a:not([href=\\\"\\\"]),\\n  .markdown-body a[href],\\n  .markdown-body a:not([href=\\\"\\\"]) {\\n    text-decoration: none;\\n  }\\n\\n  #content {\\n    padding: 0 32px;\\n  }\\n\\n  #content-head {\\n    display: none;\\n  }\\n\\n  .guide-page-title {\\n    font-size: 29px !important;\\n  }\\n\\n  .back-button .github-button {\\n    border-radius: 0 !important;\\n    border-width: 0 !important;\\n    background-color: inherit !important;\\n  }\\n\\n  .cookbook-nav-container {\\n    display: flex;\\n    flex-direction: row;\\n    justify-content: space-between;\\n    align-items: center;\\n  }\\n\\n  .authors-container {\\n    display: flex;\\n    flex-direction: row;\\n    flex-flow: row wrap;\\n    gap: 2px 14px;\\n    margin-top: 8px;\\n  }\\n\\n  .author-container {\\n    line-height: 1.5em;\\n    display: flex;\\n    flex-direction: row;\\n    vertical-align: middle;\\n    flex-shrink: 0;\\n  }\\n\\n  .author-image {\\n    height: 1.5em;\\n    margin: 0px 6px 6px 0px !important;\\n    border-radius: 50%;\\n  }\\n\\n  .author-name {\\n    white-space: nowrap;\\n    vertical-align: middle;\\n  }\\n\\n  @media only screen and (min-width: 620px) {\\n    .guide-page-title {\\n      width: 70%;\\n    }\\n\\n    .header {\\n      display: flex;\\n      flex-direction: row;\\n      align-items: center;\\n      justify-content: space-between;\\n      padding: 9px 0 17px 0;\\n    }\\n\\n    .git--button {\\n      width: 145px !important;\\n      display: flex;\\n      flex-direction: row;\\n      justify-content: center;\\n      align-items: center;\\n      padding: 8px 16px;\\n\\n      width: 154px;\\n      height: 35px;\\n\\n      background: #D4D9D4;\\n      border: 1px solid #9DAAA4;\\n      border-radius: 6px;\\n    }\\n  }\\n\\n\\n  @media only screen and (min-width: 1024px) {\\n    .guide-page-title {\\n      font-size: 46px !important;\\n    }\\n\\n    .header {\\n      padding: 9px 0 32px 0;\\n    }\\n  }\\n</style>\"\n}\n[/block]\n\n\n## Motivation\nRetrieval-augmented generation (RAG) allows language models to generate grounded answers to questions about documents. However, the complexity of the documents can significantly influence overall RAG performance. For instance, the documents may be PDFs that contain a mix of text and tables. \n\nMore broadly, the implementation of a RAG pipeline - including parsing and chunking of documents, along with the embedding and retrieval of the chunks - is critical to the accuracy of grounded answers. Additionally, it is sometimes not sufficient to merely retrieve the answers; a user may want further postprocessing performed on the output. This use case would benefit from giving the model access to tools.\n\n## Objective\nIn this notebook, we will guide you through best practices for setting up a RAG pipeline to process documents that contain both tables and text. We will also demonstrate how to create a [ReAct](https://python.langchain.com/v0.1/docs/modules/agents/agent_types/react/) agent with a Cohere model, and then give the agent access to a RAG pipeline tool to improve accuracy. The general structure of the notebook is as follows:\n\n- individual components around parsing, retrieval and generation are covered for documents with mixed tabular and textual data\n- a class object is created that can be used to instantiate the pipeline with parametric input\n- the RAG pipeline is then used as a tool for a Cohere ReACT agent\n\n# Reference Documents\nWe recommend the following notebook as a guide to [semi-structured RAG](https://github.com/langchain-ai/langchain/blob/master/cookbook/Semi_Structured_RAG.ipynb).\n\nWe also recommend the following notebook to explore various parsing techniques for [PDFs](https://github.com/cohere-ai/cohere-developer-experience/blob/main/notebooks/guides/Document_Parsing_For_Enterprises.ipynb).\n\nVarious LangChain-supported parsers can be found [here](https://python.langchain.com/docs/modules/data_connection/document_loaders/pdf/).\n\n## Table of Contents\n- Section 1\n    - [Parsing](#sec_step1)\n    - [Vector Store Setup](#sec_step2)\n    - [RAG Pipeline](#sec_step3)\n- Section 2\n    - [RAG Pipeline Class](#sec_step4)\n- Section 3\n    - [ReAct Agent with RAG Tool](#sec_step5)\n\n## Install Dependencies\n\n\n```python\n# there may be other dependencies that will need installation\n# ! pip install --quiet langchain langchain_cohere langchain_experimental\n# !pip --quiet install faiss-cpu tiktoken\n# !pip install pypdf\n# !pip install pytesseract\n# !pip install opencv-python --upgrade\n# !pip install \"unstructured[all-docs]\"\n# !pip install chromadb\n```\n\n\n```python\n# LLM\nimport os\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain_community.document_loaders import WebBaseLoader\nfrom langchain_community.vectorstores import FAISS\nfrom langchain_cohere import CohereEmbeddings\nfrom pydantic import BaseModel\nfrom unstructured.partition.pdf import partition_pdf\nfrom langchain_community.document_loaders import PyPDFLoader\nimport os\nfrom typing import Any\nimport uuid\nfrom langchain.retrievers.multi_vector import MultiVectorRetriever\nfrom langchain.storage import InMemoryStore\nfrom langchain_community.vectorstores import Chroma\nfrom langchain_core.documents import Document\nimport cohere, json\nimport pandas as pd\nfrom datasets import load_dataset\nfrom joblib import Parallel, delayed\n\nos.environ['COHERE_API_KEY'] = \"\"\n```\n\n<a name=\"sec_step1\"></a>\n# Parsing\n\nTo improve RAG performance on PDFs with mixed types (text and tables), we investigated a number of parsing and chunking strategies from various libraries:\n- [PyPDFLoader (LC)](https://api.python.langchain.com/en/latest/document_loaders/langchain_community.document_loaders.pdf.PyPDFLoader.html)\n- [LlamaParse](https://docs.llamaindex.ai/en/stable/llama_cloud/llama_parse/) (Llama-Index)\n- [Unstructured](https://unstructured.io/)\n\n\nWe have found that the best option for parsing is unstructured.io since the parser can:\n- separate tables from text\n- automatically chunk the tables and text by title during the parsing step so that similar elements are grouped\n\n\n\n```python\n# UNSTRUCTURED pdf loader\n# Get elements\nraw_pdf_elements = partition_pdf(\n    filename=\"city_ny_popular_fin_report.pdf\",\n    # Unstructured first finds embedded image blocks\n    extract_images_in_pdf=False,\n    # Use layout model (YOLOX) to get bounding boxes (for tables) and find titles\n    # Titles are any sub-section of the document\n    infer_table_structure=True,\n    # Post processing to aggregate text once we have the title\n    chunking_strategy=\"by_title\",\n    # Chunking params to aggregate text blocks\n    # Attempt to create a new chunk 3800 chars\n    # Attempt to keep chunks > 2000 chars\n    max_characters=4000,\n    new_after_n_chars=3800,\n    combine_text_under_n_chars=2000,\n    image_output_dir_path='.',\n)\n\n```\n\n\n```python\n# extract table and textual objects from parser\nclass Element(BaseModel):\n    type: str\n    text: Any\n\n# Categorize by type\ncategorized_elements = []\nfor element in raw_pdf_elements:\n    if \"unstructured.documents.elements.Table\" in str(type(element)):\n        categorized_elements.append(Element(type=\"table\", text=str(element)))\n    elif \"unstructured.documents.elements.CompositeElement\" in str(type(element)):\n        categorized_elements.append(Element(type=\"text\", text=str(element)))\n\n# Tables\ntable_elements = [e for e in categorized_elements if e.type == \"table\"]\nprint(len(table_elements))\n\n# Text\ntext_elements = [e for e in categorized_elements if e.type == \"text\"]\nprint(len(text_elements))\n```\n\n    14\n    24\n\n\n<a name=\"sec_step2\"></a>\n# Vector Store Setup\n\nThere are many options for setting up a vector store. Here, we show how to do so using [Chroma](https://www.trychroma.com/) and Langchain's Multi-vector retrieval.\nAs the name implies, multi-vector retrieval allows us to store multiple vectors per document; for instance, for a single document chunk, one could keep embeddings for both the chunk itself, and a summary of that document. A summary may be able to distill more accurately what a chunk is about, leading to better retrieval.\n\nYou can read more about this here: https://python.langchain.com/docs/modules/data_connection/retrievers/multi_vector/\n\nBelow, we demonstrate the following process:\n- summaries of each chunk are embedded\n- during inference, the multi-vector retrieval returns the full context document related to the summary\n\n\n```python\nco = cohere.Client()\ndef get_chat_output(message, preamble, chat_history, model, temp, documents=None):\n    return co.chat(\n    message=message,\n    preamble=preamble,\n    chat_history=chat_history,\n    documents=documents,\n    model=model,\n    temperature=temp\n    ).text\n\ndef parallel_proc_chat(prompts,preamble,chat_history=None,model='command-r-plus',temp=0.1,n_jobs=10):\n    \"\"\"Parallel processing of chat endpoint calls.\"\"\"\n    responses = Parallel(n_jobs=n_jobs, prefer=\"threads\")(delayed(get_chat_output)(prompt,preamble,chat_history,model,temp) for prompt in prompts)\n    return responses\n\ndef rerank_cohere(query, returned_documents,model:str=\"rerank-multilingual-v3.0\",top_n:int=3):\n    response = co.rerank(\n        query=query,\n        documents=returned_documents,\n        top_n=top_n,\n        model=model,\n        return_documents=True\n    )\n    top_chunks_after_rerank = [results.document.text for results in response.results]\n    return top_chunks_after_rerank\n\n```\n\n\n```python\n# generate table and text summaries\nprompt_text = \"\"\"You are an assistant tasked with summarizing tables and text. \\ \nGive a concise summary of the table or text. Table or text chunk: {element}. Only provide the summary and no other text.\"\"\"\n\ntable_prompts = [prompt_text.format(element=i.text) for i in table_elements]\ntable_summaries = parallel_proc_chat(table_prompts,None)\ntext_prompts = [prompt_text.format(element=i.text) for i in text_elements]\ntext_summaries = parallel_proc_chat(text_prompts,None)\ntables = [i.text for i in table_elements]\ntexts = [i.text for i in text_elements]\n```\n\n\n```python\n# The vectorstore to use to index the child chunks\nvectorstore = Chroma(collection_name=\"summaries\", embedding_function=CohereEmbeddings())\n# The storage layer for the parent documents\nstore = InMemoryStore()\nid_key = \"doc_id\"\n# The retriever (empty to start)\nretriever = MultiVectorRetriever(\n    vectorstore=vectorstore,\n    docstore=store,\n    id_key=id_key,\n)\n# Add texts\ndoc_ids = [str(uuid.uuid4()) for _ in texts]\nsummary_texts = [\n    Document(page_content=s, metadata={id_key: doc_ids[i]})\n    for i, s in enumerate(text_summaries)\n]\nretriever.vectorstore.add_documents(summary_texts)\nretriever.docstore.mset(list(zip(doc_ids, texts)))\n# Add tables\ntable_ids = [str(uuid.uuid4()) for _ in tables]\nsummary_tables = [\n    Document(page_content=s, metadata={id_key: table_ids[i]})\n    for i, s in enumerate(table_summaries)\n]\nretriever.vectorstore.add_documents(summary_tables)\nretriever.docstore.mset(list(zip(table_ids, tables)))\n```\n\n<a name=\"sec_step3\"></a>\n# RAG Pipeline\n\nWith our database in place, we can run queries against it. The query process can be broken down into the following steps:\n\n- augment the query, this really helps retrieve all the relevant information\n- use each augmented query to retrieve the top k docs and then rerank them\n- concatenate all the shortlisted/reranked docs and pass them to the generation model\n\n\n```python\ndef process_query(query, retriever):\n    \"\"\"Runs query augmentation, retrieval, rerank and final generation in one call.\"\"\"\n    augmented_queries=co.chat(message=query,model='command-r-plus',temperature=0.2, search_queries_only=True)\n        #augment queries\n    if augmented_queries.search_queries:\n        reranked_docs=[]\n        for itm in augmented_queries.search_queries:\n            docs=retriever.invoke(itm.text)\n            temp_rerank = rerank_cohere(itm.text,docs)\n            reranked_docs.extend(temp_rerank)\n        documents = [{\"title\": f\"chunk {i}\", \"snippet\": reranked_docs[i]} for i in range(len(reranked_docs))]\n    else:\n        #no queries will be run through RAG\n        documents = None\n    \n    preamble = \"\"\"\n## Task & Context\nYou help people answer their questions and other requests interactively. You will be asked a very wide array of requests on all kinds of topics. You will be equipped with a wide range of search engines or similar tools to help you, which you use to research your answer. You should focus on serving the user's needs as best you can, which will be wide-ranging.\n\n## Style Guide\nUnless the user asks for a different style of answer, you should answer in full sentences, using proper grammar and spelling.\n\"\"\"\n    model = 'command-r-plus'\n    temp = 0.2\n\n    \n    \n    response = co.chat(\n      message=query,\n      documents=documents,\n      preamble=preamble,\n      model=model,\n      temperature=temp\n    )\n\n    final_answer_docs=\"\"\"The final answer is from the documents below:\n    \n    {docs}\"\"\".format(docs=str(response.documents))\n\n    final_answer = response.text\n    return final_answer, final_answer_docs\n```\n\n## Example \n\nWe can now test out a query. In this example, the final answer can be found on page 12 of the PDF, which aligns with the response provided by the model:\n\n\n```python\nquery = \"what are the charges for services in 2022\"\nfinal_answer, final_answer_docs = process_query(query, retriever)\nprint(final_answer)\nprint(final_answer_docs)\n\n\nchat_history=[{'role':\"USER\", 'message':query},{'role':\"CHATBOT\", 'message':f'The final answer is: {final_answer}.' + final_answer_docs}]\n    \n```\n\n    The charges for services in 2022 were $5,266 million.\n    The final answer is from the documents below:\n        \n        [{'id': 'doc_0', 'snippet': 'Program and General Revenues FY 2023 FY 2022 FY 2021 Category (in millions) Charges for Services (CS) $5,769 $5,266 $5,669 Operating Grants and Contributions (OGC) 27,935 31,757 28,109 Capital Grants and Contributions (CGC) 657 656 675 Real Estate Taxes (RET) 31,502 29,507 31,421 Sales and Use Taxes (SUT) 10,577 10,106 7,614 Personal Income Taxes (PIT) 15,313 15,520 15,795 Income Taxes, Other (ITO) 13,181 9,521 9,499 Other Taxes* (OT) 3,680 3,777 2,755 Investment Income* (II) 694 151 226 Unrestricted Federal and State Aid (UFSA) 234 549 108 Other* (O) Total Program and General Revenues - Primary Government 2,305 $110,250 $107,535 $104,176 708 725', 'title': 'chunk 0'}]\n\n\n### Chat History Management\n\nIn the example below, we ask a follow up question that relies on the chat history, but does not require a rerun of the RAG pipeline.\n\nWe detect questions that do not require RAG by examining the `search_queries` object returned by calling `co.chat` to generate candidate queries to answer our question. If this object is empty, then the model has determined that a document query is not needed to answer the question.\n\nIn the example below, the `else` statement is invoked based on `query2`. We still pass in the chat history, allowing the question to be answered with only the prior context.\n\n\n```python\nquery2='divide this by two'\naugmented_queries=co.chat(message=query2,model='command-r-plus',temperature=0.2, search_queries_only=True)\nif augmented_queries.search_queries:\n    print('RAG is needed')\n    final_answer, final_answer_docs = process_query(query, retriever)\n    print(final_answer)\nelse:\n    print('RAG is not needed')\n    response = co.chat(\n      message=query2,\n      model='command-r-plus',\n      chat_history=chat_history,\n      temperature=0.3\n    )\n    \n    print(\"Final answer:\")\n    print(response.text)\n```\n\n    RAG is not needed\n    Final answer:\n    The result of dividing the charges for services in 2022 by two is $2,633.\n\n\n## ------------------------------------------------------- ##\n\n<a name=\"sec_step4\"></a>\n# RAG Pipeline Class\n\nHere, we connect all of the pieces discussed above into one class object, which is then used as a tool for a Cohere ReAct agent. This class definition consolidates and clarify the key parameters used to define the RAG pipeline.\n\n\n```python\nco = cohere.Client()\n```\n\n\n```python\nclass Element(BaseModel):\n    type: str\n    text: Any\n    \nclass RAG_pipeline():\n    def __init__(self,paths):\n        self.embedding_model=\"embed-english-v3.0\"\n        self.generation_model=\"command-r-plus\"\n        self.summary_model=\"command-r-plus\"\n        self.rerank_model=\"rerank-multilingual-v3.0\"\n        self.num_docs_to_retrieve = 10\n        self.top_k_rerank=3\n        self.temperature=0.2\n        self.preamble=\"\"\"\n## Task & Context\nYou help people answer their questions and other requests interactively. You will be asked a very wide array of requests on all kinds of topics. You will be equipped with a wide range of search engines or similar tools to help you, which you use to research your answer. You should focus on serving the user's needs as best you can, which will be wide-ranging.\n\n## Style Guide\nUnless the user asks for a different style of answer, you should answer in full sentences, using proper grammar and spelling.\n\"\"\"     \n        self.n_jobs=10 #number of parallel processes to run summarization of chunks\n        self.extract_images_in_pdf=False\n        self.infer_table_structure=True\n        self.chunking_strategy=\"by_title\"\n        self.max_characters=4000\n        self.new_after_n_chars=3800\n        self.combine_text_under_n_chars=2000\n        self.image_output_dir_path='.'\n        self.paths = paths\n        self.parse_and_build_retriever()\n        \n    def parse_and_build_retriever(self,):\n        #step1, parse pdfs\n        # if condition just for debugging since perf_audit.pdf is parsed in the prev step, no need to rerun\n        parsed_pdf_list=self.parse_pdfs(self.paths)\n        #separate tables and text\n        extracted_tables, extracted_text = self.extract_text_and_tables(parsed_pdf_list)\n        #generate summaries for everything\n        tables, table_summaries, texts, text_summaries=self.generate_summaries(extracted_tables,extracted_text)\n        self.tables = tables\n        self.table_summaries = table_summaries\n        self.texts = texts\n        self.text_summaries=text_summaries\n        #setup the multivector retriever\n        self.make_retriever(tables, table_summaries, texts, text_summaries)\n        \n    def extract_text_and_tables(self,parsed_pdf_list):\n        # extract table and textual objects from parser\n        # Categorize by type\n        all_table_elements = []\n        all_text_elements = []\n        for raw_pdf_elements in parsed_pdf_list:\n            categorized_elements = []\n            for element in raw_pdf_elements:\n                if \"unstructured.documents.elements.Table\" in str(type(element)):\n                    categorized_elements.append(Element(type=\"table\", text=str(element)))\n                elif \"unstructured.documents.elements.CompositeElement\" in str(type(element)):\n                    categorized_elements.append(Element(type=\"text\", text=str(element)))\n            \n            # Tables\n            table_elements = [e for e in categorized_elements if e.type == \"table\"]\n            print(len(table_elements))\n            \n            # Text\n            text_elements = [e for e in categorized_elements if e.type == \"text\"]\n            print(len(text_elements))\n            all_table_elements.extend(table_elements)\n            all_text_elements.extend(text_elements)\n\n        return all_table_elements, all_text_elements\n            \n    def parse_pdfs(self, paths):\n\n        path_raw_elements = []\n        for path in paths:\n            raw_pdf_elements = partition_pdf(\n            filename=path,\n            # Unstructured first finds embedded image blocks\n            extract_images_in_pdf=self.extract_images_in_pdf,\n            # Use layout model (YOLOX) to get bounding boxes (for tables) and find titles\n            # Titles are any sub-section of the document\n            infer_table_structure=self.infer_table_structure,\n            # Post processing to aggregate text once we have the title\n            chunking_strategy=self.chunking_strategy,\n            # Chunking params to aggregate text blocks\n            # Attempt to create a new chunk 3800 chars\n            # Attempt to keep chunks > 2000 chars\n            max_characters=self.max_characters,\n            new_after_n_chars=self.new_after_n_chars,\n            combine_text_under_n_chars=self.combine_text_under_n_chars,\n            image_output_dir_path=self.image_output_dir_path,\n            )\n            path_raw_elements.append(raw_pdf_elements)\n        print('PDFs parsed')\n        return path_raw_elements\n        \n\n    def get_chat_output(self,message, preamble, model, temp):\n        # print(\"**message\")\n        # print(message)\n        \n        response=co.chat(\n            message=message,\n            preamble=preamble,\n            model=model,\n            temperature=temp\n            ).text\n        # print(\"**output\")\n        # print(response)\n        return response\n\n    def parallel_proc_chat(self,prompts,preamble,model,temp,n_jobs):\n        \"\"\"Parallel processing of chat endpoint calls.\"\"\"\n        responses = Parallel(n_jobs=n_jobs, prefer=\"threads\")(delayed(self.get_chat_output)(prompt,preamble,model,temp) for prompt in prompts)\n        return responses\n    \n    def rerank_cohere(self,query, returned_documents,model, top_n):\n        response = co.rerank(\n            query=query,\n            documents=returned_documents,\n            top_n=top_n,\n            model=model,\n            return_documents=True\n        )\n        top_chunks_after_rerank = [results.document.text for results in response.results]\n        return top_chunks_after_rerank\n\n    def generate_summaries(self,table_elements,text_elements):\n        # generate table and text summaries\n\n        summarize_prompt = \"\"\"You are an assistant tasked with summarizing tables and text. \\ \n        Give a concise summary of the table or text. Table or text chunk: {element}. Only provide the summary and no other text.\"\"\"\n        \n        table_prompts = [summarize_prompt.format(element=i.text) for i in table_elements]\n        table_summaries = self.parallel_proc_chat(table_prompts,self.preamble,self.summary_model,self.temperature,self.n_jobs)\n        text_prompts = [summarize_prompt.format(element=i.text) for i in text_elements]\n        text_summaries = self.parallel_proc_chat(text_prompts,self.preamble,self.summary_model,self.temperature,self.n_jobs)\n        tables = [i.text for i in table_elements]\n        texts = [i.text for i in text_elements]\n        print('summaries generated')\n        return tables, table_summaries, texts, text_summaries\n\n    def make_retriever(self,tables, table_summaries, texts, text_summaries):\n        # The vectorstore to use to index the child chunks\n        vectorstore = Chroma(collection_name=\"summaries\", embedding_function=CohereEmbeddings())\n        # The storage layer for the parent documents\n        store = InMemoryStore()\n        id_key = \"doc_id\"\n        # The retriever (empty to start)\n        retriever = MultiVectorRetriever(\n            vectorstore=vectorstore,\n            docstore=store,\n            id_key=id_key,\n            search_kwargs={\"k\": self.num_docs_to_retrieve}\n        )\n        # Add texts\n        doc_ids = [f'text_{i}' for i in range(len(texts))]#[str(uuid.uuid4()) for _ in texts]\n        summary_texts = [\n            Document(page_content=s, metadata={id_key: doc_ids[i]})\n            for i, s in enumerate(text_summaries)\n        ]\n        retriever.vectorstore.add_documents(summary_texts,ids=doc_ids)\n        retriever.docstore.mset(list(zip(doc_ids, texts)))\n        # Add tables\n        table_ids = [f'table_{i}' for i in range(len(texts))]#[str(uuid.uuid4()) for _ in tables]\n        summary_tables = [\n            Document(page_content=s, metadata={id_key: table_ids[i]})\n            for i, s in enumerate(table_summaries)\n        ]\n        retriever.vectorstore.add_documents(summary_tables,ids=table_ids)\n        retriever.docstore.mset(list(zip(table_ids, tables)))\n        self.retriever = retriever\n        print('retriever built')\n        \n    def process_query(self,query):\n        \"\"\"Runs query augmentation, retrieval, rerank and generation in one call.\"\"\"\n        augmented_queries=co.chat(message=query,model=self.generation_model,temperature=self.temperature, search_queries_only=True)\n        #augment queries\n        if augmented_queries.search_queries:\n            reranked_docs=[]\n            for itm in augmented_queries.search_queries:\n                docs=self.retriever.invoke(itm.text)\n                temp_rerank = self.rerank_cohere(itm.text,docs,model=self.rerank_model,top_n=self.top_k_rerank)\n                reranked_docs.extend(temp_rerank)\n            documents = [{\"title\": f\"chunk {i}\", \"snippet\": reranked_docs[i]} for i in range(len(reranked_docs))]\n        else:\n            documents = None\n            \n        response = co.chat(\n          message=query,\n          documents=documents,\n          preamble=self.preamble,\n          model=self.generation_model,\n          temperature=self.temperature\n        )\n    \n        final_answer_docs=\"\"\"The final answer is from the documents below:\n        \n        {docs}\"\"\".format(docs=str(response.documents))\n    \n        final_answer = response.text\n        return final_answer, final_answer_docs\n\n```\n\n\n```python\nrag_object=RAG_pipeline(paths=[\"city_ny_popular_fin_report.pdf\"])\n```\n\n    This function will be deprecated in a future release and `unstructured` will simply use the DEFAULT_MODEL from `unstructured_inference.model.base` to set default model name\n\n\n    PDFs parsed\n    14\n    24\n    summaries generated\n    retriever built\n\n\n<a name=\"sec_step5\"></a>\n# Cohere ReAct Agent with RAG Tool\n\nFinally, we build a simple agent that utilizes the RAG pipeline defined above. We do this by granting the agent access to two tools:\n\n- the end-to-end RAG pipeline \n- a Python interpreter\n\nThe intention behind coupling these tools is to enable the model to perform mathematical and other postprocessing operations on RAG outputs using Python.\n\n\n```python\nfrom langchain.agents import Tool\nfrom langchain_experimental.utilities import PythonREPL\nfrom langchain.agents import AgentExecutor\nfrom langchain_cohere.react_multi_hop.agent import create_cohere_react_agent\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_cohere.chat_models import ChatCohere\nfrom langchain.tools.retriever import create_retriever_tool\nfrom langchain_core.pydantic_v1 import BaseModel, Field\nfrom langchain_core.tools import tool\n\nclass react_agent():\n    def __init__(self,rag_retriever,model=\"command-r-plus\",temperature=0.2):\n        self.llm = ChatCohere(model=model, temperature=temperature)\n        self.preamble=\"\"\"\n## Task & Context\nYou help people answer their questions and other requests interactively. You will be asked a very wide array of requests on all kinds of topics. You will be equipped with a wide range of search engines or similar tools to help you, which you use to research your answer. You should focus on serving the user's needs as best you can, which will be wide-ranging.\n\n## Style Guide\nUnless the user asks for a different style of answer, you should answer in full sentences, using proper grammar and spelling.\n\n## Guidelines\nYou are an expert who answers the user's question. \nYou have access to a vectorsearch tool that will use your query to search through documents and find the relevant answer.\nYou also have access to a python interpreter tool which you can use to run code for mathematical operations.\n\"\"\"\n        self.get_tools(rag_retriever)\n        self.build_agent()\n        \n    def get_tools(self,rag_retriever):\n        @tool\n        def vectorsearch(query: str):\n            \"\"\"Uses the query to search through a list of documents and return the most relevant documents as well as the answer.\"\"\"\n            final_answer, final_answer_docs=rag_retriever.process_query(query)\n            return final_answer + final_answer_docs\n        vectorsearch.name = \"vectorsearch\" # use python case\n        vectorsearch.description = \"Uses the query to search through a list of documents and return the most relevant documents as well as the answer.\"\n        class vectorsearch_inputs(BaseModel):\n            query: str = Field(description=\"the users query\")\n        vectorsearch.args_schema = vectorsearch_inputs\n\n        \n        python_repl = PythonREPL()\n        python_tool = Tool(\n            name=\"python_repl\",\n            description=\"Executes python code and returns the result. The code runs in a static sandbox without interactive mode, so print output or save output to a file.\",\n            func=python_repl.run,\n        )\n        python_tool.name = \"python_interpreter\"\n        class ToolInput(BaseModel):\n            code: str = Field(description=\"Python code to execute.\")\n        python_tool.args_schema = ToolInput\n\n        self.alltools = [vectorsearch,python_tool]\n\n    def build_agent(self):\n        # Prompt template\n        prompt = ChatPromptTemplate.from_template(\"{input}\")\n        # Create the ReAct agent\n        agent = create_cohere_react_agent(\n            llm=self.llm,\n            tools=self.alltools,\n            prompt=prompt,\n        )\n        self.agent_executor = AgentExecutor(agent=agent, tools=self.alltools, verbose=True,return_intermediate_steps=True)\n\n\n    def run_agent(self,query,history=None):\n        if history:\n            response=self.agent_executor.invoke({\n            \"input\": query,\n            \"preamble\": self.preamble,\n            \"chat_history\": history\n        })\n        else:\n            response=self.agent_executor.invoke({\n            \"input\": query,\n            \"preamble\": self.preamble,\n        })\n        return response\n         \n```\n\n\n```python\nagent_object=react_agent(rag_retriever=rag_object)\n```\n\n\n```python\nstep1_response=agent_object.run_agent(\"what are the charges for services in 2022 and 2023\")\n```\n\n    \n    \n    \u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n    \u001b[32;1m\u001b[1;3m\n    I will search for the charges for services in 2022 and 2023.\n    {'tool_name': 'vectorsearch', 'parameters': {'query': 'charges for services in 2022 and 2023'}}\n    \u001b[0m\u001b[36;1m\u001b[1;3mThe charges for services in 2022 were $5,266 million and in 2023 were $5,769 million.The final answer is from the documents below:\n            \n            [{'id': 'doc_0', 'snippet': 'Program and General Revenues FY 2023 FY 2022 FY 2021 Category (in millions) Charges for Services (CS) $5,769 $5,266 $5,669 Operating Grants and Contributions (OGC) 27,935 31,757 28,109 Capital Grants and Contributions (CGC) 657 656 675 Real Estate Taxes (RET) 31,502 29,507 31,421 Sales and Use Taxes (SUT) 10,577 10,106 7,614 Personal Income Taxes (PIT) 15,313 15,520 15,795 Income Taxes, Other (ITO) 13,181 9,521 9,499 Other Taxes* (OT) 3,680 3,777 2,755 Investment Income* (II) 694 151 226 Unrestricted Federal and State Aid (UFSA) 234 549 108 Other* (O) Total Program and General Revenues - Primary Government 2,305 $110,250 $107,535 $104,176 708 725', 'title': 'chunk 0'}]\u001b[0m\u001b[32;1m\u001b[1;3mRelevant Documents: 0\n    Cited Documents: 0\n    Answer: The charges for services in 2022 were $5,266 million and in 2023 were $5,769 million.\n    Grounded answer: The charges for services in <co: 0>2022</co: 0> were <co: 0>$5,266 million</co: 0> and in <co: 0>2023</co: 0> were <co: 0>$5,769 million</co: 0>.\u001b[0m\n    \n    \u001b[1m> Finished chain.\u001b[0m\n\n\nJust like earlier, we can also pass chat history to the LangChain agent to refer to for any other queries.\n\n\n```python\nfrom langchain_core.messages import HumanMessage, AIMessage\n```\n\n\n```python\nchat_history=[\nHumanMessage(content=step1_response['input']),\nAIMessage(content=step1_response['output'])\n]\n```\n\n\n```python\nagent_object.run_agent(\"what is the mean of the two values\",history=chat_history)\n```\n\n    \n    \n    \u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n\n\n    Python REPL can execute arbitrary code. Use with caution.\n\n\n    \u001b[32;1m\u001b[1;3m\n    I will use the Python Interpreter tool to calculate the mean of the two values.\n    {'tool_name': 'python_interpreter', 'parameters': {'code': 'import numpy as np\\n\\n# Data\\nvalues = [5266, 5769]\\n\\n# Calculate the mean\\nmean_value = np.mean(values)\\n\\nprint(f\"The mean of the two values is: {mean_value:.0f} million\")'}}\n    \u001b[0m\u001b[33;1m\u001b[1;3mThe mean of the two values is: 5518 million\n    \u001b[0m\u001b[32;1m\u001b[1;3mRelevant Documents: 0\n    Cited Documents: 0\n    Answer: The mean of the two values is 5518 million.\n    Grounded answer: The mean of the two values is <co: 0>5518 million</co: 0>.\u001b[0m\n    \n    \u001b[1m> Finished chain.\u001b[0m\n\n\n\n\n\n    {'input': 'what is the mean of the two values',\n     'preamble': \"\\n## Task & Context\\nYou help people answer their questions and other requests interactively. You will be asked a very wide array of requests on all kinds of topics. You will be equipped with a wide range of search engines or similar tools to help you, which you use to research your answer. You should focus on serving the user's needs as best you can, which will be wide-ranging.\\n\\n## Style Guide\\nUnless the user asks for a different style of answer, you should answer in full sentences, using proper grammar and spelling.\\n\\n## Guidelines\\nYou are an expert who answers the user's question. \\nYou have access to a vectorsearch tool that will use your query to search through documents and find the relevant answer.\\nYou also have access to a python interpreter tool which you can use to run code for mathematical operations.\\n\",\n     'chat_history': [HumanMessage(content='what are the charges for services in 2022 and 2023'),\n      AIMessage(content='The charges for services in 2022 were $5,266 million and in 2023 were $5,769 million.')],\n     'output': 'The mean of the two values is 5518 million.',\n     'citations': [CohereCitation(start=30, end=42, text='5518 million', documents=[{'output': 'The mean of the two values is: 5518 million\\n'}])],\n     'intermediate_steps': [(AgentActionMessageLog(tool='python_interpreter', tool_input={'code': 'import numpy as np\\n\\n# Data\\nvalues = [5266, 5769]\\n\\n# Calculate the mean\\nmean_value = np.mean(values)\\n\\nprint(f\"The mean of the two values is: {mean_value:.0f} million\")'}, log='\\nI will use the Python Interpreter tool to calculate the mean of the two values.\\n{\\'tool_name\\': \\'python_interpreter\\', \\'parameters\\': {\\'code\\': \\'import numpy as np\\\\n\\\\n# Data\\\\nvalues = [5266, 5769]\\\\n\\\\n# Calculate the mean\\\\nmean_value = np.mean(values)\\\\n\\\\nprint(f\"The mean of the two values is: {mean_value:.0f} million\")\\'}}\\n', message_log=[AIMessage(content='\\nPlan: I will use the Python Interpreter tool to calculate the mean of the two values.\\nAction: ```json\\n[\\n    {\\n        \"tool_name\": \"python_interpreter\",\\n        \"parameters\": {\\n            \"code\": \"import numpy as np\\\\n\\\\n# Data\\\\nvalues = [5266, 5769]\\\\n\\\\n# Calculate the mean\\\\nmean_value = np.mean(values)\\\\n\\\\nprint(f\\\\\"The mean of the two values is: {mean_value:.0f} million\\\\\")\"\\n        }\\n    }\\n]\\n```')]),\n       'The mean of the two values is: 5518 million\\n')]}\n\n\n\n# Conclusion\nAs you can see, the RAG pipeline can be used as a tool for a Cohere ReAct agent. This allows the agent to access the RAG pipeline for document retrieval and generation, as well as a Python interpreter for postprocessing mathematical operations to improve accuracy. This setup can be used to improve the accuracy of grounded answers to questions about documents that contain both tables and text.",
    "html": "",
    "htmlmode": false,
    "fullscreen": false,
    "hidden": true,
    "revision": 8,
    "_id": "66575a45dd997e000ff4c28a",
    "__v": 0,
    "createdAt": "2024-05-29T16:39:33.333Z",
    "lastUpdatedHash": "e47128e9e9a4165e7a660ed0bd2ff5fcc1fb9e60",
    "project": "62cde2919aafea009aefb289",
    "updatedAt": "2024-07-11T01:20:29.577Z",
    "user": "5af39863989da435b05d284e"
  },
  "meta": {
    "user": {
      "allowedProjects": ["cohere-ai", "cohere-enterprise"],
      "apiKey": "",
      "email": "andrewjiang@hey.com",
      "name": "Andrew Jiang",
      "version": 1,
      "Name": "Andrew Jiang",
      "Email": "andrewjiang@hey.com",
      "APIKey": "",
      "AllowedProjects": ["cohere-ai", "cohere-enterprise"]
    },
    "baseUrl": "/",
    "hidden": true,
    "title": "Agentic RAG for PDFs with mixed data",
    "metaTitle": "Agentic RAG for PDFs with mixed data",
    "keywords": "",
    "description": "",
    "image": [],
    "slug": "agentic-rag-mixed-data",
    "type": "custompage",
    "full": false
  }
}
