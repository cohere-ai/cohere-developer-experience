{
  "custompage": {
    "metadata": {
      "image": [],
      "title": "",
      "description": "",
      "keywords": ""
    },
    "algolia": {
      "recordCount": 18,
      "publishPending": false,
      "updatedAt": "2024-07-11T01:20:20.238Z"
    },
    "title": "Chunking Strategies",
    "slug": "chunking-strategies",
    "body": "[block:html]\n{\n  \"html\": \"<div class=\\\"cookbook-nav-container\\\">\\n  <a href=\\\"/page/cookbooks\\\" class=\\\"back-button pt-10 group inline-block cursor-pointer font-medium \\\" rel=\\\"noreferrer\\\"\\n    target=\\\"_self\\\">\\n    <div class=\\\"pr-1 inline-block group-hover:no-underline\\\">\\n      <svg width=\\\"11.8\\\" height=\\\"11\\\" viewBox=\\\"0 0 14 13\\\" fill=\\\"none\\\" xmlns=\\\"http://www.w3.org/2000/svg\\\">\\n        <path\\n          d=\\\"M1.1554 7.20808C1.35066 7.40335 1.66724 7.40335 1.8625 7.20808L7.18477 1.88582C7.38003 1.69055 7.38003 1.37397 7.18477 1.17871L6.83121 0.825157C6.63595 0.629895 6.31937 0.629896 6.12411 0.825157L0.801842 6.14742C0.60658 6.34269 0.60658 6.65927 0.801842 6.85453L1.1554 7.20808Z\\\"\\n          fill=\\\"currentColor\\\" />\\n        <path\\n          d=\\\"M1.1554 5.79226C1.35066 5.597 1.66724 5.597 1.8625 5.79226L7.18477 11.1145C7.38003 11.3098 7.38003 11.6264 7.18477 11.8216L6.83121 12.1752C6.63595 12.3705 6.31937 12.3705 6.12411 12.1752L0.801842 6.85292C0.60658 6.65766 0.60658 6.34108 0.801842 6.14582L1.1554 5.79226Z\\\"\\n          fill=\\\"currentColor\\\" />\\n        <path\\n          d=\\\"M2.52491 6.23674C2.52492 5.9606 2.74878 5.73675 3.02491 5.73675H6.28412C6.4513 5.73675 6.60742 5.8203 6.70015 5.95941L7.03347 6.45941C7.25499 6.79169 7.01679 7.23675 6.61745 7.23675H3.0249C2.74876 7.23675 2.5249 7.01289 2.5249 6.73674L2.52491 6.23674Z\\\"\\n          fill=\\\"currentColor\\\" />\\n        <path\\n          d=\\\"M13.5517 6.73676C13.5517 7.0129 13.3278 7.23675 13.0517 7.23675H8.79246C8.62528 7.23675 8.46916 7.1532 8.37643 7.0141L8.04311 6.5141C7.8216 6.18182 8.05979 5.73675 8.45914 5.73675H13.0517C13.3278 5.73675 13.5517 5.96062 13.5517 6.23676L13.5517 6.73676Z\\\"\\n          fill=\\\"currentColor\\\" />\\n      </svg>\\n    </div>\\n    Back to Cookbooks\\n  </a>\\n\\n  <a href=https://github.com/cohere-ai/cohere-developer-experience/blob/main/notebooks/guides/Chunking_strategies.ipynb class=\\\"github-button pt-10 group inline-block cursor-pointer font-medium \\\" rel=\\\"noreferrer\\\"\\n    target=\\\"_blank\\\">\\n    Open in GitHub\\n    <div class=\\\"pl-1 inline-block group-hover:no-underline\\\">\\n      <svg width=\\\"14\\\" height=\\\"10\\\" viewBox=\\\"0 0 14 10\\\" fill=\\\"none\\\" xmlns=\\\"http://www.w3.org/2000/svg\\\">\\n        <path\\n          d=\\\"M8.63218 0.366821C8.35604 0.366821 8.13218 0.590679 8.13218 0.866821V8.39364C8.13218 8.66978 8.35604 8.89364 8.63218 8.89364H9.13218C9.40832 8.89364 9.63218 8.66978 9.63218 8.39364V0.866821C9.63218 0.590678 9.40832 0.366821 9.13218 0.366821H8.63218Z\\\"\\n          fill=\\\"currentColor\\\" />\\n        <path\\n          d=\\\"M9.63332 1.36796C9.63332 1.6441 9.40946 1.86796 9.13332 1.86796H1.6065C1.33035 1.86796 1.1065 1.6441 1.1065 1.36796V0.867956C1.1065 0.591813 1.33035 0.367956 1.6065 0.367956H9.13332C9.40946 0.367956 9.63332 0.591813 9.63332 0.867956V1.36796Z\\\"\\n          fill=\\\"currentColor\\\" />\\n        <path\\n          d=\\\"M8.35063 2.02206C8.54588 2.21732 8.54588 2.5339 8.35062 2.72916L6.04601 5.03377C5.9278 5.15198 5.75833 5.20329 5.59439 5.1705L5.00515 5.05264C4.61356 4.97432 4.46728 4.49118 4.74966 4.2088L7.28997 1.66849C7.48523 1.47323 7.80182 1.47323 7.99708 1.6685L8.35063 2.02206Z\\\"\\n          fill=\\\"currentColor\\\" />\\n        <path\\n          d=\\\"M0.199967 9.46558C0.0047119 9.27032 0.0047151 8.95374 0.199974 8.75848L3.21169 5.74677C3.3299 5.62855 3.49938 5.57724 3.66331 5.61003L4.25256 5.72789C4.64414 5.80621 4.79042 6.28935 4.50804 6.57173L1.26063 9.81915C1.06536 10.0144 0.748774 10.0144 0.553513 9.81914L0.199967 9.46558Z\\\"\\n          fill=\\\"currentColor\\\" />\\n      </svg>\\n    </div>\\n  </a>\\n</div>\\n\\n<div>\\n  <h1>Chunking Strategies</h1>\\n</div>\\n\\n<!-- Authors -->\\n<div class=\\\"authors-container\\\">\\n  \\n  <div class=\\\"author-container\\\">\\n    <img class=\\\"author-image\\\" src=\\\"https://files.readme.io/c5dc5a3-Ania.jpg\\\" alt=\\\"Ania Bialas\\\" />\\n    <p class=\\\"author-name\\\">Ania Bialas</p>\\n  </div>\\n  \\n</div>\\n\\n<style>\\n  .header {\\n    padding: 9px 0 17px 0;\\n    display: flex;\\n    flex-direction: column;\\n  }\\n\\n  a[href],\\n  .field-description a:not([href=\\\"\\\"]),\\n  .markdown-body a[href],\\n  .markdown-body a:not([href=\\\"\\\"]) {\\n    text-decoration: none;\\n  }\\n\\n  #content {\\n    padding: 0 32px;\\n  }\\n\\n  #content-head {\\n    display: none;\\n  }\\n\\n  .guide-page-title {\\n    font-size: 29px !important;\\n  }\\n\\n  .back-button .github-button {\\n    border-radius: 0 !important;\\n    border-width: 0 !important;\\n    background-color: inherit !important;\\n  }\\n\\n  .cookbook-nav-container {\\n    display: flex;\\n    flex-direction: row;\\n    justify-content: space-between;\\n    align-items: center;\\n  }\\n\\n  .authors-container {\\n    display: flex;\\n    flex-direction: row;\\n    flex-flow: row wrap;\\n    gap: 2px 14px;\\n    margin-top: 8px;\\n  }\\n\\n  .author-container {\\n    line-height: 1.5em;\\n    display: flex;\\n    flex-direction: row;\\n    vertical-align: middle;\\n    flex-shrink: 0;\\n  }\\n\\n  .author-image {\\n    height: 1.5em;\\n    margin: 0px 6px 6px 0px !important;\\n    border-radius: 50%;\\n  }\\n\\n  .author-name {\\n    white-space: nowrap;\\n    vertical-align: middle;\\n  }\\n\\n  @media only screen and (min-width: 620px) {\\n    .guide-page-title {\\n      width: 70%;\\n    }\\n\\n    .header {\\n      display: flex;\\n      flex-direction: row;\\n      align-items: center;\\n      justify-content: space-between;\\n      padding: 9px 0 17px 0;\\n    }\\n\\n    .git--button {\\n      width: 145px !important;\\n      display: flex;\\n      flex-direction: row;\\n      justify-content: center;\\n      align-items: center;\\n      padding: 8px 16px;\\n\\n      width: 154px;\\n      height: 35px;\\n\\n      background: #D4D9D4;\\n      border: 1px solid #9DAAA4;\\n      border-radius: 6px;\\n    }\\n  }\\n\\n\\n  @media only screen and (min-width: 1024px) {\\n    .guide-page-title {\\n      font-size: 46px !important;\\n    }\\n\\n    .header {\\n      padding: 9px 0 32px 0;\\n    }\\n  }\\n</style>\"\n}\n[/block]\n\n```python\n%%capture\n!pip install cohere\n!pip install -qU langchain-text-splitters\n!pip install llama-index-embeddings-cohere\n!pip install llama-index-postprocessor-cohere-rerank\n```\n\n\n```python\nimport requests\nfrom typing import List\n\nfrom bs4 import BeautifulSoup\n\nimport cohere\nfrom getpass import getpass\nfrom IPython.display import HTML, display\n\nfrom langchain_text_splitters import CharacterTextSplitter\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\n\nfrom llama_index.core import Document\nfrom llama_index.embeddings.cohere import CohereEmbedding\nfrom llama_index.postprocessor.cohere_rerank import CohereRerank\nfrom llama_index.core import VectorStoreIndex, ServiceContext\n```\n\n\n```python\nco_model = 'command-r'\nco_api_key = getpass(\"Enter Cohere API key: \")\nco = cohere.Client(api_key=co_api_key)\n```\n\n    Enter Cohere API key: ··········\n\n\n## Introduction\n\nChunking is an essential component of any RAG-based system. This cookbook aims to demonstrate how different chunking strategies affect the results of LLM-generated output. There are multiple considerations that need to be taken into account when designing chunking strategy. Therefore, we begin by providing a framework for these strategies and then jump into a practical example. We will focus our example on transcript calls, which create a unique challenge because of their rich content and the change of people speaking throughout the text.\n\n## Table of contents\n\n1. [Chunking strategies framework](#chunking-strategies-framework)\n2. [Getting started](#getting-started)\n3. [Example 1: Chunking using content-independent strategies](#example-1)\n4. [Example 2: Chunking using content-dependent strategies](#example-2)\n5. [Discussion](#discussion)\n\n<a name=\"chunking-strategies-framework\"></a>\n## Chunking strategies framework\n\n### Document splitting\n\nBy document splitting, we mean deciding on the conditions under which we will break the text. At this stage, we should ask, *\"Are there any parts of consecutive text we want to ensure we do not break?\"*. If the answer is \"no\", then, the content-independent splitting strategies are helpful. On the other hand, in scenarios like transcripts or meeting notes, we probably would like to keep the content of one speaker together, which might require us to deploy content-dependent strategies.\n\n#### Content-independent splitting strategies\n\nWe split the document based on some content-independent conditions, among the most popular ones are:\n- splitting by the number of characters,\n- splitting by sentence,\n- splitting by a given character, for example, `\\n` for paragraphs.\n\nThe advantage of this scenario is that we do not need to make any assumptions about the text. However, some considerations remain, like whether we want to preserve some semantic structure, for example, sentences or paragraphs. Sentence splitting is better suited if we are looking for small chunks to ensure accuracy. Conversely, paragraphs preserve more context and might be more useful in open-ended questions.\n\n#### Content-dependent splitting strategies\n\nOn the other hand, there are scenarios in which we care about preserving some text structure. Then, we develop custom splitting strategies based on the document's content. A prime example is call transcripts. In such scenarios, we aim to ensure that one person's speech is fully contained within a chunk.\n\n### Creating chunks from the document splits\n\nAfter the document is split, we need to decide on the desired **size** of our chunks (the split only defines how we break the document, but we can create bigger chunks from multiple splits).\n\nSmaller chunks support more accurate retrieval. However, they might lack context. On the other hand, larger chunks offer more context, but they reduce the effectiveness of the retrieval. It is important to experiment with different settings to find the optimal balance.\n\n### Overlapping chunks\n\nOverlapping chunks is a useful technique to have in the toolbox. Especially when we employ content-independent splitting strategies, it helps us mitigate some of the pitfalls of breaking the document without fully understanding the text. Overlapping guarantees that there is always some buffer between the chunks, and even if an important piece of information might be split in the original splitting strategy, it is more probable that the full information will be captured in the next chunk. The disadvantage of this method is that it creates redundancy.\n\n<a name=\"getting-started\"></a>\n## Getting started\n\nDesigning a robust chunking strategy is as much a science as an art. There are no straightforward answers; the most effective strategies often emerge through experimentation. Therefore, let's dive straight into an example to illustrate this concept.\n\n## Utils\n\n\n```python\ndef set_css():\n  display(HTML('''\n  <style>/\n    pre {\n        white-space: pre-wrap;\n    }\n  </style>\n  '''))\nget_ipython().events.register('pre_run_cell', set_css)\n\nset_css()\n```\n\n\n\n<style>/\n  pre {\n      white-space: pre-wrap;\n  }\n</style>\n\n\n\n\n```python\ndef insert_citations(text: str, citations: List[dict]):\n    \"\"\"\n    A helper function to pretty print citations.\n    \"\"\"\n    offset = 0\n    # Process citations in the order they were provided\n    for citation in citations:\n        # Adjust start/end with offset\n        start, end = citation['start'] + offset, citation['end'] + offset\n        placeholder = \"[\" + \", \".join(doc[4:] for doc in citation[\"document_ids\"]) + \"]\"\n        # ^ doc[4:] removes the 'doc_' prefix, and leaves the quoted document\n        modification = f'{text[start:end]} {placeholder}'\n        # Replace the cited text with its bolded version + placeholder\n        text = text[:start] + modification + text[end:]\n        # Update the offset for subsequent replacements\n        offset += len(modification) - (end - start)\n\n    return text\n\ndef build_retriever(documents, top_n=5):\n  # Create the embedding model\n  embed_model = CohereEmbedding(\n      cohere_api_key=co_api_key,\n      model_name=\"embed-english-v3.0\",\n      input_type=\"search_query\",\n  )\n\n  # Load the data, for this example data needs to be in a test file\n  index = VectorStoreIndex.from_documents(\n      documents,\n      embed_model=embed_model\n  )\n\n  # Create a cohere reranker\n  cohere_rerank = CohereRerank(api_key=co_api_key)\n\n  # Create the retriever\n  retriever = index.as_retriever(node_postprocessors=[cohere_rerank], similarity_top_k=top_n)\n  return retriever\n```\n\n\n\n<style>/\n  pre {\n      white-space: pre-wrap;\n  }\n</style>\n\n\n\n## Load the data\n\nIn this example we will work with an 2023 Tesla earning call transcript.\n\n\n```python\n# Get all investement memos (19) in bvp repository\nurl_path = 'https://www.fool.com/earnings/call-transcripts/2024/01/24/tesla-tsla-q4-2023-earnings-call-transcript/'\nresponse = requests.get(url_path)\nsoup = BeautifulSoup(response.content, 'html.parser')\n\ntarget_divs = soup.find(\"div\", {\"class\": \"article-body\"}).find_all(\"p\")[2:]\nprint('Length of the script: ', len(target_divs))\n\nprint()\nprint('Example of processed text:')\ntext = '\\n\\n'.join([div.get_text() for div in target_divs])\nprint(text[:500])\n```\n\n\n\n<style>/\n  pre {\n      white-space: pre-wrap;\n  }\n</style>\n\n\n\n    Length of the script:  385\n    \n    Example of processed text:\n    Martin Viecha\n    \n    Good afternoon, everyone, and welcome to Tesla's fourth-quarter 2023 Q&A webcast. My name is Martin Viecha, VP of investor relations, and I'm joined today by Elon Musk, Vaibhav Taneja, and a number of other executives. Our Q4 results were announced at about 3 p.m. Central Time in the update that we published at the same link as this webcast.\n    \n    During this call, we will discuss our business outlook and make forward-looking statements. These comments are based on our predictions and \n\n\n<a id=\"example-1\"></a>\n## Example 1: Chunking using content-independent strategies\n\nLet's begin with a simple content-independent strategy. We aim to answer the question, `Who mentions Jonathan Nolan?`. We chose this question as it is easily verifiable and it requires to identify the speaker. The answer to this question can be found in the downloaded transcript, here is the relevant passage:\n\n\n```\nElon Musk -- Chief Executive Officer and Product Architect\n\nYeah. The creators of Westworld, Jonathan Nolan, Lisa Joy Nolan, are friends -- are all friends of mine, actually. And I invited them to come see the lab and, like, well, come see it, hopefully soon. It's pretty well -- especially the sort of subsystem test stands where you've just got like one leg on a test stand just doing repetitive exercises and one arm on a test stand pretty well.\n```\n\n\n```python\n# Define the question\nquestion = \"Who mentions Jonathan Nolan?\"\n```\n\n\n\n<style>/\n  pre {\n      white-space: pre-wrap;\n  }\n</style>\n\n\n\nIn this case, we are more concerned about accuracy than a verbose answer, so we **focus on keeping the chunks small**. To ensure that the desired size is not exceeded, we will randomly split the list of characters, in our case `[\"\\n\\n\", \"\\n\", \" \", \"\"]`.\n\nWe employ the `RecursiveCharacterTextSplitter` from [LangChain](https://python.langchain.com/docs/get_started/introduction) for this task.\n\n\n```python\n# Define the chunking function\ndef get_chunks(text, chunk_size, chunk_overlap):\n  text_splitter = RecursiveCharacterTextSplitter(\n    chunk_size=chunk_size,\n    chunk_overlap=chunk_overlap,\n    length_function=len,\n    is_separator_regex=False,\n  )\n\n  documents = text_splitter.create_documents([text])\n  documents = [Document(text=doc.page_content) for doc in documents]\n\n  return documents\n```\n\n\n\n<style>/\n  pre {\n      white-space: pre-wrap;\n  }\n</style>\n\n\n\n### Experiment 1 - no overlap\nIn our first experiment we define the chunk size as 500 and allow **no overlap between consecutive chunks**.\n\nSubsequently, we implement the standard RAG pipeline. We feed the chunks into a retriever, selecting the `top_n` most pertinent to the query chunks, and supply them as context to the generation model. Throughout this pipeline, we leverage [Cohere's endpoints](https://docs.cohere.com/reference/about), specifically, `co.embed`, `co.re.rank`, and finally, `co.chat`.\n\n\n```python\nchunk_size = 500\nchunk_overlap = 0\ndocuments = get_chunks(text, chunk_size, chunk_overlap)\nretriever = build_retriever(documents)\n\nsource_nodes = retriever.retrieve(question)\nprint('Number of docuemnts: ',len(source_nodes))\nsource_nodes= [{\"text\": ni.get_content()}for ni in source_nodes]\n\n\nresponse = co.chat(\n  message=question,\n  documents=source_nodes,\n  model=co_model\n)\nresponse = response\nprint(response.text)\n```\n\n\n\n<style>/\n  pre {\n      white-space: pre-wrap;\n  }\n</style>\n\n\n\n    Number of docuemnts:  5\n    An unknown speaker mentions Jonathan Nolan in a conversation about the creators of Westworld. They mention that Jonathan Nolan and Lisa Joy Nolan are friends of theirs, and that they have invited them to visit the lab.\n\n\nA notable feature of [`co.chat`](https://docs.cohere.com/reference/chat) is its ability to ground the model's answer within the context. This means we can identify which chunks were used to generate the answer. Below, we show the previous output of the model together with the citation reference, where `[num]` represents the index of the chunk.\n\n\n```python\nprint(insert_citations(response.text, response.citations))\n```\n\n\n\n<style>/\n  pre {\n      white-space: pre-wrap;\n  }\n</style>\n\n\n\n    An unknown speaker [0] mentions Jonathan Nolan in a conversation about the creators of Westworld. [0] They mention that Jonathan Nolan and Lisa Joy Nolan [0] are friends [0] of theirs, and that they have invited them to visit the lab. [0]\n\n\nIndeed, by printing the cited chunk, we can validate that the text was divided so that the generation model could not provide the correct response. Notably, the speaker's name is not included in the context, which is why the model refes to an `unknown speaker`.\n\n\n```python\nprint(source_nodes[0])\n```\n\n\n\n<style>/\n  pre {\n      white-space: pre-wrap;\n  }\n</style>\n\n\n\n    {'text': \"Yeah. The creators of Westworld, Jonathan Nolan, Lisa Joy Nolan, are friends -- are all friends of mine, actually. And I invited them to come see the lab and, like, well, come see it, hopefully soon. It's pretty well -- especially the sort of subsystem test stands where you've just got like one leg on a test stand just doing repetitive exercises and one arm on a test stand pretty well.\\n\\nYeah.\\n\\nUnknown speaker\\n\\nWe're not entering Westworld anytime soon.\"}\n\n\n### Experiment 2 - allow overlap\nIn the previous experiment, we discovered that the chunks were generated in a way that made it impossible to generate the correct answer. The name of the speaker was not included in the relevant chunk.\n\nTherefore, this time to mitigate this issue, we **allow for overlap between consecutive chunks**.\n\n\n```python\nchunk_size = 500\nchunk_overlap = 100\ndocuments = get_chunks(text,chunk_size, chunk_overlap)\nretriever = build_retriever(documents)\n\nsource_nodes = retriever.retrieve(question)\nprint('Number of docuemnts: ',len(source_nodes))\nsource_nodes= [{\"text\": ni.get_content()}for ni in source_nodes]\n\n\nresponse = co.chat(\n  message=question,\n  documents=source_nodes,\n  model=co_model\n)\nresponse = response\nprint(response.text)\n```\n\n\n\n<style>/\n  pre {\n      white-space: pre-wrap;\n  }\n</style>\n\n\n\n    Number of docuemnts:  5\n    Elon Musk mentions Jonathan Nolan. Musk is the CEO and Product Architect of the lab that resembles the set of Westworld, a show created by Jonathan Nolan and Lisa Joy Nolan.\n\n\nAgain, we can print the text along with the citations.\n\n\n```python\nprint(insert_citations(response.text, response.citations))\n```\n\n\n\n<style>/\n  pre {\n      white-space: pre-wrap;\n  }\n</style>\n\n\n\n    Elon Musk [0] mentions Jonathan Nolan. Musk is the CEO and Product Architect [0] of the lab [0] that resembles the set of Westworld [0], a show created by Jonathan Nolan [0] and Lisa Joy Nolan. [0]\n\n\nAnd investigate the chunks which were used as context to answer the query.\n\n\n\n\n```python\nsource_nodes[0]\n```\n\n\n\n<style>/\n  pre {\n      white-space: pre-wrap;\n  }\n</style>\n\n\n\n\n\n\n    {'text': \"Yeah, not the best reference.\\n\\nElon Musk -- Chief Executive Officer and Product Architect\\n\\nYeah. The creators of Westworld, Jonathan Nolan, Lisa Joy Nolan, are friends -- are all friends of mine, actually. And I invited them to come see the lab and, like, well, come see it, hopefully soon. It's pretty well -- especially the sort of subsystem test stands where you've just got like one leg on a test stand just doing repetitive exercises and one arm on a test stand pretty well.\\n\\nYeah.\"}\n\n\n\nAs we can see, by allowing overlap we managed to get the correct answer to our question.\n\n<a id=\"example-2\"></a>\n## Example 2: Chunking using content-dependent strategies\n\nIn the previous experiment, we provided an example of how using or not using overlapping can affect a model's performance,  particularly in documents such as call transcripts where subjects change frequently. Ensuring that each chunk contains all relevant information is crucial. While we managed to retrieve the correct information by introducing overlapping into the chunking strategy, this might still not be the optimal approach for transcripts with longer speaker speeches.\n\nTherefore, in this experiment, we will adopt a content-dependent strategy.\n\nOur proposed approach entails segmenting the text whenever a new speaker begins speaking, which requires preprocessing the text accordingly.\n\n### Preprocess the text\n\nFirstly, let's observe that in the HTML text, each time the speaker changes, their name is enclosed within `<p><strong>Name</p></strong>` tags, denoting the speaker's name in bold letters.\n\nTo facilitate our text chunking process, we'll use the above observation and introduce a unique character sequence `###`, which we'll utilize as a marker for splitting the text.\n\n\n```python\nprint('HTML text')\nprint(target_divs[:3])\nprint('-------------------\\n')\n\ntext_custom = []\nfor div in target_divs:\n  if div.get_text() is None:\n    continue\n  if str(div).startswith('<p><strong>'):\n    text_custom.append(f'### {div.get_text()}')\n  else:\n    text_custom.append(div.get_text())\n\ntext_custom = '\\n'.join(text_custom)\nprint(text_custom[:500])\n```\n\n\n\n<style>/\n  pre {\n      white-space: pre-wrap;\n  }\n</style>\n\n\n\n    HTML text\n    [<p><strong>Martin Viecha</strong></p>, <p>Good afternoon, everyone, and welcome to Tesla's fourth-quarter 2023 Q&amp;A webcast. My name is Martin Viecha, VP of investor relations, and I'm joined today by Elon Musk, Vaibhav Taneja, and a number of other executives. Our Q4 results were announced at about 3 p.m. Central Time in the update that we published at the same link as this webcast.</p>, <p>During this call, we will discuss our business outlook and make forward-looking statements. These comments are based on our predictions and expectations as of today. Actual events or results could differ materially due to a number of risks and uncertainties, including those mentioned in our most recent filings with the SEC. [Operator instructions] But before we jump into Q&amp;A, Elon has some opening remarks.</p>]\n    -------------------\n    \n    ### Martin Viecha\n    Good afternoon, everyone, and welcome to Tesla's fourth-quarter 2023 Q&A webcast. My name is Martin Viecha, VP of investor relations, and I'm joined today by Elon Musk, Vaibhav Taneja, and a number of other executives. Our Q4 results were announced at about 3 p.m. Central Time in the update that we published at the same link as this webcast.\n    During this call, we will discuss our business outlook and make forward-looking statements. These comments are based on our predictions an\n\n\nIn this approach, we prioritize splitting the text at the appropriate separator, `###.` To ensure this behavior, we'll use `CharacterTextSplitter` from [LangChain](https://python.langchain.com/docs/get_started/introduction), guaranteeing such behavior. From our analysis of the text and the fact that we aim to preserve entire speaker speeches intact, we anticipate that most of them will exceed a length of 500. Hence, we'll increase the chunk size to 1000.\n\n\n```python\nseparator = \"###\"\nchunk_size = 1000\nchunk_overlap = 0\n\ntext_splitter = CharacterTextSplitter(\n    separator = separator,\n    chunk_size=chunk_size,\n    chunk_overlap=chunk_overlap,\n    length_function=len,\n    is_separator_regex=False,\n)\n\ndocuments = text_splitter.create_documents([text_custom])\ndocuments = [Document(text=doc.page_content) for doc in documents]\n\nretriever = build_retriever(documents)\n\nsource_nodes = retriever.retrieve(question)\nprint('Number of docuemnts: ',len(source_nodes))\nsource_nodes= [{\"text\": ni.get_content()}for ni in source_nodes]\n\nresponse = co.chat(\n  message=question,\n  documents=source_nodes,\n  model=co_model\n)\nresponse = response\nprint(response.text)\n```\n\n\n\n<style>/\n  pre {\n      white-space: pre-wrap;\n  }\n</style>\n\n\n\n    WARNING:langchain_text_splitters.base:Created a chunk of size 5946, which is longer than the specified 1000\n    WARNING:langchain_text_splitters.base:Created a chunk of size 4092, which is longer than the specified 1000\n    WARNING:langchain_text_splitters.base:Created a chunk of size 1782, which is longer than the specified 1000\n    WARNING:langchain_text_splitters.base:Created a chunk of size 1392, which is longer than the specified 1000\n    WARNING:langchain_text_splitters.base:Created a chunk of size 2046, which is longer than the specified 1000\n    WARNING:langchain_text_splitters.base:Created a chunk of size 1152, which is longer than the specified 1000\n    WARNING:langchain_text_splitters.base:Created a chunk of size 1304, which is longer than the specified 1000\n    WARNING:langchain_text_splitters.base:Created a chunk of size 1295, which is longer than the specified 1000\n    WARNING:langchain_text_splitters.base:Created a chunk of size 2090, which is longer than the specified 1000\n    WARNING:langchain_text_splitters.base:Created a chunk of size 1251, which is longer than the specified 1000\n\n\n    Number of docuemnts:  5\n    Elon Musk mentions Jonathan Nolan. Musk is friends with the creators of Westworld, Jonathan Nolan and Lisa Joy Nolan.\n\n\nBelow we validate the answer using citations.\n\n\n```python\nprint(insert_citations(response.text, response.citations))\n```\n\n\n\n<style>/\n  pre {\n      white-space: pre-wrap;\n  }\n</style>\n\n\n\n    Elon Musk [0] mentions Jonathan Nolan. [0] Musk is friends [0] with the creators of Westworld [0], Jonathan Nolan [0] and Lisa Joy Nolan. [0]\n\n\n\n```python\nsource_nodes[0]\n```\n\n\n\n<style>/\n  pre {\n      white-space: pre-wrap;\n  }\n</style>\n\n\n\n\n\n\n    {'text': \"Elon Musk -- Chief Executive Officer and Product Architect\\nYeah. The creators of Westworld, Jonathan Nolan, Lisa Joy Nolan, are friends -- are all friends of mine, actually. And I invited them to come see the lab and, like, well, come see it, hopefully soon. It's pretty well -- especially the sort of subsystem test stands where you've just got like one leg on a test stand just doing repetitive exercises and one arm on a test stand pretty well.\\nYeah.\\n### Unknown speaker\\nWe're not entering Westworld anytime soon.\\n### Elon Musk -- Chief Executive Officer and Product Architect\\nRight, right. Yeah. I take -- take safety very very seriously.\\n### Martin Viecha\\nThank you. The next question from Norman is: How many Cybertruck orders are in the queue? And when do you anticipate to be able to fulfill existing orders?\"}\n\n\n\n<a id=\"discussion\"></a>\n## Discussion\n\nThis example highlights some of the concerns that arise when implementing chunking strategies. This is a field of ongoing research, and many exciting surveys have been published in domain-specific applications. For example, this [paper](https://arxiv.org/pdf/2402.05131.pdf) examines different chunking strategies in finance.",
    "html": "",
    "htmlmode": false,
    "fullscreen": false,
    "hidden": true,
    "revision": 28,
    "_id": "664c7831b2c1da00679c4bdf",
    "__v": 0,
    "createdAt": "2024-05-21T10:32:17.198Z",
    "lastUpdatedHash": "2120b2ed8e6c192237eec397a2da8e74b1d52e09",
    "project": "62cde2919aafea009aefb289",
    "updatedAt": "2024-07-11T01:20:20.239Z",
    "user": "5af39863989da435b05d284e"
  },
  "meta": {
    "user": {
      "allowedProjects": ["cohere-ai", "cohere-enterprise"],
      "apiKey": "",
      "email": "andrewjiang@hey.com",
      "name": "Andrew Jiang",
      "version": 1,
      "Name": "Andrew Jiang",
      "Email": "andrewjiang@hey.com",
      "APIKey": "",
      "AllowedProjects": ["cohere-ai", "cohere-enterprise"]
    },
    "baseUrl": "/",
    "hidden": true,
    "title": "Chunking Strategies",
    "metaTitle": "Chunking Strategies",
    "keywords": "",
    "description": "",
    "image": [],
    "slug": "chunking-strategies",
    "type": "custompage",
    "full": false
  }
}
