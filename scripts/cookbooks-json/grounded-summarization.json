{
  "custompage": {
    "metadata": {
      "image": [],
      "title": "",
      "description": "",
      "keywords": ""
    },
    "algolia": {
      "recordCount": 7,
      "publishPending": false,
      "updatedAt": "2024-07-11T01:20:23.410Z"
    },
    "title": "Grounded Summarization Using Command R",
    "slug": "grounded-summarization",
    "body": "[block:html]\n{\n  \"html\": \"<div class=\\\"cookbook-nav-container\\\">\\n  <a href=\\\"/page/cookbooks\\\" class=\\\"back-button pt-10 group inline-block cursor-pointer font-medium \\\" rel=\\\"noreferrer\\\"\\n    target=\\\"_self\\\">\\n    <div class=\\\"pr-1 inline-block group-hover:no-underline\\\">\\n      <svg width=\\\"11.8\\\" height=\\\"11\\\" viewBox=\\\"0 0 14 13\\\" fill=\\\"none\\\" xmlns=\\\"http://www.w3.org/2000/svg\\\">\\n        <path\\n          d=\\\"M1.1554 7.20808C1.35066 7.40335 1.66724 7.40335 1.8625 7.20808L7.18477 1.88582C7.38003 1.69055 7.38003 1.37397 7.18477 1.17871L6.83121 0.825157C6.63595 0.629895 6.31937 0.629896 6.12411 0.825157L0.801842 6.14742C0.60658 6.34269 0.60658 6.65927 0.801842 6.85453L1.1554 7.20808Z\\\"\\n          fill=\\\"currentColor\\\" />\\n        <path\\n          d=\\\"M1.1554 5.79226C1.35066 5.597 1.66724 5.597 1.8625 5.79226L7.18477 11.1145C7.38003 11.3098 7.38003 11.6264 7.18477 11.8216L6.83121 12.1752C6.63595 12.3705 6.31937 12.3705 6.12411 12.1752L0.801842 6.85292C0.60658 6.65766 0.60658 6.34108 0.801842 6.14582L1.1554 5.79226Z\\\"\\n          fill=\\\"currentColor\\\" />\\n        <path\\n          d=\\\"M2.52491 6.23674C2.52492 5.9606 2.74878 5.73675 3.02491 5.73675H6.28412C6.4513 5.73675 6.60742 5.8203 6.70015 5.95941L7.03347 6.45941C7.25499 6.79169 7.01679 7.23675 6.61745 7.23675H3.0249C2.74876 7.23675 2.5249 7.01289 2.5249 6.73674L2.52491 6.23674Z\\\"\\n          fill=\\\"currentColor\\\" />\\n        <path\\n          d=\\\"M13.5517 6.73676C13.5517 7.0129 13.3278 7.23675 13.0517 7.23675H8.79246C8.62528 7.23675 8.46916 7.1532 8.37643 7.0141L8.04311 6.5141C7.8216 6.18182 8.05979 5.73675 8.45914 5.73675H13.0517C13.3278 5.73675 13.5517 5.96062 13.5517 6.23676L13.5517 6.73676Z\\\"\\n          fill=\\\"currentColor\\\" />\\n      </svg>\\n    </div>\\n    Back to Cookbooks\\n  </a>\\n\\n  <a href=https://github.com/cohere-ai/cohere-developer-experience/blob/main/notebooks/guides/Grounded_summarisation_using_Command_R.ipynb class=\\\"github-button pt-10 group inline-block cursor-pointer font-medium \\\" rel=\\\"noreferrer\\\"\\n    target=\\\"_blank\\\">\\n    Open in GitHub\\n    <div class=\\\"pl-1 inline-block group-hover:no-underline\\\">\\n      <svg width=\\\"14\\\" height=\\\"10\\\" viewBox=\\\"0 0 14 10\\\" fill=\\\"none\\\" xmlns=\\\"http://www.w3.org/2000/svg\\\">\\n        <path\\n          d=\\\"M8.63218 0.366821C8.35604 0.366821 8.13218 0.590679 8.13218 0.866821V8.39364C8.13218 8.66978 8.35604 8.89364 8.63218 8.89364H9.13218C9.40832 8.89364 9.63218 8.66978 9.63218 8.39364V0.866821C9.63218 0.590678 9.40832 0.366821 9.13218 0.366821H8.63218Z\\\"\\n          fill=\\\"currentColor\\\" />\\n        <path\\n          d=\\\"M9.63332 1.36796C9.63332 1.6441 9.40946 1.86796 9.13332 1.86796H1.6065C1.33035 1.86796 1.1065 1.6441 1.1065 1.36796V0.867956C1.1065 0.591813 1.33035 0.367956 1.6065 0.367956H9.13332C9.40946 0.367956 9.63332 0.591813 9.63332 0.867956V1.36796Z\\\"\\n          fill=\\\"currentColor\\\" />\\n        <path\\n          d=\\\"M8.35063 2.02206C8.54588 2.21732 8.54588 2.5339 8.35062 2.72916L6.04601 5.03377C5.9278 5.15198 5.75833 5.20329 5.59439 5.1705L5.00515 5.05264C4.61356 4.97432 4.46728 4.49118 4.74966 4.2088L7.28997 1.66849C7.48523 1.47323 7.80182 1.47323 7.99708 1.6685L8.35063 2.02206Z\\\"\\n          fill=\\\"currentColor\\\" />\\n        <path\\n          d=\\\"M0.199967 9.46558C0.0047119 9.27032 0.0047151 8.95374 0.199974 8.75848L3.21169 5.74677C3.3299 5.62855 3.49938 5.57724 3.66331 5.61003L4.25256 5.72789C4.64414 5.80621 4.79042 6.28935 4.50804 6.57173L1.26063 9.81915C1.06536 10.0144 0.748774 10.0144 0.553513 9.81914L0.199967 9.46558Z\\\"\\n          fill=\\\"currentColor\\\" />\\n      </svg>\\n    </div>\\n  </a>\\n</div>\\n\\n<div>\\n  <h1>Grounded Summarization Using Command R</h1>\\n</div>\\n\\n<style>\\n  .header {\\n    padding: 9px 0 17px 0;\\n    display: flex;\\n    flex-direction: column;\\n  }\\n\\n  a[href],\\n  .field-description a:not([href=\\\"\\\"]),\\n  .markdown-body a[href],\\n  .markdown-body a:not([href=\\\"\\\"]) {\\n    text-decoration: none;\\n  }\\n\\n  #content {\\n    padding: 0 32px;\\n  }\\n\\n  #content-head {\\n    display: none;\\n  }\\n\\n  .guide-page-title {\\n    font-size: 29px !important;\\n  }\\n\\n  .back-button .github-button {\\n    border-radius: 0 !important;\\n    border-width: 0 !important;\\n    background-color: inherit !important;\\n  }\\n\\n  .cookbook-nav-container {\\n    display: flex;\\n    flex-direction: row;\\n    justify-content: space-between;\\n    align-items: center;\\n  }\\n\\n  @media only screen and (min-width: 620px) {\\n    .guide-page-title {\\n      width: 70%;\\n    }\\n\\n    .header {\\n      display: flex;\\n      flex-direction: row;\\n      align-items: center;\\n      justify-content: space-between;\\n      padding: 9px 0 17px 0;\\n    }\\n\\n    .git--button {\\n      width: 145px !important;\\n      display: flex;\\n      flex-direction: row;\\n      justify-content: center;\\n      align-items: center;\\n      padding: 8px 16px;\\n\\n      width: 154px;\\n      height: 35px;\\n\\n      background: #D4D9D4;\\n      border: 1px solid #9DAAA4;\\n      border-radius: 6px;\\n    }\\n  }\\n\\n\\n  @media only screen and (min-width: 1024px) {\\n    .guide-page-title {\\n      font-size: 46px !important;\\n    }\\n\\n    .header {\\n      padding: 9px 0 32px 0;\\n    }\\n  }\\n</style>\"\n}\n[/block]\n\nNote: we are in the process of updating the links in this notebook. If a link doesn't work, please open an issue and we'll rectify it ASAP. Thanks for your understanding!\n\nLinks to add:\n* Cell 1: long-form, grounded summarisation blog post\n* Section 4: to text-rank method (context filtering)\n\n\nThis notebook provides the code to produce the outputs described in [this blog post](https://docs.google.com/document/d/1Eeakpz_FZoeMzJnQieqQWCpPtQuNiTGW4fueU9J0QHA/edit).\n\n## Table of contents\n\n1. [Setup](#setup)\n2. [Out-of-the-box summarization with Command-R](#out-of-the-box-summarization-with-command-r)\n3. [Introduce citations to the summary for grounding](#introduce-citations-to-the-summary-for-grounding)\n4. [Reduce the cost of summarization calls](#reduce-the-cost-of-summarization-calls)\n\n<a id=\"setup\"></a>\n<a name=\"setup\"></a>\n## 1. Setup\n\n\n```python\n%%capture\n\nimport cohere\nimport networkx as nx\nimport nltk\nnltk.download(\"punkt\")\nfrom nltk.tokenize import sent_tokenize\nimport numpy as np\nimport spacy\n\nfrom collections import deque\nfrom getpass import getpass\nimport re\nfrom typing import List, Tuple\n\nco_api_key = getpass(\"Enter your Cohere API key: \")\nco_model = \"command-r\"\nco = cohere.Client(co_api_key)\n\n```\n\n\n```python\n\nfrom google.colab import drive\ndrive.mount(\"/content/drive\", force_remount=True)\n\nfpath = \"drive/Shareddrives/FDE/Cookbooks/Long-form summarisation/ai_and_future_of_work.txt\"\nwith open(fpath, \"r\") as f:\n  text = f.read()\n\nnum_tokens = co.tokenize(text).length\nprint(f\"Loaded IMF report with {num_tokens} tokens\")\n\n\n```\n\n### Aside: define utils\n\n\n```python\n\ndef split_text_into_sentences(text: str) -> List[str]:\n    sentences =  sent_tokenize(text)\n    return sentences\n\ndef group_sentences_into_passages(sentence_list: List[str], n_sentences_per_passage: int = 10):\n    \"\"\"\n    Group sentences into passages of n_sentences sentences.\n    \"\"\"\n    passages = []\n    passage = \"\"\n    for i, sentence in enumerate(sentence_list):\n        passage += sentence + \" \"\n        if (i + 1) % n_sentences_per_passage == 0:\n            passages.append(passage)\n            passage = \"\"\n    return passages\n\ndef build_simple_chunks(text, n_sentences: int = 10):\n    \"\"\"\n    Build chunks of text from the input text.\n    \"\"\"\n    sentences = split_text_into_sentences(text)\n    chunks = group_sentences_into_passages(sentences, n_sentences_per_passage=n_sentences)\n    return chunks\n\n\n\ndef insert_citations(text: str, citations: List[dict]):\n    \"\"\"\n    A helper function to pretty print citations.\n    \"\"\"\n    offset = 0\n    # Process citations in the order they were provided\n    for citation in citations:\n        # Adjust start/end with offset\n        start, end = citation['start'] + offset, citation['end'] + offset\n        placeholder = \"[\" + \", \".join(doc[4:] for doc in citation[\"document_ids\"]) + \"]\"\n        # ^ doc[4:] removes the 'doc_' prefix, and leaves the quoted document\n        modification = f'{text[start:end]} {placeholder}'\n        # Replace the cited text with its bolded version + placeholder\n        text = text[:start] + modification + text[end:]\n        # Update the offset for subsequent replacements\n        offset += len(modification) - (end - start)\n\n    return text\n\n\n\ndef textrank(text: str, co, max_tokens: int, n_sentences_per_passage: int) -> str:\n    \"\"\"\n    Shortens `text` by extracting key units of text from `text` based on their centrality and concatenating them.\n    The output is the concatenation of those key units, in their original order. Centrality is graph-theoretic\n    measure of connectedness of a node; the more connected a node is to surrounding nodes (and the more sparsely\n    those neighbours are connected), the higher centrality.\n\n    Key passages are identified via clustering in a three-step process:\n    1. Break up `long` into chunks (either sentences or passages, based on `unit`)\n    2. Embed each chunk using Cohere's embedding model and construct a similarity matrix\n    3. Compute the centrality of each chunk\n    4. Keep the highest-centrality chunks until `max_tokens` is reached\n    5. Put together shorterned text by reordering chunks in their original order\n\n    This approach is based on summarise.long_doc_summarization.extraction::extract_single_doc with sorting by\n    centrality. Adapted here because installing the `summarise` repo would have added a lot of unused functionalities\n    and dependencies.\n    \"\"\"\n\n    # 1. Chunk text into units\n    chunks = build_simple_chunks(text, n_sentences_per_passage)\n\n    # 2. Embed and construct similarity matrix\n    embeddings = np.array(\n        co.embed(\n            texts=chunks,\n            model=\"embed-english-v3.0\",\n            input_type=\"clustering\",\n        ).embeddings\n    )\n    similarities = np.dot(embeddings, embeddings.T)\n\n    # 3. Compute centrality and sort sentences by centrality\n    # Easiest to use networkx's `degree` function with similarity as weight\n    g = nx.from_numpy_array(similarities, edge_attr=\"weight\")\n    centralities = g.degree(weight=\"weight\")\n    idcs_sorted_by_centrality = [node for node, degree in sorted(centralities, key=lambda item: item[1], reverse=True)]\n\n    # 4. Add chunks back in order of centrality\n    selected = _add_chunks_by_priority(co, chunks, idcs_sorted_by_centrality, max_tokens)\n\n    # 5. Put condensed text back in original order\n    separator = \"\\n\"\n    short = separator.join([chunk for index, chunk in sorted(selected, key=lambda item: item[0], reverse=False)])\n\n    return short\n\n\ndef _add_chunks_by_priority(\n    co, chunks: List[str], idcs_sorted_by_priority: List[int], max_tokens: int\n) -> List[Tuple[int, str]]:\n    \"\"\"\n    Given chunks of text and their indices sorted by priority (highest priority first), this function\n    fills the model context window with as many highest-priority chunks as possible.\n\n    The output is a list of (index, chunk) pairs, ordered by priority. To stitch back the chunks into\n    a cohesive text that preserves chronological order, sort the output on its index.\n    \"\"\"\n\n    selected = []\n    num_tokens = 0\n    idcs_queue = deque(idcs_sorted_by_priority)\n\n    while num_tokens < max_tokens and len(idcs_queue) > 0:\n        next_idx = idcs_queue.popleft()\n        num_tokens += co.tokenize(chunks[next_idx]).length - 2\n        # num_tokens += len(tokenizer.encode(chunks[next_idx]).ids) - 2\n        # ^ removing BOS and EOS tokens from count\n        selected.append((next_idx, chunks[next_idx]))\n        # ^ keep index and chunk, to reorder chronologically\n    if num_tokens > max_tokens:\n        selected.pop()\n\n    return selected\n\n```\n\n<a id=\"out-of-the-box-summarization-with-command-r\"></a>\n<a name=\"out-of-the-box-summarization-with-command-r\"></a>\n## 2. Out-of-the-box summarization with Command-R\n\nFirst, let's see Command-R's out-of-the-box performance. It's a 128k-context model, so we can pass the full IMF report in a single call. We replicate the exact instructions from the original tweet (correcting for a minor typo) for enabling fair comparisons.\n\n\n```python\nprompt_template = \"\"\"\\\n## text\n{text}\n\n## instructions\nStep 1. Read the entire text from the first to the last page.\nStep 2. Create a summary of every chapter from the first to the last page.\n\n## summary\n\"\"\"\n\nprompt = prompt_template.format(text=text)\nresp = co.chat(\n  message=prompt,\n  model=co_model,\n  temperature=0.3,\n  return_prompt=True\n)\n\nnum_tokens_in = co.tokenize(resp.prompt).length\nnum_tokens_out = resp.meta[\"billed_units\"][\"output_tokens\"]\nprint(f\"Generated summary with {num_tokens_in} tokens in, {num_tokens_out} tokens out\")\nprint()\nprint(\"--- Out-of-the-box summary with Command-R ---\")\nprint()\nprint(resp.text)\n\n```\n\n<a id=\"introduce-citations-to-the-summary-for-grounding\"></a>\n<a name=\"introduce-citations-to-the-summary-for-grounding\"></a>\n## 3. Introduce citations to the summary for grounding\n\nWhen summarizing long documents, introducing citations is one simple method for checking the factuality of the summary without needing to read the full document.\n\n\nWe've trained Command-R to introduce citations whenever prompted by our grounded generations instructions. Triggering this grounded mode is straightforward. Starting from the previous snippet, we only need to make two changes:\n1. Pass our text to the `documents` argument\n2. Pass our instructions to the `message` argument\n\nFor more information on how to enable grounded generation via our `co.chat` API, please refer to our [documentation](https://docs.cohere.com/reference/chat).\n\nFinally, note that we chunk the IMF report into multiple documents before passing them to `co.chat`. This isn't necessary (`co.chat` annotates citations at the character level), but allows for more human-readable citations.\n\n\n```python\nsummarize_preamble = \"\"\"\\\nYou will receive a series of text fragments from an article that are presented in chronological order. \\\nAs the assistant, you must generate responses to user's requests based on the information given in the fragments. \\\nEnsure that your responses are accurate and truthful, and that you reference your sources where appropriate to answer \\\nthe queries, regardless of their complexity.\\\n\"\"\"\n\ninstructions = \"\"\"\\\n## instructions\nStep 1. Read the entire text from the first to the last page.\nStep 2. Create a summary of every chapter from the first to the last page.\n\"\"\"\n\nchunked = build_simple_chunks(text, n_sentences=30)\nresp = co.chat(\n  preamble=summarize_preamble,\n  message=instructions,\n  documents=[{\"text\": chunk} for chunk in chunked],\n  model=co_model,\n  temperature=0.3,\n  return_prompt=True\n)\n\nnum_tokens_in = co.tokenize(resp.prompt).length\nnum_tokens_out = resp.meta[\"billed_units\"][\"output_tokens\"]\nprint(f\"Generated summary with {num_tokens_in} tokens in, {num_tokens_out} tokens out\")\nprint()\nprint(\"--- Summary with citations using grounded generation in Command-R ---\")\nprint()\nprint(resp.text)\n\n```\n\nLet's display the citations inside our answer:\n\n\n```python\nprint(insert_citations(resp.text, resp.citations))\n```\n\nWe can now visualise which section of the answer is based on which passage in the main text. Verifying factuality is straightforward: pick a section and verify that the relevant information is contained in the cited chunk.\n\nFor instance, let's verify the statement\n```\nAround 40% of employment worldwide is exposed to AI [1, 6]\n```\nby checking its chunk:\n\n\n```python\nprint(chunked[6])\n```\n\nSeems convincing!\nBy repeating such checks, it's straightforward to build trust in your summaries.\n\n<a id=\"reduce-the-cost-of-summarization-calls\"></a>\n<a name=\"reduce-the-cost-of-summarization-calls\"></a>\n## 4. Reduce the cost of summarization calls\n\nEven though Command-R is an efficient, light-weight model, for some applications we may accept trading off some summarization quality for lower costs. To do this, we must reduce the amount of tokens sent to the model -- but how do we select the most relevant bits?\n\nWe have a whole notebook dedicated to methods for reducing context length. Here, we call our 'text-rank' method to select maximally central chunks in a graph based on the chunk-to-chunk similarties. For more detail, please refer [to this cookbook](https://colab.research.google.com/drive/1zxSAbruOWwWJHNsj3N56uxZtUeiS7Evd).\n\n\n```python\nnum_tokens = 8192\nshortened = textrank(text, co, num_tokens, n_sentences_per_passage=30)\n\nchunked = build_simple_chunks(shortened)\nresp = co.chat(\n  message=instructions,\n  documents=[{\"text\": chunk} for chunk in chunked],\n  model=co_model,\n  temperature=0.3,\n  return_prompt=True\n)\n\nnum_tokens_in = co.tokenize(resp.prompt).length\nnum_tokens_out = resp.meta[\"billed_units\"][\"output_tokens\"]\nprint(f\"Generated summary with {num_tokens_in} tokens in, {num_tokens_out} tokens out\")\nprint()\nprint(\"--- Summary with citations using text-rank + grounding in Command-R ---\")\nprint()\nprint(resp.text)\n\n```\n\nThe summary is looking convincing! In practice, the trade-off between cost-efficiency and performance should be considered carefully.",
    "html": "",
    "htmlmode": false,
    "fullscreen": false,
    "hidden": true,
    "revision": 6,
    "_id": "664cbbdf624a1f00537dcb1f",
    "__v": 0,
    "createdAt": "2024-05-21T15:21:03.016Z",
    "lastUpdatedHash": "837d321c41da61eefe9f8986afbfdeda8e5b75bf",
    "project": "62cde2919aafea009aefb289",
    "updatedAt": "2024-07-11T01:20:23.411Z",
    "user": "5af39863989da435b05d284d"
  },
  "meta": {
    "user": {
      "allowedProjects": ["cohere-ai", "cohere-enterprise"],
      "apiKey": "",
      "email": "andrewjiang@hey.com",
      "name": "Andrew Jiang",
      "version": 1,
      "Name": "Andrew Jiang",
      "Email": "andrewjiang@hey.com",
      "APIKey": "",
      "AllowedProjects": ["cohere-ai", "cohere-enterprise"]
    },
    "baseUrl": "/",
    "hidden": true,
    "title": "Grounded Summarization Using Command R",
    "metaTitle": "Grounded Summarization Using Command R",
    "keywords": "",
    "description": "",
    "image": [],
    "slug": "grounded-summarization",
    "type": "custompage",
    "full": false
  }
}
