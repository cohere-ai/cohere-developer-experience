{
  "custompage": {
    "metadata": {
      "image": [],
      "title": "",
      "description": "",
      "keywords": ""
    },
    "algolia": {
      "recordCount": 3,
      "publishPending": false,
      "updatedAt": "2024-07-11T01:20:25.266Z"
    },
    "title": "Migrating Monolithic Prompts to Command-R with RAG",
    "slug": "migrating-prompts",
    "body": "[block:html]\n{\n  \"html\": \"<div class=\\\"cookbook-nav-container\\\">\\n  <a href=\\\"/page/cookbooks\\\" class=\\\"back-button pt-10 group inline-block cursor-pointer font-medium \\\" rel=\\\"noreferrer\\\"\\n    target=\\\"_self\\\">\\n    <div class=\\\"pr-1 inline-block group-hover:no-underline\\\">\\n      <svg width=\\\"11.8\\\" height=\\\"11\\\" viewBox=\\\"0 0 14 13\\\" fill=\\\"none\\\" xmlns=\\\"http://www.w3.org/2000/svg\\\">\\n        <path\\n          d=\\\"M1.1554 7.20808C1.35066 7.40335 1.66724 7.40335 1.8625 7.20808L7.18477 1.88582C7.38003 1.69055 7.38003 1.37397 7.18477 1.17871L6.83121 0.825157C6.63595 0.629895 6.31937 0.629896 6.12411 0.825157L0.801842 6.14742C0.60658 6.34269 0.60658 6.65927 0.801842 6.85453L1.1554 7.20808Z\\\"\\n          fill=\\\"currentColor\\\" />\\n        <path\\n          d=\\\"M1.1554 5.79226C1.35066 5.597 1.66724 5.597 1.8625 5.79226L7.18477 11.1145C7.38003 11.3098 7.38003 11.6264 7.18477 11.8216L6.83121 12.1752C6.63595 12.3705 6.31937 12.3705 6.12411 12.1752L0.801842 6.85292C0.60658 6.65766 0.60658 6.34108 0.801842 6.14582L1.1554 5.79226Z\\\"\\n          fill=\\\"currentColor\\\" />\\n        <path\\n          d=\\\"M2.52491 6.23674C2.52492 5.9606 2.74878 5.73675 3.02491 5.73675H6.28412C6.4513 5.73675 6.60742 5.8203 6.70015 5.95941L7.03347 6.45941C7.25499 6.79169 7.01679 7.23675 6.61745 7.23675H3.0249C2.74876 7.23675 2.5249 7.01289 2.5249 6.73674L2.52491 6.23674Z\\\"\\n          fill=\\\"currentColor\\\" />\\n        <path\\n          d=\\\"M13.5517 6.73676C13.5517 7.0129 13.3278 7.23675 13.0517 7.23675H8.79246C8.62528 7.23675 8.46916 7.1532 8.37643 7.0141L8.04311 6.5141C7.8216 6.18182 8.05979 5.73675 8.45914 5.73675H13.0517C13.3278 5.73675 13.5517 5.96062 13.5517 6.23676L13.5517 6.73676Z\\\"\\n          fill=\\\"currentColor\\\" />\\n      </svg>\\n    </div>\\n    Back to Cookbooks\\n  </a>\\n\\n  <a href=https://github.com/cohere-ai/cohere-developer-experience/blob/main/notebooks/guides/Migrating_Monolithic_Prompts_to_Command_R_with_RAG.ipynb class=\\\"github-button pt-10 group inline-block cursor-pointer font-medium \\\" rel=\\\"noreferrer\\\"\\n    target=\\\"_blank\\\">\\n    Open in GitHub\\n    <div class=\\\"pl-1 inline-block group-hover:no-underline\\\">\\n      <svg width=\\\"14\\\" height=\\\"10\\\" viewBox=\\\"0 0 14 10\\\" fill=\\\"none\\\" xmlns=\\\"http://www.w3.org/2000/svg\\\">\\n        <path\\n          d=\\\"M8.63218 0.366821C8.35604 0.366821 8.13218 0.590679 8.13218 0.866821V8.39364C8.13218 8.66978 8.35604 8.89364 8.63218 8.89364H9.13218C9.40832 8.89364 9.63218 8.66978 9.63218 8.39364V0.866821C9.63218 0.590678 9.40832 0.366821 9.13218 0.366821H8.63218Z\\\"\\n          fill=\\\"currentColor\\\" />\\n        <path\\n          d=\\\"M9.63332 1.36796C9.63332 1.6441 9.40946 1.86796 9.13332 1.86796H1.6065C1.33035 1.86796 1.1065 1.6441 1.1065 1.36796V0.867956C1.1065 0.591813 1.33035 0.367956 1.6065 0.367956H9.13332C9.40946 0.367956 9.63332 0.591813 9.63332 0.867956V1.36796Z\\\"\\n          fill=\\\"currentColor\\\" />\\n        <path\\n          d=\\\"M8.35063 2.02206C8.54588 2.21732 8.54588 2.5339 8.35062 2.72916L6.04601 5.03377C5.9278 5.15198 5.75833 5.20329 5.59439 5.1705L5.00515 5.05264C4.61356 4.97432 4.46728 4.49118 4.74966 4.2088L7.28997 1.66849C7.48523 1.47323 7.80182 1.47323 7.99708 1.6685L8.35063 2.02206Z\\\"\\n          fill=\\\"currentColor\\\" />\\n        <path\\n          d=\\\"M0.199967 9.46558C0.0047119 9.27032 0.0047151 8.95374 0.199974 8.75848L3.21169 5.74677C3.3299 5.62855 3.49938 5.57724 3.66331 5.61003L4.25256 5.72789C4.64414 5.80621 4.79042 6.28935 4.50804 6.57173L1.26063 9.81915C1.06536 10.0144 0.748774 10.0144 0.553513 9.81914L0.199967 9.46558Z\\\"\\n          fill=\\\"currentColor\\\" />\\n      </svg>\\n    </div>\\n  </a>\\n</div>\\n\\n<div>\\n  <h1>Migrating Monolithic Prompts to Command-R with RAG</h1>\\n</div>\\n\\n<style>\\n  .header {\\n    padding: 9px 0 17px 0;\\n    display: flex;\\n    flex-direction: column;\\n  }\\n\\n  a[href],\\n  .field-description a:not([href=\\\"\\\"]),\\n  .markdown-body a[href],\\n  .markdown-body a:not([href=\\\"\\\"]) {\\n    text-decoration: none;\\n  }\\n\\n  #content {\\n    padding: 0 32px;\\n  }\\n\\n  #content-head {\\n    display: none;\\n  }\\n\\n  .guide-page-title {\\n    font-size: 29px !important;\\n  }\\n\\n  .back-button .github-button {\\n    border-radius: 0 !important;\\n    border-width: 0 !important;\\n    background-color: inherit !important;\\n  }\\n\\n  .cookbook-nav-container {\\n    display: flex;\\n    flex-direction: row;\\n    justify-content: space-between;\\n    align-items: center;\\n  }\\n\\n  @media only screen and (min-width: 620px) {\\n    .guide-page-title {\\n      width: 70%;\\n    }\\n\\n    .header {\\n      display: flex;\\n      flex-direction: row;\\n      align-items: center;\\n      justify-content: space-between;\\n      padding: 9px 0 17px 0;\\n    }\\n\\n    .git--button {\\n      width: 145px !important;\\n      display: flex;\\n      flex-direction: row;\\n      justify-content: center;\\n      align-items: center;\\n      padding: 8px 16px;\\n\\n      width: 154px;\\n      height: 35px;\\n\\n      background: #D4D9D4;\\n      border: 1px solid #9DAAA4;\\n      border-radius: 6px;\\n    }\\n  }\\n\\n\\n  @media only screen and (min-width: 1024px) {\\n    .guide-page-title {\\n      font-size: 46px !important;\\n    }\\n\\n    .header {\\n      padding: 9px 0 32px 0;\\n    }\\n  }\\n</style>\"\n}\n[/block]\n\n\nCommand-R is a powerful LLM optimized for long context tasks such as retrieval augmented generation (RAG). Migrating a monolithic task such as question-answering or query-focused summarization to RAG can improve the quality of responses due to reduced hallucination and improved conciseness through grounding.\n\nPreviously, migrating an existing use case to RAG involved a lot of manual work around indexing documents, implementing at least a basic search strategy, extensive post-processing to introduce proper grounding through citations, and of course fine-tuning an LLM to work well in the RAG paradigm.\n\nThis cookbook demonstrates automatic migration of monolithic prompts through two diverse use cases where an original prompt is broken down into two parts: (1) context; and (2) instructions. The former can be done automatically or through simple chunking, while the latter is done automatically by Command-R through single shot prompt optimization.\n\nThe two use cases demonstrated here are:\n\n1. Autobiography Assistant; and\n2. Legal Question Answering\n\n\n```python\n#!pip install cohere\n```\n\n\n```python\nimport json\nimport os\nimport re\n\nimport cohere\nimport getpass\n```\n\n\n```python\nCO_API_KEY = getpass.getpass('cohere API key:')\n```\n\n    cohere API key:··········\n\n\n\n```python\nco = cohere.Client(CO_API_KEY)\n```\n\n## Autobiography Assistant\n\nThis application scenario is a common LLM-as-assistant use case: given some context, help the user to complete a task. In this case, the task is to write a concise autobiographical summary.\n\n\n```python\noriginal_prompt = '''## information\nCurrent Job Title: Senior Software Engineer\nCurrent Company Name: GlobalSolTech\nWork Experience: Over 15 years of experience in software engineering, specializing in AI and machine learning. Proficient in Python, C++, and Java, with expertise in developing algorithms for natural language processing, computer vision, and recommendation systems.\nCurrent Department Name: AI Research and Development\nEducation: B.Sc. in Physics from Trent University (2004), Ph.D. in Statistics from HEC in Paris (2010)\nHobbies: I love hiking in the mountains, free diving, and collecting and restoring vintage world war one mechanical watches.\nFamily: Married with 4 children and 3 grandchildren.\n\n## instructions\nYour task is to assist a user in writing a short biography for social media.\nThe length of the text should be no more than 100 words.\nWrite the summary in first person.'''\n```\n\n\n```python\nresponse = co.chat(\n    message=original_prompt,\n    model='command-r',\n)\n```\n\n\n```python\nprint(response.text)\n```\n\n    I'm a Senior Software Engineer at GlobalSolTech, with over 15 years of experience in AI and machine learning. My expertise lies in developing innovative algorithms for natural language processing, computer vision, and recommendation systems. I hold a B.Sc. in Physics and a Ph.D. in Statistics and enjoy hiking, free diving, and collecting vintage watches in my spare time. I'm passionate about using my skills to contribute to cutting-edge AI research and development. At GlobalSolTech, I'm proud to be part of a dynamic team driving technological advancement.\n\n\nUsing Command-R, we can automatically upgrade the original prompt to a RAG-style prompt to get more faithful adherence to the instructions, a clearer and more concise prompt, and in-line citations for free. Consider the following meta-prompt:\n\n\n```python\nmeta_prompt = f'''Below is a task for an LLM delimited with ## Original Task. Your task is to split that task into two parts: (1) the context; and (2) the instructions.\nThe context should be split into several separate parts and returned as a JSON object where each part has a name describing its contents and the value is the contents itself.\nMake sure to include all of the context contained in the original task description and do not change its meaning.\nThe instructions should be re-written so that they are very clear and concise. Do not change the meaning of the instructions or task, just make sure they are very direct and clear.\nReturn everything in a JSON object with the following structure:\n\n{{\n  \"context\": [{{\"<description of part 1>\": \"<content of part 1>\"}}, ...],\n  \"instructions\": \"<the re-written instructions>\"\n}}\n\n## Original Task\n{original_prompt}\n'''\n```\n\n\n```python\nprint(meta_prompt)\n```\n\n    Below is a task for an LLM delimited with ## Original Task. Your task is to split that task into two parts: (1) the context; and (2) the instructions.\n    The context should be split into several separate parts and returned as a JSON object where each part has a name describing its contents and the value is the contents itself.\n    Make sure to include all of the context contained in the original task description and do not change its meaning.\n    The instructions should be re-written so that they are very clear and concise. Do not change the meaning of the instructions or task, just make sure they are very direct and clear.\n    Return everything in a JSON object with the following structure:\n    \n    {\n      \"context\": [{\"<description of part 1>\": \"<content of part 1>\"}, ...],\n      \"instructions\": \"<the re-written instructions>\"\n    }\n    \n    ## Original Task\n    ## information\n    Current Job Title: Senior Software Engineer\n    Current Company Name: GlobalSolTech\n    Work Experience: Over 15 years of experience in software engineering, specializing in AI and machine learning. Proficient in Python, C++, and Java, with expertise in developing algorithms for natural language processing, computer vision, and recommendation systems.\n    Current Department Name: AI Research and Development\n    Education: B.Sc. in Physics from Trent University (2004), Ph.D. in Statistics from HEC in Paris (2010)\n    Hobbies: I love hiking in the mountains, free diving, and collecting and restoring vintage world war one mechanical watches.\n    Family: Married with 4 children and 3 grandchildren.\n    \n    ## instructions\n    Your task is to assist a user in writing a short biography for social media.\n    The length of the text should be no more than 100 words.\n    Write the summary in first person.\n    \n\n\nCommand-R returns with the following:\n\n\n```python\nupgraded_prompt = co.chat(\n    message=meta_prompt,\n    model='command-r',\n)\n```\n\n\n```python\nprint(upgraded_prompt.text)\n```\n\n    Here is the task delved into a JSON object as requested:\n    ```json\n    {\n      \"context\": [\n        {\n          \"Work Experience\": \"Over 15 years of AI and machine learning engineering experience. Proficient in Python, C++, and Java, with expertise in developing algorithms for natural language processing, computer vision, and recommendation systems.\"\n        },\n        {\n          \"Education\": \"B.Sc. in Physics (Trent University, 2004) and Ph.D. in Statistics (HEC Paris, 2010).\"\n        },\n        {\n          \"Personal Life\": \"I’m a married senior software engineer with 4 children and 3 grandchildren. I enjoy hiking, free diving, and vintage watch restoration.\"\n        },\n        {\n          \"Current Position\": \"I work at GlobalSolTech in the AI Research and Development department as a senior software engineer.\"\n        }\n      ],\n      \"instructions\": \"Using the provided information, write a concise, first-person social media biography of no more than 100 words.\"\n    }\n    ```\n\n\nTo extract the returned information, we will write two simple functions to post-process out the JSON and then parse it.\n\n\n```python\ndef get_json(text: str) -> str:\n    matches = [m.group(1) for m in re.finditer(\"```([\\w\\W]*?)```\", text)]\n    if len(matches):\n        postproced = matches[0]\n        if postproced[:4] == 'json':\n            return postproced[4:]\n        return postproced\n    return text\n```\n\n\n```python\ndef get_prompt_and_docs(text: str) -> tuple:\n    json_obj = json.loads(get_json(text))\n    prompt = json_obj['instructions']\n    docs = []\n    for item in json_obj['context']:\n        for k,v in item.items():\n            docs.append({\"title\": k, \"snippet\": v})\n    return prompt, docs\n```\n\n\n```python\nnew_prompt, docs = get_prompt_and_docs(upgraded_prompt.text)\n```\n\n\n```python\nnew_prompt, docs\n```\n\n\n\n\n    ('Using the provided information, write a concise, first-person social media biography of no more than 100 words.',\n     [{'title': 'Work Experience',\n       'snippet': 'Over 15 years of AI and machine learning engineering experience. Proficient in Python, C++, and Java, with expertise in developing algorithms for natural language processing, computer vision, and recommendation systems.'},\n      {'title': 'Education',\n       'snippet': 'B.Sc. in Physics (Trent University, 2004) and Ph.D. in Statistics (HEC Paris, 2010).'},\n      {'title': 'Personal Life',\n       'snippet': 'I’m a married senior software engineer with 4 children and 3 grandchildren. I enjoy hiking, free diving, and vintage watch restoration.'},\n      {'title': 'Current Position',\n       'snippet': 'I work at GlobalSolTech in the AI Research and Development department as a senior software engineer.'}])\n\n\n\nAs we can see above, the new prompt is much more concise and gets right to the point. The context has been split into 4 \"documents\" that Command-R can ground the information to. Now let's run the same task with the new prompt while leveraging the `documents=` parameter. Note that the `docs` variable is a list of dict objects with `title` describing the contents of a text and `snippet` containing the text itself:\n\n\n```python\nresponse = co.chat(\n    message=new_prompt,\n    model='command-r',\n    documents=docs,\n)\n```\n\n\n```python\nprint(response.text)\n```\n\n    I'm a senior software engineer with a Ph.D. in Statistics and over 15 years of AI and machine learning engineering experience. My current focus at GlobalSolTech's AI R&D department is developing algorithms for natural language processing, computer vision, and recommendation systems. In my free time, I enjoy hiking, freediving, and restoring vintage watches, and I'm a married father of four with three grandchildren.\n\n\nThe response is concise. More importantly, we can ensure that there is no hallucination because the text is automatically grounded in the input documents. Using the simple function below, we can add this grounding information to the text as citations:\n\n\n```python\ndef insert_citations(text: str, citations: list[dict], add_one: bool=False):\n    \"\"\"\n    A helper function to pretty print citations.\n    \"\"\"\n    offset = 0\n    # Process citations in the order they were provided\n    for citation in citations:\n        # Adjust start/end with offset\n        start, end = citation.start + offset, citation.end + offset\n        if add_one:\n            cited_docs = [str(int(doc[4:]) + 1) for doc in citation.document_ids]\n        else:\n            cited_docs = [doc[4:] for doc in citation.document_ids]\n        # Shorten citations if they're too long for convenience\n        if len(cited_docs) > 3:\n            placeholder = \"[\" + \", \".join(cited_docs[:3]) + \"...]\"\n        else:\n            placeholder = \"[\" + \", \".join(cited_docs) + \"]\"\n        # ^ doc[4:] removes the 'doc_' prefix, and leaves the quoted document\n        modification = f'{text[start:end]} {placeholder}'\n        # Replace the cited text with its bolded version + placeholder\n        text = text[:start] + modification + text[end:]\n        # Update the offset for subsequent replacements\n        offset += len(modification) - (end - start)\n\n    return text\n```\n\n\n```python\nprint(insert_citations(response.text, response.citations, True))\n```\n\n    I'm a senior software engineer [3, 4] with a Ph.D. in Statistics [2] and over 15 years of AI and machine learning engineering experience. [1] My current focus at GlobalSolTech's AI R&D department [4] is developing algorithms for natural language processing, computer vision, and recommendation systems. [1] In my free time, I enjoy hiking, freediving, and restoring vintage watches [3], and I'm a married father of four with three grandchildren. [3]\n\n\nNow let's move on to an arguably more difficult problem.\n\n## Legal Question Answering\n\nOn March 21st, the DOJ announced that it is [suing Apple](https://www.theverge.com/2024/3/21/24107659/apple-doj-lawsuit-antitrust-documents-suing) for anti-competitive practices. The [complaint](https://www.justice.gov/opa/media/1344546/dl) is 88 pages long and consists of about 230 paragraphs of text. To understand what the suit alleges, a common use case would be to ask for a summary. Because Command-R has a context window of 128K, even an 88-page legal complaint fits comfortably within the window.\n\n\n```python\napple = open('data/apple_mod.txt').read()\n```\n\n\n```python\ntokens = co.tokenize(text=apple, model='command-r')\nlen(tokens.tokens)\n```\n\n\n\n\n    29697\n\n\n\nWe can set up a prompt template that allows us to ask questions on the original text.\n\n\n```python\nprompt_template = '''\n{legal_text}\n\n{question}\n'''\n```\n\n\n```python\nquestion = '''Please summarize the attached legal complaint succinctly. Focus on answering the question: what does the complaint allege?'''\nrendered_prompt = prompt_template.format(legal_text=apple, question=question)\n```\n\n\n```python\nresponse = co.chat(\n    message=rendered_prompt,\n    model='command-r',\n    temperature=0.3,\n)\n```\n\n\n```python\nprint(response.text)\n```\n\n    The complaint alleges that Apple has violated antitrust laws by engaging in a pattern of anticompetitive conduct to maintain its monopoly power over the U.S. markets for smartphones and performance smartphones. Apple is accused of using its control over app distribution and access to its operating system to impede competition and innovation. Specifically, the company is said to have restricted developers' ability to create certain apps and limited the functionality of others, making it harder for consumers to switch away from iPhones to rival smartphones. This conduct is alleged to have harmed consumers and developers by reducing choice, increasing prices, and stifling innovation. The plaintiffs seek injunctive relief and potential monetary awards to remedy these illegal practices.\n\n\nThe summary seems clear enough. But we are interested in the specific allegations that the DOJ makes. For example, skimming the full complaint, it looks like the DOJ is alleging that Apple could encrypt text messages sent to Android phones if it wanted to do so. We can amend the rendered prompt and ask:\n\n\n```python\nquestion = '''Does the DOJ allege that Apple could encrypt text messages sent to Android phones?'''\nrendered_prompt = prompt_template.format(legal_text=apple, question=question)\n```\n\n\n```python\nresponse = co.chat(\n    message=rendered_prompt,\n    model='command-r',\n)\n```\n\n\n```python\nprint(response.text)\n```\n\n    Yes, the DOJ alleges that Apple could allow iPhone users to send encrypted messages to Android users while still using iMessage on their iPhones but chooses not to do so. According to the DOJ, this would instantly improve the privacy and security of iPhones and other smartphones.\n\n\nThis is a very interesting allegation that at first glance suggests that the model could be hallucinating. Because RAG has been shown to help reduce hallucinations and grounds its responses in the input text, we should convert this prompt to the RAG style paradigm to gain confidence in its response.\n\nWhile previously we asked Command-R to chunk the text for us, the legal complaint is highly structured with numbered paragraphs so we can use the following function to break the complaint into input docs ready for RAG:\n\n\n```python\ndef chunk_doc(input_doc: str) -> list:\n    chunks = []\n    current_para = 'Preamble'\n    current_chunk = ''\n    # pattern to find an integer number followed by a dot (finding the explicitly numbered paragraph numbers)\n    pattern = r'^\\d+\\.$'\n\n    for line in input_doc.splitlines():\n        if re.match(pattern, line):\n            chunks.append((current_para.replace('.', ''), current_chunk))\n            current_chunk = ''\n            current_para = line\n        else:\n            current_chunk += line + '\\n'\n\n    docs = []\n    for chunk in chunks:\n        docs.append({\"title\": chunk[0], \"snippet\": chunk[1]})\n\n    return docs\n```\n\n\n```python\nchunks = chunk_doc(apple)\n```\n\n\n```python\nprint(chunks[18])\n```\n\n    {'title': '18', 'snippet': '\\nProtecting competition and the innovation that competition inevitably ushers in\\nfor consumers, developers, publishers, content creators, and device manufacturers is why\\nPlaintiffs bring this lawsuit under Section 2 of the Sherman Act to challenge Apple’s\\nmaintenance of its monopoly over smartphone markets, which affect hundreds of millions of\\nAmericans every day. Plaintiffs bring this case to rid smartphone markets of Apple’s\\nmonopolization and exclusionary conduct and to ensure that the next generation of innovators\\ncan upend the technological world as we know it with new and transformative technologies.\\n\\n\\nII.\\n\\nDefendant Apple\\n\\n'}\n\n\nWe can now try the same question but ask it directly to Command-R with the chunks as grounding information.\n\n\n```python\nresponse = co.chat(\n    message='''Does the DOJ allege that Apple could encrypt text messages sent to Android phones?''',\n    model='command-r',\n    documents=chunks,\n)\n```\n\n\n```python\nprint(response.text)\n```\n\n    Yes, according to the DOJ, Apple could encrypt text messages sent from iPhones to Android phones. The DOJ claims that Apple degrades the security and privacy of its users by impeding cross-platform encryption and preventing developers from fixing the broken cross-platform messaging experience. Apple's conduct makes it harder to switch from iPhone to Android, as messages sent from iPhones to Android phones are unencrypted.\n\n\nThe responses seem similar, but we should add citations and check the citation to get confidence in the response.\n\n\n```python\nprint(insert_citations(response.text, response.citations))\n```\n\n    Yes, according to the DOJ, Apple could encrypt text messages sent from iPhones to Android phones. [144] The DOJ claims that Apple degrades the security and privacy [144] of its users by impeding cross-platform encryption [144] and preventing developers from fixing the broken cross-platform messaging experience. [93] Apple's conduct makes it harder to switch from iPhone to Android [144], as messages sent from iPhones to Android phones are unencrypted. [144]\n\n\nThe most important passage seems to be paragraph 144. Paragraph 93 is also cited. Let's check what they contain.\n\n\n```python\nprint(chunks[144]['snippet'])\n```\n\n    \n    Apple is also willing to make the iPhone less secure and less private if that helps\n    maintain its monopoly power. For example, text messages sent from iPhones to Android phones\n    are unencrypted as a result of Apple’s conduct. If Apple wanted to, Apple could allow iPhone\n    users to send encrypted messages to Android users while still using iMessage on their iPhone,\n    which would instantly improve the privacy and security of iPhone and other smartphone users.\n    \n    \n\n\n\n```python\nprint(chunks[93]['snippet'])\n```\n\n    \n    Recently, Apple blocked a third-party developer from fixing the broken cross-\n    platform messaging experience in Apple Messages and providing end-to-end encryption for\n    messages between Apple Messages and Android users. By rejecting solutions that would allow\n    for cross-platform encryption, Apple continues to make iPhone users’ less secure than they could\n    otherwise be.\n    \n    ii.\n    \n    \n\n\nParagraph 144 indeed contains the important allegation: **If Apple wanted to, Apple could allow iPhone users to send encrypted messages to Android users**.\n\n\nIn this cookbook we have shown how one can easily take an existing monolithic prompt and migrate it to the RAG paradigm to get less hallucination, grounded information, and in-line citations. We also demonstrated Command-R's ability to re-write an instruction prompt in a single shot to make it more concise and potentially lead to higher quality completions.",
    "html": "",
    "htmlmode": false,
    "fullscreen": false,
    "hidden": true,
    "revision": 7,
    "_id": "664cbbe7f8c657005bc15c69",
    "__v": 0,
    "createdAt": "2024-05-21T15:21:11.013Z",
    "lastUpdatedHash": "249edf25b2c12fcf0b5650fade08a58051f650ca",
    "project": "62cde2919aafea009aefb289",
    "updatedAt": "2024-07-11T01:20:25.266Z",
    "user": "5af39863989da435b05d284d"
  },
  "meta": {
    "user": {
      "allowedProjects": ["cohere-ai", "cohere-enterprise"],
      "apiKey": "",
      "email": "andrewjiang@hey.com",
      "name": "Andrew Jiang",
      "version": 1,
      "Name": "Andrew Jiang",
      "Email": "andrewjiang@hey.com",
      "APIKey": "",
      "AllowedProjects": ["cohere-ai", "cohere-enterprise"]
    },
    "baseUrl": "/",
    "hidden": true,
    "title": "Migrating Monolithic Prompts to Command-R with RAG",
    "metaTitle": "Migrating Monolithic Prompts to Command-R with RAG",
    "keywords": "",
    "description": "",
    "image": [],
    "slug": "migrating-prompts",
    "type": "custompage",
    "full": false
  }
}
