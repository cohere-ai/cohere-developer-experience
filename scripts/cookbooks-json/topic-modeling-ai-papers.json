{
  "custompage": {
    "metadata": {
      "image": [],
      "title": "",
      "description": "",
      "keywords": ""
    },
    "algolia": {
      "recordCount": 1,
      "publishPending": false,
      "updatedAt": "2024-07-11T01:20:28.569Z"
    },
    "title": "Topic Modeling AI Papers",
    "slug": "topic-modeling-ai-papers",
    "body": "[block:html]\n{\n  \"html\": \"<div class=\\\"cookbook-nav-container\\\">\\n  <a href=\\\"/page/cookbooks\\\" class=\\\"back-button pt-10 group inline-block cursor-pointer font-medium \\\" rel=\\\"noreferrer\\\"\\n    target=\\\"_self\\\">\\n    <div class=\\\"pr-1 inline-block group-hover:no-underline\\\">\\n      <svg width=\\\"11.8\\\" height=\\\"11\\\" viewBox=\\\"0 0 14 13\\\" fill=\\\"none\\\" xmlns=\\\"http://www.w3.org/2000/svg\\\">\\n        <path\\n          d=\\\"M1.1554 7.20808C1.35066 7.40335 1.66724 7.40335 1.8625 7.20808L7.18477 1.88582C7.38003 1.69055 7.38003 1.37397 7.18477 1.17871L6.83121 0.825157C6.63595 0.629895 6.31937 0.629896 6.12411 0.825157L0.801842 6.14742C0.60658 6.34269 0.60658 6.65927 0.801842 6.85453L1.1554 7.20808Z\\\"\\n          fill=\\\"currentColor\\\" />\\n        <path\\n          d=\\\"M1.1554 5.79226C1.35066 5.597 1.66724 5.597 1.8625 5.79226L7.18477 11.1145C7.38003 11.3098 7.38003 11.6264 7.18477 11.8216L6.83121 12.1752C6.63595 12.3705 6.31937 12.3705 6.12411 12.1752L0.801842 6.85292C0.60658 6.65766 0.60658 6.34108 0.801842 6.14582L1.1554 5.79226Z\\\"\\n          fill=\\\"currentColor\\\" />\\n        <path\\n          d=\\\"M2.52491 6.23674C2.52492 5.9606 2.74878 5.73675 3.02491 5.73675H6.28412C6.4513 5.73675 6.60742 5.8203 6.70015 5.95941L7.03347 6.45941C7.25499 6.79169 7.01679 7.23675 6.61745 7.23675H3.0249C2.74876 7.23675 2.5249 7.01289 2.5249 6.73674L2.52491 6.23674Z\\\"\\n          fill=\\\"currentColor\\\" />\\n        <path\\n          d=\\\"M13.5517 6.73676C13.5517 7.0129 13.3278 7.23675 13.0517 7.23675H8.79246C8.62528 7.23675 8.46916 7.1532 8.37643 7.0141L8.04311 6.5141C7.8216 6.18182 8.05979 5.73675 8.45914 5.73675H13.0517C13.3278 5.73675 13.5517 5.96062 13.5517 6.23676L13.5517 6.73676Z\\\"\\n          fill=\\\"currentColor\\\" />\\n      </svg>\\n    </div>\\n    Back to Cookbooks\\n  </a>\\n\\n  <a href=https://github.com/cohere-ai/cohere-developer-experience/blob/main/notebooks/guides/Topic_Modeling_AI_Papers.ipynb class=\\\"github-button pt-10 group inline-block cursor-pointer font-medium \\\" rel=\\\"noreferrer\\\"\\n    target=\\\"_blank\\\">\\n    Open in GitHub\\n    <div class=\\\"pl-1 inline-block group-hover:no-underline\\\">\\n      <svg width=\\\"14\\\" height=\\\"10\\\" viewBox=\\\"0 0 14 10\\\" fill=\\\"none\\\" xmlns=\\\"http://www.w3.org/2000/svg\\\">\\n        <path\\n          d=\\\"M8.63218 0.366821C8.35604 0.366821 8.13218 0.590679 8.13218 0.866821V8.39364C8.13218 8.66978 8.35604 8.89364 8.63218 8.89364H9.13218C9.40832 8.89364 9.63218 8.66978 9.63218 8.39364V0.866821C9.63218 0.590678 9.40832 0.366821 9.13218 0.366821H8.63218Z\\\"\\n          fill=\\\"currentColor\\\" />\\n        <path\\n          d=\\\"M9.63332 1.36796C9.63332 1.6441 9.40946 1.86796 9.13332 1.86796H1.6065C1.33035 1.86796 1.1065 1.6441 1.1065 1.36796V0.867956C1.1065 0.591813 1.33035 0.367956 1.6065 0.367956H9.13332C9.40946 0.367956 9.63332 0.591813 9.63332 0.867956V1.36796Z\\\"\\n          fill=\\\"currentColor\\\" />\\n        <path\\n          d=\\\"M8.35063 2.02206C8.54588 2.21732 8.54588 2.5339 8.35062 2.72916L6.04601 5.03377C5.9278 5.15198 5.75833 5.20329 5.59439 5.1705L5.00515 5.05264C4.61356 4.97432 4.46728 4.49118 4.74966 4.2088L7.28997 1.66849C7.48523 1.47323 7.80182 1.47323 7.99708 1.6685L8.35063 2.02206Z\\\"\\n          fill=\\\"currentColor\\\" />\\n        <path\\n          d=\\\"M0.199967 9.46558C0.0047119 9.27032 0.0047151 8.95374 0.199974 8.75848L3.21169 5.74677C3.3299 5.62855 3.49938 5.57724 3.66331 5.61003L4.25256 5.72789C4.64414 5.80621 4.79042 6.28935 4.50804 6.57173L1.26063 9.81915C1.06536 10.0144 0.748774 10.0144 0.553513 9.81914L0.199967 9.46558Z\\\"\\n          fill=\\\"currentColor\\\" />\\n      </svg>\\n    </div>\\n  </a>\\n</div>\\n\\n<div>\\n  <h1>Topic Modeling AI Papers</h1>\\n</div>\\n\\n<style>\\n  .header {\\n    padding: 9px 0 17px 0;\\n    display: flex;\\n    flex-direction: column;\\n  }\\n\\n  a[href],\\n  .field-description a:not([href=\\\"\\\"]),\\n  .markdown-body a[href],\\n  .markdown-body a:not([href=\\\"\\\"]) {\\n    text-decoration: none;\\n  }\\n\\n  #content {\\n    padding: 0 32px;\\n  }\\n\\n  #content-head {\\n    display: none;\\n  }\\n\\n  .guide-page-title {\\n    font-size: 29px !important;\\n  }\\n\\n  .back-button .github-button {\\n    border-radius: 0 !important;\\n    border-width: 0 !important;\\n    background-color: inherit !important;\\n  }\\n\\n  .cookbook-nav-container {\\n    display: flex;\\n    flex-direction: row;\\n    justify-content: space-between;\\n    align-items: center;\\n  }\\n\\n  @media only screen and (min-width: 620px) {\\n    .guide-page-title {\\n      width: 70%;\\n    }\\n\\n    .header {\\n      display: flex;\\n      flex-direction: row;\\n      align-items: center;\\n      justify-content: space-between;\\n      padding: 9px 0 17px 0;\\n    }\\n\\n    .git--button {\\n      width: 145px !important;\\n      display: flex;\\n      flex-direction: row;\\n      justify-content: center;\\n      align-items: center;\\n      padding: 8px 16px;\\n\\n      width: 154px;\\n      height: 35px;\\n\\n      background: #D4D9D4;\\n      border: 1px solid #9DAAA4;\\n      border-radius: 6px;\\n    }\\n  }\\n\\n\\n  @media only screen and (min-width: 1024px) {\\n    .guide-page-title {\\n      font-size: 46px !important;\\n    }\\n\\n    .header {\\n      padding: 9px 0 32px 0;\\n    }\\n  }\\n</style>\"\n}\n[/block]\n\n\nNatural Language Processing (NLP) is a hot topic in machine learning. It involves analyzing and understanding text-based data. Clustering algorithms are quite popular in NLP. They group a set of unlabeled texts in such a way that texts in the same cluster are more like one another. Topic modeling is one application of clustering in NLP. It uses unsupervised learning to extract topics from a collection of documents. Other applications include automatic document organization and fast information retrieval or filtering.\n\nYou'll learn how to use Cohere’s NLP tools to perform semantic search and clustering of AI Papers. This will help you discover trends in AI. You'll scrape the Journal of Artificial Intelligence Research. The output is a list of recently published AI papers. You’ll use Cohere’s Embed Endpoint to generate word embeddings using your list of AI papers. Finally, visualize the embeddings and proceed to build semantic search and topic modeling.\n\nTo follow along with this tutorial, you need to be familiar with Python. Make sure you have python version 3.6+ installed in your development machine. You can also use Google Colab to try out the project in the cloud. Finally, you need to have a Cohere Account. If you haven’t signed up already, register for a New Cohere Account. All new accounts receive $75 free credits. You'll access a Pay-As-You-Go option after finishing your credits.\n\nFirst, you need to install the python dependencies required to run the project. Use pip to install them using the command below\n\n```python\npip install requests beautifulsoup4 cohere altair clean-text numpy pandas sklearn > /dev/null\n```\n\nCreate a new python file named cohere_nlp.py. Write all your code in this file. import the dependencies and initialize Cohere’s client.\n\n```python\nimport cohere\n\napi_key = '<API_KEY>' \nco = cohere.Client(api_key)\n```\n\nThis tutorial focuses on applying topic modeling to look for recent trends in AI. This task requires you to source a list of titles for AI papers. Use web scraping techniques to collect a list of AI papers. Use the Journal of Artificial Intelligence Research as your data source. Finally, you will clean this data by removing unwanted characters and stop words.\n\nFirst, import the required libraries to make web requests and process the web content .\n\n```python\nimport requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\nimport numpy as np\nfrom cleantext import clean\n```\n\nNext, make an HTTP request to the source website that has an archive of the AI papers. \n\n```python\nURL = \"https://www.jair.org/index.php/jair/issue/archive\"\npage = requests.get(URL)\n```\n\nUse this archive to get the list of AI papers published. This archive has papers published since 2015. This tutorial considers papers published recently, on or after 2020 only.\n\n```python\nsoup = BeautifulSoup(page.content, \"html.parser\")\narchive_links = []\n\nfor link in soup.select('a.title'):\n  vol = link.text\n  link = link.get('href')\n  year = int(vol[vol.find(\"(\")+1:vol.find(\")\")])\n  if year >= 2020:\n    archive_links.append({ 'year': year, 'link': link })\n```\n\nFinally, you’ll need to clean the titles of the AI papers gathered. Remove trailing white spaces and unwanted characters. Use the NTLK library to get English stop words and filter them out.\n\n```python\npapers = []\nfor archive in archive_links:\n  page = requests.get(archive['link'])\n  soup = BeautifulSoup(page.content, \"html.parser\")\n  links = soup.select('h3.media-heading a')\n  for link in links:\n    # clean the title\n    title = clean(text=link.text,\n            fix_unicode=True,\n            to_ascii=True,\n            lower=True,\n            no_line_breaks=False,\n            no_urls=False,\n            no_emails=False,\n            no_phone_numbers=False,\n            no_numbers=False,\n            no_digits=False,\n            no_currency_symbols=False,\n            no_punct=False,\n            replace_with_punct=\"\",\n            replace_with_url=\"This is a URL\",\n            replace_with_email=\"Email\",\n            replace_with_phone_number=\"\",\n            replace_with_number=\"123\",\n            replace_with_digit=\"0\",\n            replace_with_currency_symbol=\"$\",\n            lang=\"en\")\n    papers.append({ 'year': archive['year'], 'title': title, 'link': link.get('href') })\n\n```\n\nThe dataset created using this process has 258 AI papers published between 2020 and 2022. Use pandas library to create a data frame to hold our text data. \n\n```python\ndf = pd.DataFrame(papers)\nprint(len(df))\n```\n\n```\n260\n```\n\nWord embedding is a technique for learning the representation of words. Words that have same meanings have similar representation. You can use these embeddings to:\n\n•\tcluster large amounts of text  \n•\tmatch a query with other similar sentences  \n•\tperform classification tasks like sentiment classification\n\nCohere’s platform provides an Embed Endpoint that returns text embeddings. An embedding is a list of floating-point numbers. They capture the semantic meaning of the represented text. Models used to create these embeddings are available in 3 sizes: small, medium, and large. Small models are faster while large models offer better performance.\n\nWrite a function to create the word embeddings using Cohere. The function should read as follows:\n\n```python\ndef get_embeddings(text,model='medium'):\n  output = co.embed(\n                model=model,\n                texts=[text])\n  return output.embeddings[0]\n```\n\nCreate a new column in your pandas data frame to hold the embeddings created.\n\n```python\ndf['title_embeds'] = df['title'].apply(get_embeddings)\n```\n\nCongratulations! You have created the word embeddings . Now, you will proceed to visualize the embeddings using a scatter plot. First, you need to reduce the dimensions of the word embeddings. You’ll use the Principal Component Analysis (PCA) method to achieve this task. Import the necessary packages and create a function to return the principle components.\n\n```python\nfrom sklearn.decomposition import PCA\n\ndef get_pc(arr,n):\n  pca = PCA(n_components=n)\n  embeds_transform = pca.fit_transform(arr)\n  return embeds_transform\n```\n\nNext, create a function to generate a scatter plot chart. You’ll use the altair library to create the charts.\n\n```python\nimport altair as alt\ndef generate_chart(df,xcol,ycol,lbl='off',color='basic',title=''):\n  chart = alt.Chart(df).mark_circle(size=500).encode(\n    x= alt.X(xcol,\n      scale=alt.Scale(zero=False),\n      axis=alt.Axis(labels=False, ticks=False, domain=False)\n    ),\n\n    y= alt.Y(ycol,\n      scale=alt.Scale(zero=False),\n      axis=alt.Axis(labels=False, ticks=False, domain=False)\n    ),\n    \n    color= alt.value('#333293') if color == 'basic' else color,\n    tooltip=['title']\n  )\n\n  if lbl == 'on':\n    text = chart.mark_text(align='left', baseline='middle',dx=15, size=13,color='black').encode(text='title', color= alt.value('black'))\n  else:\n    text = chart.mark_text(align='left', baseline='middle',dx=10).encode()\n\n  result = (chart + text).configure(background=\"#FDF7F0\"\n        ).properties(\n        width=800,\n        height=500,\n        title=title\n       ).configure_legend(\n  orient='bottom', titleFontSize=18,labelFontSize=18)\n        \n  return result\n\n```\n\nFinally, use the embeddings with reduced dimensionality to create a scatter plot. \n\n```python\nsample = 200\nembeds = np.array(df['title_embeds'].tolist())\nembeds_pc2 = get_pc(embeds,2)\ndf_pc2 = pd.concat([df, pd.DataFrame(embeds_pc2)], axis=1)\n\ndf_pc2.columns = df_pc2.columns.astype(str)\nprint(df_pc2.iloc[:sample])\n\n```\n\n```\n     year                                              title  \\\n0    2022  metric-distortion bounds under limited informa...   \n1    2022  recursion in abstract argumentation is hard --...   \n2    2022  crossing the conversational chasm: a primer on...   \n3    2022  hebo: pushing the limits of sample-efficient h...   \n4    2022  learning bayesian networks under sparsity cons...   \n..    ...                                                ...   \n195  2020  using machine learning for decreasing state un...   \n196  2020  mapping the landscape of artificial intelligen...   \n197  2020  contrasting the spread of misinformation in on...   \n198  2020  to regulate or not: a social dynamics analysis...   \n199  2020  qualitative numeric planning: reductions and c...   \n\n                                                  link  \\\n0    https://www.jair.org/index.php/jair/article/vi...   \n1    https://www.jair.org/index.php/jair/article/vi...   \n2    https://www.jair.org/index.php/jair/article/vi...   \n3    https://www.jair.org/index.php/jair/article/vi...   \n4    https://www.jair.org/index.php/jair/article/vi...   \n..                                                 ...   \n195  https://www.jair.org/index.php/jair/article/vi...   \n196  https://www.jair.org/index.php/jair/article/vi...   \n197  https://www.jair.org/index.php/jair/article/vi...   \n198  https://www.jair.org/index.php/jair/article/vi...   \n199  https://www.jair.org/index.php/jair/article/vi...   \n\n                                          title_embeds          0          1  \n0    [-0.7879443, 0.14064652, -1.1886923, 1.0581255...   3.773588  13.307009  \n1    [0.37486058, 2.2867563, 0.48023587, -1.3632454...  13.585676   8.620154  \n2    [-0.7070194, -0.5557753, 2.6077378, 0.11462678...  17.785715  -8.959141  \n3    [-0.19081053, 0.05036301, -0.48858774, 0.66812...  -0.499191  -5.828358  \n4    [0.84096915, -1.0650194, -0.8836163, -1.631231...  -1.372444  -5.549065  \n..                                                 ...        ...        ...  \n195  [-1.4959816, 0.8587867, 1.1109167, -0.9420541,... -12.141110 -21.291473  \n196  [-1.7567614, 0.12333965, -0.41682896, 0.820096...   9.577528  -8.201695  \n197  [-0.25555933, -1.6548307, -1.1497015, -1.00241...   4.017263  16.588557  \n198  [-1.1415248, 0.9333024, 0.18291989, 2.2976398,...  -2.903703   6.331687  \n199  [-0.8152119, 0.7301979, -1.6634299, -0.395152,... -12.204317 -11.396169  \n\n[200 rows x 6 columns]\n```\n\nHere’s a chart demonstrating the word embeddings for AI papers. It is important to note that the chart represents a sample size of 200 papers.\n\n```python\ngenerate_chart(df_pc2.iloc[:sample],'0','1',title='2D Embeddings')\n```\n\n<div id=\"altair-viz-ffafc1c95c504e37b9eb68aa8d931214\"></div>\n\nData searching techniques focus on using keywords to retrieve text-based information. You can take this a level higher. Use search queries to determine the intent and contextual meaning. In this section, you’ll use Cohere to create embeddings for the search query. Use the embeddings to get the similarity with your dataset’s embeddings. The output is a list of similar AI papers.\n\nFirst, create a function to get similarities between two embeddings. This will use the cosine similarity algorithm from the sci-kit learn library.\n\n```python\nfrom sklearn.metrics.pairwise import cosine_similarity\n\ndef get_similarity(target,candidates):\n  # Turn list into array\n  candidates = np.array(candidates)\n  target = np.expand_dims(np.array(target),axis=0)\n\n  # Calculate cosine similarity\n  sim = cosine_similarity(target,candidates)\n  sim = np.squeeze(sim).tolist()\n  sort_index = np.argsort(sim)[::-1]\n  sort_score = [sim[i] for i in sort_index]\n  similarity_scores = zip(sort_index,sort_score)\n\n  # Return similarity scores\n  return similarity_scores\n\n```\n\nNext, create embeddings for the search query\n\n```python\nnew_query = \"graph network strategies\"\n\nnew_query_embeds = get_embeddings(new_query)\n\n```\n\nFinally, check the similarity between the two embeddings. Display the top 10 similar papers using your result\n\n```python\nsimilarity = get_similarity(new_query_embeds,embeds[:sample])\n\nprint('Query:')\nprint(new_query,'\\n')\n\nprint('Similar queries:')\nfor idx,sim in similarity:\n  print(f'Similarity: {sim:.2f};',df.iloc[idx]['title'])\n\n```\n\n```\nQuery:\ngraph network strategies \n\nSimilar queries:\nSimilarity: 0.49; amp chain graphs: minimal separators and structure learning algorithms\nSimilarity: 0.46; pure nash equilibria in resource graph games\nSimilarity: 0.44; general value function networks\nSimilarity: 0.42; on the online coalition structure generation problem\nSimilarity: 0.42; efficient local search based on dynamic connectivity maintenance for minimum connected dominating set\nSimilarity: 0.42; graph kernels: a survey\nSimilarity: 0.39; rwne: a scalable random-walk based network embedding framework with personalized higher-order proximity preserved\nSimilarity: 0.39; the petlon algorithm to plan efficiently for task-level-optimal navigation\nSimilarity: 0.38; election manipulation on social networks: seeding, edge removal, edge addition\nSimilarity: 0.38; a semi-exact algorithm for quickly computing a maximum weight clique in large sparse graphs\nSimilarity: 0.37; probabilistic temporal networks with ordinary distributions: theory, robustness and expected utility\nSimilarity: 0.36; adaptive greedy versus non-adaptive greedy for influence maximization\nSimilarity: 0.35; classifier chains: a review and perspectives\nSimilarity: 0.35; learning bayesian networks under sparsity constraints: a parameterized complexity analysis\nSimilarity: 0.34; optimally deceiving a learning leader in stackelberg games\nSimilarity: 0.34; planning high-level paths in hostile, dynamic, and uncertain environments\nSimilarity: 0.34; computational complexity of computing symmetries in finite-domain planning\nSimilarity: 0.33; on the indecisiveness of kelly-strategyproof social choice functions\nSimilarity: 0.33; qualitative numeric planning: reductions and complexity\nSimilarity: 0.33; intelligence in strategic games\nSimilarity: 0.33; steady-state planning in expected reward multichain mdps\nSimilarity: 0.32; hybrid-order network consensus for distributed multi-agent systems\nSimilarity: 0.32; evaluating strategic structures in multi-agent inverse reinforcement learning\nSimilarity: 0.32; multi-agent advisor q-learning\nSimilarity: 0.32; fairness in influence maximization through randomization\nSimilarity: 0.32; a sufficient statistic for influence in structured multiagent environments\nSimilarity: 0.32; constraint and satisfiability reasoning for graph coloring\nSimilarity: 0.31; sum-of-products with default values: algorithms and complexity results\nSimilarity: 0.31; contrastive explanations of plans through model restrictions\nSimilarity: 0.31; zone path construction (zac) based approaches for effective real-time ridesharing\nSimilarity: 0.31; an external knowledge enhanced graph-based neural network for sentence ordering\nSimilarity: 0.31; finding and recognizing popular coalition structures\nSimilarity: 0.30; constraint-based diversification of jop gadgets\nSimilarity: 0.30; improved high dimensional discrete bayesian network inference using triplet region construction\nSimilarity: 0.30; migrating techniques from search-based multi-agent path finding solvers to sat-based approach\nSimilarity: 0.30; proactive dynamic distributed constraint optimization problems\nSimilarity: 0.30; safe multi-agent pathfinding with time uncertainty\nSimilarity: 0.30; a logic-based explanation generation framework for classical and hybrid planning problems\nSimilarity: 0.30; scalable online planning for multi-agent mdps\nSimilarity: 0.30; on sparse discretization for graphical games\nSimilarity: 0.30; fond planning with explicit fairness assumptions\nSimilarity: 0.29; jointly learning environments and control policies with projected stochastic gradient ascent\nSimilarity: 0.29; computational benefits of intermediate rewards for goal-reaching policy learning\nSimilarity: 0.29; two-phase multi-document event summarization on core event graphs\nSimilarity: 0.29; efficient large-scale multi-drone delivery using transit networks\nSimilarity: 0.29; fast adaptive non-monotone submodular maximization subject to a knapsack constraint\nSimilarity: 0.29; path counting for grid-based navigation\nSimilarity: 0.29; preferences single-peaked on a tree: multiwinner elections and structural results\nSimilarity: 0.29; merge-and-shrink: a compositional theory of transformations of factored transition systems\nSimilarity: 0.29; lilotane: a lifted sat-based approach to hierarchical planning\nSimilarity: 0.29; cost-optimal planning, delete relaxation, approximability, and heuristics\nSimilarity: 0.29; learning over no-preferred and preferred sequence of items for robust recommendation\nSimilarity: 0.28; a few queries go a long way: information-distortion tradeoffs in matching\nSimilarity: 0.28; constrained multiagent markov decision processes: a taxonomy of problems and algorithms\nSimilarity: 0.28; contiguous cake cutting: hardness results and approximation algorithms\nSimilarity: 0.28; online relaxation refinement for satisficing planning: on partial delete relaxation, complete hill-climbing, and novelty pruning\nSimilarity: 0.28; playing codenames with language graphs and word embeddings\nSimilarity: 0.28; a theoretical perspective on hyperdimensional computing\nSimilarity: 0.28; reasoning with pcp-nets\nSimilarity: 0.28; improving the effectiveness and efficiency of stochastic neighbour embedding with isolation kernel\nSimilarity: 0.27; teaching people by justifying tree search decisions: an empirical study in curling\nSimilarity: 0.27; learning optimal decision sets and lists with sat\nSimilarity: 0.27; efficient multi-objective reinforcement learning via multiple-gradient descent with iteratively discovered weight-vector sets\nSimilarity: 0.27; liquid democracy: an algorithmic perspective\nSimilarity: 0.27; contrasting the spread of misinformation in online social networks\nSimilarity: 0.27; on the computational complexity of non-dictatorial aggregation\nSimilarity: 0.27; planning with critical section macros: theory and practice\nSimilarity: 0.27; regarding goal bounding and jump point search\nSimilarity: 0.27; using machine learning for decreasing state uncertainty in planning\nSimilarity: 0.26; strategyproof mechanisms for additively separable and fractional hedonic games\nSimilarity: 0.26; ranking sets of objects: the complexity of avoiding impossibility results\nSimilarity: 0.26; socially responsible ai algorithms: issues, purposes, and challenges\nSimilarity: 0.26; inductive logic programming at 30: a new introduction\nSimilarity: 0.26; to regulate or not: a social dynamics analysis of an idealised ai race\nSimilarity: 0.25; learning temporal causal sequence relationships from real-time time-series\nSimilarity: 0.25; integrated offline and online decision making under uncertainty\nSimilarity: 0.25; the computational complexity of understanding binary classifier decisions\nSimilarity: 0.25; a survey of opponent modeling in adversarial domains\nSimilarity: 0.25; cooperation and learning dynamics under wealth inequality and diversity in individual risk\nSimilarity: 0.25; sunny-as2: enhancing sunny for algorithm selection\nSimilarity: 0.25; game plan: what ai can do for football, and what football can do for ai\nSimilarity: 0.25; approximating perfect recall when model checking strategic abilities: theory and applications\nSimilarity: 0.25; efficient retrieval of matrix factorization-based top-k recommendations: a survey of recent approaches\nSimilarity: 0.25; evolutionary dynamics and phi-regret minimization in games\nSimilarity: 0.24; labeled bipolar argumentation frameworks\nSimilarity: 0.24; optimal any-angle pathfinding on a sphere\nSimilarity: 0.24; learning from disagreement: a survey\nSimilarity: 0.24; on the cluster admission problem for cloud computing\nSimilarity: 0.24; aggregation over metric spaces: proposing and voting in elections, budgeting, and legislation\nSimilarity: 0.24; ordinal maximin share approximation for goods\nSimilarity: 0.24; constraint solving approaches to the business-to-business meeting scheduling problem\nSimilarity: 0.24; objective bayesian nets for integrating consistent datasets\nSimilarity: 0.24; quantum mathematics in artificial intelligence\nSimilarity: 0.24; task-aware verifiable rnn-based policies for partially observable markov decision processes\nSimilarity: 0.23; samba: a generic framework for secure federated multi-armed bandits\nSimilarity: 0.23; automated reinforcement learning (autorl): a survey and open problems\nSimilarity: 0.23; generic constraint-based block modeling using constraint programming\nSimilarity: 0.23; a comprehensive framework for learning declarative action models\nSimilarity: 0.23; optimizing for interpretability in deep neural networks with tree regularization\nSimilarity: 0.22; impact of imputation strategies on fairness in machine learning\nSimilarity: 0.22; set-to-sequence methods in machine learning: a review\nSimilarity: 0.22; the ai liability puzzle and a fund-based work-around\nSimilarity: 0.22; agent-based markov modeling for improved covid-19 mitigation policies\nSimilarity: 0.22; the parameterized complexity of motion planning for snake-like robots\nSimilarity: 0.22; induction and exploitation of subgoal automata for reinforcement learning\nSimilarity: 0.22; point at the triple: generation of text summaries from knowledge base triples\nSimilarity: 0.22; trends in integration of vision and language research: a survey of tasks, datasets, and methods\nSimilarity: 0.21; multi-document summarization with determinantal point process attention\nSimilarity: 0.21; reward machines: exploiting reward function structure in reinforcement learning\nSimilarity: 0.21; computing bayes-nash equilibria in combinatorial auctions with verification\nSimilarity: 0.21; taking principles seriously: a hybrid approach to value alignment in artificial intelligence\nSimilarity: 0.21; two-facility location games with minimum distance requirement\nSimilarity: 0.21; structure from randomness in halfspace learning with the zero-one loss\nSimilarity: 0.21; avoiding negative side effects of autonomous systems in the open world\nSimilarity: 0.21; learning realistic patterns from visually unrealistic stimuli: generalization and data anonymization\nSimilarity: 0.21; properties of switch-list representations of boolean functions\nSimilarity: 0.21; conceptual modeling of explainable recommender systems: an ontological formalization to guide their design and development\nSimilarity: 0.21; predicting decisions in language based persuasion games\nSimilarity: 0.20; epidemioptim: a toolbox for the optimization of control policies in epidemiological models\nSimilarity: 0.20; ffci: a framework for interpretable automatic evaluation of summarization\nSimilarity: 0.20; analysis of the impact of randomization of search-control parameters in monte-carlo tree search\nSimilarity: 0.20; output space entropy search framework for multi-objective bayesian optimization\nSimilarity: 0.20; multiobjective tree-structured parzen estimator\nSimilarity: 0.19; on the decomposition of abstract dialectical frameworks and the complexity of naive-based semantics\nSimilarity: 0.19; hebo: pushing the limits of sample-efficient hyper-parameter optimisation\nSimilarity: 0.19; superintelligence cannot be contained: lessons from computability theory\nSimilarity: 0.19; goal recognition for deceptive human agents through planning and gaze\nSimilarity: 0.19; dimensional inconsistency measures and postulates in spatio-temporal databases\nSimilarity: 0.19; explainable deep learning: a field guide for the uninitiated\nSimilarity: 0.19; multi-label classification neural networks with hard logical constraints\nSimilarity: 0.19; declarative algorithms and complexity results for assumption-based argumentation\nSimilarity: 0.19; worst-case bounds on power vs. proportion in weighted voting games with an application to false-name manipulation\nSimilarity: 0.19; collie: continual learning of language grounding from language-image embeddings\nSimilarity: 0.18; a word selection method for producing interpretable distributional semantic word vectors\nSimilarity: 0.18; the bottleneck simulator: a model-based deep reinforcement learning approach\nSimilarity: 0.18; metric-distortion bounds under limited information\nSimilarity: 0.18; neural natural language generation: a survey on multilinguality, multimodality, controllability and learning\nSimilarity: 0.18; on the evolvability of monotone conjunctions with an evolutionary mutation mechanism\nSimilarity: 0.18; a metric space for point process excitations\nSimilarity: 0.18; autotelic agents with intrinsically motivated goal-conditioned reinforcement learning: a short survey\nSimilarity: 0.18; the application of machine learning techniques for predicting match results in team sport: a review\nSimilarity: 0.18; incremental event calculus for run-time reasoning\nSimilarity: 0.18; a survey on the explainability of supervised machine learning\nSimilarity: 0.18; madras : multi agent driving simulator\nSimilarity: 0.17; maximin share allocations on cycles\nSimilarity: 0.17; the rediscovery hypothesis: language models need to meet linguistics\nSimilarity: 0.17; some inapproximability results of map inference and exponentiated determinantal point processes\nSimilarity: 0.17; visually grounded models of spoken language: a survey of datasets, architectures and evaluation techniques\nSimilarity: 0.17; the complexity landscape of outcome determination in judgment aggregation\nSimilarity: 0.17; survey and evaluation of causal discovery methods for time series\nSimilarity: 0.17; agent-based modeling for predicting pedestrian trajectories around an autonomous vehicle\nSimilarity: 0.16; rethinking fairness: an interdisciplinary survey of critiques of hegemonic ml fairness approaches\nSimilarity: 0.16; mapping the landscape of artificial intelligence applications against covid-19\nSimilarity: 0.16; crossing the conversational chasm: a primer on natural language processing for multilingual task-oriented dialogue systems\nSimilarity: 0.16; neural machine translation: a review\nSimilarity: 0.16; viewpoint: ethical by designer - how to grow ethical designers of artificial intelligence\nSimilarity: 0.16; core challenges in embodied vision-language planning\nSimilarity: 0.16; neural character-level syntactic parsing for chinese\nSimilarity: 0.16; marginal distance and hilbert-schmidt covariances-based independence tests for multivariate functional data\nSimilarity: 0.16; instance-level update in dl-lite ontologies through first-order rewriting\nSimilarity: 0.16; the societal implications of deep reinforcement learning\nSimilarity: 0.16; benchmark and survey of automated machine learning frameworks\nSimilarity: 0.16; experimental comparison and survey of twelve time series anomaly detection algorithms\nSimilarity: 0.15; annotator rationales for labeling tasks in crowdsourcing\nSimilarity: 0.15; representative committees of peers\nSimilarity: 0.15; fine-grained prediction of political leaning on social media with unsupervised deep learning\nSimilarity: 0.15; out of context: a new clue for context modeling of aspect-based sentiment analysis\nSimilarity: 0.15; loss functions, axioms, and peer review\nSimilarity: 0.15; on the tractability of shap explanations\nSimilarity: 0.15; doubly robust crowdsourcing\nSimilarity: 0.15; adversarial framework with certified robustness for time-series domain via statistical features\nSimilarity: 0.14; on quantifying literals in boolean logic and its applications to explainable ai\nSimilarity: 0.14; bribery and control in stable marriage\nSimilarity: 0.14; finding the hardest formulas for resolution\nSimilarity: 0.14; supervised visual attention for simultaneous multimodal machine translation\nSimilarity: 0.14; a tight bound for stochastic submodular cover\nSimilarity: 0.14; ethics and governance of artificial intelligence: evidence from a survey of machine learning researchers\nSimilarity: 0.13; flexible bayesian nonlinear model configuration\nSimilarity: 0.13; multilingual machine translation: deep analysis of language-specific encoder-decoders\nSimilarity: 0.13; multilabel classification with partial abstention: bayes-optimal prediction under label independence\nSimilarity: 0.13; viewpoint: ai as author bridging the gap between machine learning and literary theory\nSimilarity: 0.13; confident learning: estimating uncertainty in dataset labels\nSimilarity: 0.13; belief change and 3-valued logics: characterization of 19,683 belief change operators\nSimilarity: 0.13; get out of the bag! silos in ai ethics education: unsupervised topic modeling analysis of global ai curricula\nSimilarity: 0.12; on the distortion value of elections with abstention\nSimilarity: 0.12; measuring the occupational impact of ai: tasks, cognitive abilities and ai benchmarks\nSimilarity: 0.11; fair division of indivisible goods for a class of concave valuations\nSimilarity: 0.10; admissibility in probabilistic argumentation\nSimilarity: 0.10; recursion in abstract argumentation is hard --- on the complexity of semantics based on weak admissibility\nSimilarity: 0.10; weighted first-order model counting in the two-variable fragment with counting quantifiers\nSimilarity: 0.10; incompatibilities between iterated and relevance-sensitive belief revision\nSimilarity: 0.10; automatic recognition of the general-purpose communicative functions defined by the iso 24617-2 standard for dialog act annotation\nSimilarity: 0.09; nlp methods for extraction of symptoms from unstructured data for use in prognostic covid-19 analytic models\nSimilarity: 0.09; welfare guarantees in schelling segregation\nSimilarity: 0.09; casa: conversational aspect sentiment analysis for dialogue understanding\nSimilarity: 0.09; a survey of algorithms for black-box safety validation of cyber-physical systems\nSimilarity: 0.07; relevance in belief update\nSimilarity: 0.06; on super strong eth\nSimilarity: 0.06; image captioning as an assistive technology: lessons learned from vizwiz 2020 challenge\nSimilarity: 0.03; confronting abusive language online: a survey from the ethical and human rights perspective\n```\n\nVisualizing semantic search: <https://github.com/cohere-ai/cohere-developer-experience/blob/main/notebooks/Visualizing_Text_Embeddings.ipynb>\n\nClustering is a process of grouping similar documents into clusters. It allows you to organize many documents into a smaller number of groups. As a result, you can discover emerging patterns in the documents. In this section, you will use the k-Means clustering algorithm to identify the top 5 clusters. \n\nFirst, import the k-means algorithm from the scikit-learn package. Then configure two variables: the number of clusters and a duplicate dataset.\n\n```python\nfrom sklearn.cluster import KMeans\n\ndf_clust = df_pc2.copy()\nn_clusters=5\n\n```\n\nNext, initialize the k-means model and use it to fit the embeddings to create the clusters.\n\n```python\nkmeans_model = KMeans(n_clusters=n_clusters, random_state=0)\nclasses = kmeans_model.fit_predict(embeds).tolist()\nprint(classes)\ndf_clust['cluster'] = (list(map(str,classes)))\n\n```\n\n```\n[2, 0, 3, 4, 4, 3, 1, 1, 0, 3, 0, 2, 0, 1, 1, 0, 0, 2, 0, 1, 3, 2, 1, 3, 0, 2, 2, 0, 2, 1, 1, 2, 2, 1, 0, 1, 1, 1, 2, 2, 2, 4, 3, 3, 3, 3, 2, 1, 2, 4, 3, 0, 2, 0, 1, 1, 0, 4, 0, 2, 2, 3, 1, 2, 4, 1, 2, 1, 4, 0, 3, 3, 4, 2, 0, 2, 2, 2, 0, 0, 0, 4, 1, 4, 1, 2, 0, 4, 1, 1, 4, 1, 4, 1, 4, 1, 0, 0, 4, 2, 4, 3, 4, 3, 2, 0, 2, 1, 1, 4, 2, 4, 2, 2, 0, 3, 1, 3, 2, 3, 1, 2, 0, 4, 4, 1, 0, 0, 4, 1, 1, 2, 2, 1, 2, 3, 0, 0, 1, 1, 1, 0, 4, 1, 4, 2, 4, 2, 4, 3, 2, 0, 1, 4, 1, 1, 2, 2, 0, 1, 1, 1, 1, 1, 1, 2, 2, 0, 2, 0, 4, 2, 4, 2, 1, 0, 3, 0, 1, 0, 2, 2, 1, 4, 1, 3, 4, 1, 0, 2, 1, 2, 0, 2, 4, 1, 4, 2, 2, 1, 0, 0, 1, 0, 2, 1, 0, 4, 1, 4, 0, 2, 1, 4, 1, 3, 2, 4, 2, 0, 1, 0, 3, 0, 2, 4, 1, 1, 3, 2, 3, 1, 3, 4, 2, 2, 0, 1, 1, 1, 4, 1, 0, 4, 3, 2, 2, 2, 2, 0, 1, 3, 1, 3, 4, 2, 4, 2, 1, 3]\n```\n\nFinally, plot a scatter plot to visualize the 5 clusters in our sample size.\n\n```python\ndf_clust.columns = df_clust.columns.astype(str)\ngenerate_chart(df_clust.iloc[:sample],'0','1',lbl='off',color='cluster',title='Clustering with 5 Clusters')\n```\n\n<div id=\"altair-viz-11d2bea9f1674b80b31c9ada88b1ec59\"></div>\n\nLet's recap the NLP tasks implemented in this tutorial. You’ve created word embeddings, perform a semantic search, and text clustering. Cohere’s platform provides NLP tools that are easy and intuitive to integrate. You can create digital experiences that support powerful NLP capabilities like text clustering. It’s easy to Register a Cohere account and gain access to an API key. New cohere accounts have $75 free credits for the first 3 months. It also offers a Pay-as-you-go Pricing Model that bills you upon usage.",
    "html": "",
    "htmlmode": false,
    "fullscreen": false,
    "hidden": true,
    "revision": 6,
    "_id": "664deee092e87f00128747d7",
    "__v": 0,
    "createdAt": "2024-05-22T13:10:56.181Z",
    "lastUpdatedHash": "4e53a946318620e19a44d038cee3bc3e36370505",
    "project": "62cde2919aafea009aefb289",
    "updatedAt": "2024-07-11T01:20:28.569Z",
    "user": "63fcfa48e37787000ae6fbdd"
  },
  "meta": {
    "user": {
      "allowedProjects": ["cohere-ai", "cohere-enterprise"],
      "apiKey": "",
      "email": "andrewjiang@hey.com",
      "name": "Andrew Jiang",
      "version": 1,
      "Name": "Andrew Jiang",
      "Email": "andrewjiang@hey.com",
      "APIKey": "",
      "AllowedProjects": ["cohere-ai", "cohere-enterprise"]
    },
    "baseUrl": "/",
    "hidden": true,
    "title": "Topic Modeling AI Papers",
    "metaTitle": "Topic Modeling AI Papers",
    "keywords": "",
    "description": "",
    "image": [],
    "slug": "topic-modeling-ai-papers",
    "type": "custompage",
    "full": false
  }
}
